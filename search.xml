<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线性基基础]]></title>
    <url>%2F2019%2F02%2F03%2FLinearBase%2F</url>
    <content type="text"><![CDATA[线性基我们在碰到XOR问题的时候，通常难以下笔。这是因为XOR问题的解法通常难以构造，直观上建模比较困难。为了解决这一类问题，线性基由此而提出。通过我们所熟知的XOR的一些完美的性质，于是我们可以把一堆数压缩成64个数，可以保证这64个数的异或结果的值域与原数组的值域一样。而这个压缩的过程就是线性基的构造过程，我们理解起来也很简单。 基上面所说的压缩成64个数也许你会有一些疑问，我们不妨设想，现在有一组二进制基:1，10，100，1000，…。我们其实可以通过这一些基相互异或得到$[0,2^{65}-1]$区间内所有的数。但是我们需要构造与原数组所有子集异或值域一样的基，就被称作线性基，线性基保证了产生的基的个数最少。类似于线性代数中的极大无关组。所谓基的概念也是从线性代数中引用过来的。 构造方法首先我们申请一个数组$base[64]$，对于$base[i]$而言存储的是二进制中最高位的1在第$i$位的数。64表示二进制的数位，最高为64位，可以向上叠加。 我们对原数组中每一个数$x$，从高位到低位遍历，若当前最高位$i$为1，我们检查$base[i]$是否有值，有值的话，我们令$x=x$^ $base[i]$，否则我们令$base[i]=x$ 构造代码123456789101112131415long long base[100];scanf("%lld",&amp;n);for(i=0; i&lt;n; i++) &#123; scanf("%lld",&amp;a); for(j=63; j&gt;=0; j--) &#123; if(a&amp;(1LL&lt;&lt;j)) &#123; if(base[j]) &#123; a^=base[j]; &#125; else &#123; base[j]=a; break; &#125; &#125; &#125;&#125; 线性基的性质线性基的任何一个非空子集都不会使得其$xor​$和为0，证明方法使用反证法即可。这个性质可以保证线性基的算法的合法性。 来解决一些问题吧异或最大值问题给我们一堆数，我们希望知道哪些数相互异或后结果最大，输出最大的值即可。 解法我们使用首先构造线性基，初始化$ans=0$，之后对于线性基从高位向低位扫，若当前的$base[i]$与ans异或后结果更大，那么就异或。否则跳过这一位即可。 异或判断问题给我们一堆数，再给我们一个数$x$，判断$x$是否可以由这些数的一个子集异或得到。 解法我们首先构造线性基，之后对于$x$，我们只需要对于$x$从高位到低位每个1与$base[i]$异或即可，若最后$x$的值为0，那么就可以被构造出来，否则无法被构造出来。其实意是也是很简单的，就相当于插入过程，如果$x$不需要导出新的$base[i]$，说明$x$无用，那么就一定可以被构造出来。否则需要导出新的$base[i]$来辅助构造$x$。 CF1011G将原数组分为几段，每段内异或得到一个值，且任何一些连续段的异或值都不为0，最多能够划分多少段？ 解法这个题目用到了上文介绍到的性质，线性基的任何一个非空子集都不会使得其$xor$和为0。于是本题的答案就是构造线性基的结果中,$base[i]$不为0的个数。 异或种数问题给我们一堆数，问这些数的所有子集异或所得到的数有多少种？ 解法答案就是2^线性基中基的个数。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>LinearBase</tag>
        <tag>codeforces</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小问题集合-长期更新]]></title>
    <url>%2F2019%2F01%2F31%2FProblemSet%2F</url>
    <content type="text"><![CDATA[这是一些有意思的小问题，长期更新题目 现在有$n$类钞票，每种有无限张，我们从中带走一些钞票。给定一个数$k$，希望我们保证$[1,k]$这个面值区间的每一个数我们都可以从带走的钞票中凑出来。问对于给定的$k$，我们至少要需要带走几张钞票，不能满足输出$-1$。$n&lt;1e5,k&lt;1e9$ 给定一个$1$到$n$的排列，求一个子序列使得其逆序对数与长度比值最大，输出这个比值。$n&lt;100$ 给定两个1到$n$的排列，求这两个排列的最长公共序列。$n&lt;1e5$ 给定一个$n*n$矩阵，其中数字各不相同，找到一个矩阵的局部最小(大)值的坐标输出即可。$n&lt;1e5$ 给出$n$个数，这些数互不相同，我们希望删除其中的一个数，使得剩余的所有数异或和最大。$n&lt;1e 5$ 给出$n$个数，这些数互不相同，我们希望删除其中的两个数，使得剩余的所有数异或和最大。$n&lt;1e 5​$ 给出$n+1$个数，每个数都在$[1,n]$之间，且只有一个数出现两次，要求$O(n)$时间$O(1)$空间找出。 给$n$个数，保证存在唯一一个出现奇数次的数，找到这个数。$n&lt;1e5$ 给$n$个数求最大最小数。比较次数控制在$1.5n$次之内。 在一大堆数中求top100，要求时间复杂度小于$O(nlogn)$ 解法 我们使用贪心的思想，有一个小trick就是我们对于选中的数集的和$sum$而言，我们下一次选的数一定是在$sum+1$之内的，所以只需要每次贪心的选取就可以了。时间复杂度$O(\log k)$。 遇到这种比值的且$n$较小的，一般都是最大密度子图问题，这个就是一个裸题，但是很多时候我们往往想不到。时间复杂度$O(E100^2)$ 由于排列当中没有相同的数字，那么我们可以把问题变成最长上升序列问题。时间复杂度$O(n\log n)$ 这个题目明显是一道分治的题目，我们可以每次递归问题到一个小矩阵，做法很简单，我们至于需要见检查中间一列，中间一行是否有数满足条件，若满足条件，输出。不满足条件，向检查过程中最小值(一定在小矩阵内部)出现的那一个矩阵进行递归即可。时间复杂度$O(n)$ 我们在原数组中删除一个数$x$，相当于在数组中添加一个数$x$。这是异或有意思的性质。于是我们直接拿最初的异或结果$sum$，向原数组中的每个数异或一遍得到最大的结果就可以了。时间复杂度$O(n)$ 我们在原数组中删除两个数$x，y$，相当于在数组中添加两个数$x，y$。我们首先构造一个01字典树,树高为64。于是我们直接拿最初的异或结果$sum$，向原数组中的每个数异或，接下来我们只需要按照异或结果的反方向在树上走一遍就好了，之后我们可以得到答案。时间复杂度$O(64n)$ 我们只需要对原数组求和之后减去$n*(1+n)/2$即可。时间复杂度$O( n )$ 我们只需要对原数组的所有数异或一边即可。时间复杂度$O( n )​$ 我们对$n$进行两两分组，之后两个数之间先确定大小。再与当前最大值与当前最小值进行比较，即这里可以省去不必要的比较。 我们固定一个大小为100的小根堆，每次插入，调整，删除即可。时间复杂度$O(nlog100)$]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Nice-Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最短路问题]]></title>
    <url>%2F2019%2F01%2F11%2FMinDistance%2F</url>
    <content type="text"><![CDATA[最短路问题最短路问题是一类十分典型的图论问题，给定一个图，求起点$s$至终点$t$的最短路径。使用数学语言描述为： 给定图$G(V,E)$ $e=(from,to,distance)\in E$ 表示边集$E$中的边有三个参数表示起点，终点，距离 $s,t\in V$ 起点终点都属于点集$V$，求最短距离$MinDis(s,t)$ 这类问题理解起来也很简单，典型的问题就是火车买票问题，求上海到北京怎么买票花费最少，或者用时最短，这都可以建模为最短路问题。关于最短路问题，分为两类： 单源最短路 多源最短路 虽然多源最短路可以通过多次调用单源最短路的算法达到效果，但是对于一些算法的出发点是不同的，所以还是归为两类。对于多源最短路问题，最典型的算法是Floyd算法，通过$O(V^3)$的时间可以求出来任意两点的最短路径。对于单源最短路问题，方法多种多样，代表性的算法是Bellman-ford、Spfa、Dijkstra。本文将会依次介绍上述4种算法。 Bellman-ford我们按照时间顺序介绍上述算法，在1956年，提出了第一个基于动态规划思想的最短路算法Bellman-ford。这个算法为后面的所有算法打下了理论基础，所以Bellman-ford算法是一个非常值得我们仔细思考的算法。 首先回到我们的问题，我们希望求$s-t$的最短路径，由于我们中间最多有$V-2$个节点，经过$V-1$条不同路径，那么我们相当于我们从$s$出发，每次尝试一下$V$种节点，那么我们总共会有$(V-1)^{V}$可能的路径，这个估计明显是个上界，显然是不可解的。于是我们采用动态规划化简上面的求解过程，仔细想想，这个过程和和隐马尔可夫模型中的Viterbi算法的过程一样。，所以我认为Viterbi算法(1967年提出)极大的受到了Bellman-ford算法的启发现在看来神经网络还真是玄学啊，感觉和之前的算法没什么共通性我们定义状态转移方程：$$dp[i][step]=min(dp[i][step-1],dp[j][step-1]+dis[j][i])\quad j=1,2,3,…,n$$上面的dis数组表示节点$j$与节点$i$之间的距离。step表示当前最多允许通过几个节点。 注意：我们使用Bellman-ford算法还有一个原因，就是Bellman-ford可以用来判断负环，负环的意思就是说网络中存在一个环，环上的边的所有权值相加为负数。这样的话，我们找最短路，就可以一直沿着这个环走，从而得到一个无穷小的值，但是这显然是不合理的。所以我们可以通过Bellman-ford算法进行检测，检查的方式也很简单，我们在运行了$V-1$次程序之后，再运行一次，如果存在负环的话，那么我们对每个环至少可以走一圈，也就是说，如果还有节点的最短距离可以继续缩短的话，那么就存在负环，否则就不存在。请好好思考为什么，如果不明白这个，那么我只能说你还没有掌握到Bellman-ford算法的精髓。 代码实现本文的代码能够AC HDU-2544。 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;pair&lt;int,int&gt; &gt; vec[1005];//end-distanceint dp[1005][1005],n,m,INF=1e9+7;int BellmanFord(int s,int t) &#123; int i,j,step; for(i=0; i&lt;n; i++) &#123; for(j=0; j&lt;n; j++) &#123; dp[i][j]=INF; &#125; &#125; for(i=0; i&lt;n; i++) &#123; dp[s][i]=0; &#125; for(step=1; step&lt;n; step++) &#123; for(i=0; i&lt;n; i++) &#123; for(auto x : vec[i]) &#123; dp[i][step]=min(dp[i][step],min(dp[i][step-1],dp[x.first][step-1]+x.second)); &#125; &#125; &#125; return dp[t][n-1];&#125;int main(void) &#123; int i,j,a,b,c; while(scanf("%d %d",&amp;n,&amp;m)!=EOF) &#123; if(n==0&amp;&amp;m==0) break; for(i=0; i&lt;n; i++) &#123; vec[i].clear(); &#125; for(i=0; i&lt;m; i++) &#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); a--; b--; vec[a].push_back(make_pair(b,c)); vec[b].push_back(make_pair(a,c)); &#125; printf("%d\n",BellmanFord(0,n-1)); &#125;&#125; 时间复杂度分析很明显Bellman-ford算法需要更新$V-1$次，同时每一次我们需要对每条边进行检查，时间复杂度为$O(VE)$ SpfaSpfa的全称是shortest path faster algorithm。这个算法的名字倒还是蛮中肯，并没有说是fastest。这个算法也是一种单源最短路径算法，其本质是对Bellman-ford算法的队列优化。它省去了一些冗余操作。其想法主要在Bellman-ford的计算方式上。我们再次看看状态转移方程：$$dp[i][step]=min(dp[i][step-1],dp[j][step-1]+dis[j][i])\quad j=1,2,3,…,n$$我们很可能存在这样一种情况，对于第$step$轮循环来说$dp[j][step-1]$的值和第$step-1$的$dp[j][step-1]$是一样的，同时我们在上一轮已经计算过了，那么对于这一轮来说，再计算完全是浪费的计算，我们可不可以减少这样的冗余计算呢？ 其实改进的方式也很简单，我们在递推过程中，如果当前的最短距离的值减小了，我们才用这个减小的值去更新别的值，直到所有的节点的距离不再减小。换句话说，当我们检查到当前的节点的最短距离变小了，我们才用这个最短距离去检查是不是能让其相邻节点的最短路也减小距离，请注意我们并不是立即更新，而是放入队列进行等待，依次更新，这样是有很大好处的，请仔细想想为什么我们会这样操作。我们每次更新的这种操作也叫松弛操作。 注意：Spfa也是可以判断负环的，判断的方式也很简单，只需要对每个节点检查入队次数即可，因为一个节点入队的次数是不会超过$n$次的，所以当一个点入队次数超过$V$，就肯定存在负环。因为一个点到源点的距离不可能被更新$V$次，因为路径上最多只有$V-1$个节点，不存在还有其他节点能够再松弛距离。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;pair&lt;int,int&gt; &gt; vec[1005];int dp[1005],n,m,INF=1e9+7,vis[1005];queue&lt;int&gt; que;int Spfa(int s,int t)&#123; int i,T; for(i=0; i&lt;n; i++) &#123; dp[i]=INF; &#125; dp[s]=0; que.push(s); vis[s]=1; while(que.size())&#123; T=que.front(); vis[T]=0; que.pop(); for(auto x : vec[T])&#123; if(dp[x.first]&gt;dp[T]+x.second)&#123; dp[x.first]=dp[T]+x.second; if(!vis[x.first])&#123; vis[x.first]=1; que.push(x.first); &#125; &#125; &#125; &#125; return dp[t];&#125;int main(void) &#123; int i,j,a,b,c; while(scanf("%d %d",&amp;n,&amp;m)!=EOF) &#123; if(n==0&amp;&amp;m==0) break; for(i=0; i&lt;n; i++) &#123; vec[i].clear(); &#125; for(i=0; i&lt;m; i++) &#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); a--; b--; vec[a].push_back(make_pair(b,c)); vec[b].push_back(make_pair(a,c)); &#125; printf("%d\n",Spfa(0,n-1)); &#125;&#125; 时间复杂度分析Spfa的时间复杂度不是很好分析。。虽然Spfa大部分时候都比较快，但是某些精心构造的图上可以让spfa跑的很慢，目前证明的结果是Spfa的时间复杂度和Bellman-ford是一样的，最坏情况下时间复杂度为$O(VE)$。但是对于大部分含有噪声的数据上跑的还是很快，这也是我们为什么说噪声是个好东西，实际会让你的算法效果变的更优。 Flody弗洛依德算法是一个多源最短路径算法，这个算法的发明人曾是一个文科专业毕业的学生，由于难以找到工作，转行计算机，但是却在计算机行业干的风生水起，最后走上人生巅峰拿了图灵奖。废话说了一堆，让我们聊一聊这个大名鼎鼎的算法，这个算法美就美在它只用了几行代码就实现了多源最短路。废话不多说，先看代码。 代码实现1234567int floyd(int s,int t)&#123; for(k=0;k&lt;n;k++) for(i=0;i&lt;n;i++) for(j=0;j&lt;n;j++) dis[i][j]=min(dis[i][j],dis[i][k]+dis[k][j]); return dis[s][t];&#125; 没错，就是这么简单粗暴，代码就是这么精简，但是却实现多源最短路这个神奇的任务，这是为什么呢？很多人都会说Flody算法没什么啊，很显然啊，就是对每个点都中转一次而已，没什么难度。但是事实真的是这么显然吗？为什么我没看出来？既然都是中转一遍，为什么k一定要在外层，我可以把k放在里层吗？也请读者你再带着怀疑的眼光再观察这份代码，好好想想，我觉得很有可能出现$dis[i][k]$并没有达到最优情况或者$dis[k][j]$没有达到最优情况。请在思考后再看下面的解释，我认为这会更加帮助你理解Floyd那精妙的想法。 时间复杂度分析简单粗暴的$O(V^ 3)$的时间复杂度，其实对于多源最短路来说，这个复杂度并不糟糕，我们调用$V$次Bellman-ford所需要花的时间是$O(V^2E)$但是$E$在稠密图的话，量级是趋于$V^2$的，所以对比最坏情况来说，Floyd算法不仅在数量级上取得了质的改变，同时代码的量也极其精简。 正确性证明Floyd算法的正确性真的就这么显然吗？我不是这么认为的，现在我们使用数学方法来说明为什么这样写是正确的。 我们假设源点$i$到汇点$j$中最优路径需要经过的最大节点为$x$。我们需要说明在$k=x$这一轮结束后，$dis[i][j]$一定得到了最小值。也就是说$dis[i][k],dis[k][j]$已经为最小值了。其实到此为止我们就已经发现最优子结构了，原本是不需要再往下解释的，但是我们再详细解释一下。 假设我们此时对$dis[i][k]$继续进行递归。而此时最大的为$x1$。我们可以知道$x1&lt;x$，再进行递归下去。 最后我们可以得到$x_n$是最小的需要经过的节点，而这个节点肯定是与$s$直接连接，而无法被减小的。那么我们可以回到上一层，通过这样的归纳，我们可以证明$dis[i][k]$与$dis[k][j]$一定是最小值。那么$dis[i][j]$通过$k=x$这一轮循环，一定可以得到最小值。 DijkstraDijkstra算法，就是目前最常使用的单源最短路算法，他的想法也是从Bellman-ford算法得到的，他通过仔细观察Bellman-ford算法的计算过程，总结出了一个极其优美的规律，那就是我们每一轮的松弛操作只需要对当前距离源点$s$最近的且没有被访问过的节点开始松弛就可以了，其他的节点我们不需要松弛。但是这是为什么呢？我的理解是因为我们既然做Bellman-ford的松弛操作。我们肯定是希望和spfa那样，选择最有可能减小其他节点的最短路的点去进行松弛操作，而这个点就是最近的且没有被访问过的节点。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;int dis[1005],vis[1005],ma[1005][1005],INF=1e9+7,n,m;int dijstra(int x,int y)&#123; int i,j,t; for(j=1;j&lt;=n;j++) dis[j]=ma[x][j]; memset(vis,0,sizeof(vis)); vis[x]=1; while(true)&#123; t=-1; for(i=1;i&lt;=n;i++)&#123; if(dis[i]!=INF&amp;&amp;!vis[i])&#123; if(t==-1||(dis[t]&gt;dis[i])) t=i; &#125; &#125; if(t==-1) break; vis[t]=1; for(i=1;i&lt;=n;i++)&#123; dis[i]=min(dis[i],dis[t]+ma[t][i]); &#125; &#125; return dis[y];&#125;int main(void)&#123; int T,i,j,a,b,c,t,res; while(scanf("%d %d",&amp;n,&amp;m)!=EOF)&#123; if(n==0&amp;&amp;m==0) break; for(i=0;i&lt;1005;i++)&#123; for(j=0;j&lt;1005;j++)&#123; ma[i][j]=INF; &#125; &#125; for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); ma[a][b]=min(ma[a][b],c); ma[b][a]=ma[a][b]; &#125; res=dijstra(1,n); printf("%d\n",res); &#125;&#125; 时间复杂度分析上面的代码是最简单的dijkstra实现的方法，采用的是邻接矩阵存储。时间复杂度为$O(n^2)$。当然我们可以采用二叉堆、Binomial堆、斐波那契堆进行优化，得到$O(n\log n)​$级别的Dijkstra算法。在此不再赘述。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>graph-theory</tag>
        <tag>Bellman-ford</tag>
        <tag>Dijkstra</tag>
        <tag>Spfa</tag>
        <tag>Floyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大流问题]]></title>
    <url>%2F2019%2F01%2F09%2FMaxFlow%2F</url>
    <content type="text"><![CDATA[最大流问题最大流问题是算法竞赛中经常考察的问题，其目标是解决这样一类问题：给定源点$s$和汇点$t$，给定包含源点汇点的网络，网络中每条边有其相应的所能经过的最大流量，求源点至汇点最多能有多少流量。用数学语言描述为： 给定有向图$G(V,E)$ $V$表示点集 $E$表示边集，其中每条单向边由三元组组成，$(from,to,cap)\in E$。表示起点、终点、每条边的容量。 给定源点$s$，汇点$t$，求$s$到$t$的最大流量$MaxFlow(s,t)$ 目前已经有相对较为快速的算法（EdmondKarp算法，Dinic算法）而其难点主要在于如何构建网络模型。相对常见的问题主要分为几类： 最小点/边独立集 最小点/边覆盖集 最小割问题 拆点建图 本文将介绍Ford-Fulkerson方法，EdmondKarp算法和Dinic算法。 Ford-Fulkerson方法Ford-Fulkerson算法是最早提出来的用于解决最大流问题的方法，之所以称为方法，是因为它并不严谨，它仅仅是一颗seed，它发展壮大形成了很多有趣的算法，许多科研前辈为了培养这颗seed，花费了毕生精力。其思想十分简单，每次在残余网络中寻找从源点$s$到$t$的增广路，若存在，那我们就从中任取一条，之后答案添加相应的流量，同时添加反向边再形成残余网络，若不存在，那么算法结束，我们已经得到了最大流。 什么是残余网络我们举个例子来说： 上面的这个图，其中边上的参数$a/b$表示：目前通过流量和该边的流量最大限制。由图可知，我们可以得到这两种调度方案。第一种方案的所能运输的最大流量为1，而第二种方案所能运输的最大流量为2。这个问题就很尴尬了，对于给定的图，我们怎么确保算法可以得到最优解呢？这个时候残余网络的出现给了我们一个可靠的解决方案。对于第一个图，我们选则之后，相应的添加反向边，得到下面的图： 我们可以在选择了这条路径之后，可以添加红色的反向边得到残余网络，之后在残余网络中再寻找增广路，仍然可以得到最大流为2。 所谓残余网络其实就是在中途建立退货边，让算法有一个反悔的机会，这真是一个很好的思想！很多时候这个思想可以帮助我们解决很多问题。 时间复杂度分析Ford-Fulkerson方法确实很妙，但是它的时间复杂度分析就不忍直视了，我们假设每次考虑最坏情况，首先花$O(E)$的时间BFS得到所有$s-t$的增广路，但是不巧每次挑选的增广路径的流量都为1。那么，算法的时间复杂度就为$O(FE)$，其中$F$为最大流量。当然这个时间复杂度很高，但是它跑起来还是很快的，因为实际生活中几乎是碰不到这样的图的，相对而言，这个方法还是可以用的。 Ford-Fulkerson的扩展对于Ford-Fulkerson方法的改进主要有两种。 每次找一个$s-t$流量最大的增广路 每次找一个$s-t$距离最短的增广路 对于每次找一个流量最大的增广路，想法就很简单了，就是为了更快的结束算法，但是完成这个目的太慢了。我们考虑一个更简单的问题，每次找一个尽量大的增广路，为了完成这个目标，我们仅仅只需要套一个二分就行了。 对于每次找一个最短的增广路，产生了很多分支，接下来我们介绍EdmondsKarp算法和Dinic算法，同时也解决了最小费用最大流的问题。 EdmondsKarp算法该算法是1972年提出的，它的效率比1970年提出的Dinic算法要差，而这个算法也是Dinic玩剩下的东西，但是为什么paper还是能发出来呢？这真是一个有意思的问题。当时美国和苏联冷战，科研成果并不互通，美国人还不知道Dinic那精妙的思想。 EdmondsKarp算法改善了Ford-Fulkerson中在残余网络中挑选增广路的过程，EdmondsKarp遵循的原则是每次挑选$s- t$最短的增广路，正确性是显而易见的。通过这样的改进，算法的复杂度从$O(FE)$降低到了$O(VE^2)$。 时间复杂度分析这个时间复杂度的分析其实就是Dinic算法的精髓所在，为了解释这个时间复杂度，我们引入分层网络这个概念。 分层网络我们举个例子来说： 左边是我们当前的残余网络，中间的图表示的就是从源点$s$到$t$中经过的分层后的形状。其中相同颜色的节点表示的是到$s$的距离相同，也就是节点的深度$d$相同，由于我们每次找的是$s - t$的最短路这样的话，我们每次最优的增广路肯定是不含相同颜色的，因为若含相同颜色，肯定不是最短的路径了。同样的，最右边的图表示去掉与$t$相同颜色的点，同时将不同层的边补齐，同时也想想为什么相同层不补齐呢？。 现在我们考虑这样一个情况。 我们找到了一条$s - t$的增广路，但是限制最紧的边为$u-v$这条边，那么我们在建立完反向边之后，$v$在分层图中肯定是向后延伸或者不变的，于是我们得到$d_{f’}(v)&gt;=d_f(v)$。但是如果红色的边想再次做限制最紧的点的话，$v$的层次肯定比$u$的层次低一级，于是当这条边再做限制最紧的边时，必然满足$d_{f’’’}(u)&gt;=d_f(u)+2$,$d_{f’’’}(v)&gt;=d_f(v)+2$。等于说，完成这样的一次交换，$u$和$v$的层次都向后延伸了2。由于层次大于等于汇点$t$的层次的话，这个点肯定就不在最短路上了，同时$t$的层次最大就是$V$。那么前面的操作最多只有$\frac V 2$次。相当于每条边最多只有$\frac V 2$次机会做限制最紧的边。 时间复杂度的计算那么对于每条边而言，这一部分的时间复杂度为$O(\frac V 2 E)$。同时我们每次BFS找最短路的时间复杂度为$O(E)$。那么总的时间复杂度就是$O(VE^2)$。通过上面的分析，实际上这个复杂度也是虚高的，通常不需要跑这么久，而且对于某些图跑的比Dinic还要快，但是为什么会快一些呢？如果后面你的Dinic你真正理解的话就再回头想想为什么。 Dinic算法这个算法是真的牛逼，因为EdmondKarp算法其实是Dinic的副产物。通过上面的介绍，我们可以知道，EK算法每次BFS得到最短路径，之后计算的结果就抛弃了，马上构建反向边，在残余网络中重新计算。但是实际上，我们可以利用之前的结果，我们可以通过一次BFS得到每个节点的深度，之后补上相邻层的边之后得到一个分层网络。我们通过上面的分析，$s - t$的增广路的最短路的长度肯定是每次肯定是非递减的！虽然直观上也是这样，那么这也就意味着，我们构建一次分层网络，可以直接把当前分层网络的最短路全部构建反向边，因为不这样做，下次我们找到的最短路还是这些增广路，因为它们已经是最短的了！！！(仔细想想和Dijkstra算法的思想有共同之处啊)我们通过这样做，简单来说我们一次BFS的结果找到了多条路径。 再次看看这个图，我们构建了一次分层网络，可以一次性找到3条增广路呢，我们依次构建反向边即可。要注意每次边权都要更新。 时间复杂度分析按照Dinic的思想，我们每次构建分层网络的时间是边数$O(E)$，每次我们对最短路建反向边只需要对每个节点遍历一边即可$O(V)$，由于每次这样操作一次后，$t$的深度，也就是$d_t$至少会向后延伸1，也就是说$d’_t&gt;=d_t+1$。同样的道理，深度最多是一条链，那么$d_t$最多增长$V$次。所以总时间就是$O(V^2E)$。而这个复杂度确实是相对比较满的，我们的确可以构造出来这样的图，让Dinic跑满，当然Dinic有弧优化技巧尽量避免这些情况，当然这都是后话了，本文暂不做讨论。 代码实现最大流入门题目，HDU-3549，以下是EK和Dinic的代码实现。 EdmondsKarp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;bits/stdc++.h&gt;using namespace std;const int INF = 0x3f3f3f3f;const int maxn = 5000;struct Edge&#123; int from, to, cap, flow; Edge(int u, int v, int c, int f):from(u), to(v), cap(c), flow(f) &#123;&#125;&#125;;struct EdmondsKarp&#123; int n, m; vector&lt;Edge&gt; edges; vector&lt;int&gt; G[maxn]; int a[maxn]; int p[maxn]; void init(int n) &#123; for (int i=0; i&lt;n; i++) G[i].clear(); edges.clear(); &#125; void AddEdge(int from, int to, int cap) &#123; edges.push_back(Edge(from, to, cap, 0)); edges.push_back(Edge(to, from, 0, 0)); m = edges.size(); G[from].push_back(m-2); G[to].push_back(m-1); &#125; int Maxflow(int s, int t) &#123; int flow = 0; for (;;) &#123; memset(a, 0, sizeof(a)); queue&lt;int&gt; Q; Q.push(s); a[s] = INF; while (!Q.empty()) &#123; int x = Q.front(); Q.pop(); for (int i=0; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (!a[e.to] &amp;&amp; e.cap &gt; e.flow) &#123; p[e.to] = G[x][i]; a[e.to] = min(a[x], e.cap-e.flow); Q.push(e.to); &#125; &#125; if (a[t]) break; &#125; if (!a[t]) break; for (int u=t; u!=s; u=edges[p[u]].from) &#123; edges[p[u]].flow += a[t]; edges[p[u]^1].flow -= a[t]; &#125; flow += a[t]; &#125; return flow; &#125;&#125;EK;int main(void)&#123; int T,i,j,f,t,c,ca=1,n,m; scanf("%d",&amp;T); while(T--)&#123; scanf("%d %d",&amp;n,&amp;m); EK.init(maxn); for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;f,&amp;t,&amp;c); EK.AddEdge(f,t,c); &#125; printf("Case %d: %d\n",ca++,EK.Maxflow(1,n)); &#125;&#125; Dinic123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;queue&gt;#include &lt;vector&gt;using namespace std;const int INF = 0x3f3f3f3f;const int maxn = 50000;struct Edge&#123; int from, to, cap, flow; Edge(int u, int v, int c, int f):from(u), to(v), cap(c), flow(f) &#123;&#125;&#125;;struct Dinic&#123; int n, m, s, t; vector&lt;Edge&gt; edges; vector&lt;int&gt; G[maxn]; int d[maxn]; bool vis[maxn]; int cur[maxn]; void init(int n) &#123; for (int i=0; i&lt;n; i++) G[i].clear(); edges.clear(); &#125; void AddEdge(int from, int to, int cap) &#123; edges.push_back(Edge(from, to, cap, 0)); edges.push_back(Edge(to, from, 0, 0)); m = edges.size(); G[from].push_back(m-2); G[to].push_back(m-1); &#125; bool BFS() &#123; memset(vis, false, sizeof(vis)); queue&lt;int&gt; Q; Q.push(s); d[s] = 0; vis[s] = true; while (!Q.empty()) &#123; int x = Q.front(); Q.pop(); for (int i=0; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (!vis[e.to] &amp;&amp; e.cap&gt;e.flow) &#123; vis[e.to] = 1; d[e.to] = d[x]+1; Q.push(e.to); &#125; &#125; &#125; return vis[t]; &#125; int DFS(int x, int a) &#123; if (x == t || a == 0) return a; int flow = 0, f; for (int&amp; i=cur[x]; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (d[x]+1==d[e.to] &amp;&amp; (f=DFS(e.to, min(a, e.cap-e.flow)))&gt;0) &#123; e.flow += f; edges[G[x][i]^1].flow -= f; flow += f; a -= f; if (a == 0) break; &#125; &#125; return flow; &#125; int Maxflow(int s, int t) &#123; this-&gt;s = s; this-&gt;t = t; int flow = 0; while (BFS()) &#123; memset(cur, 0, sizeof(cur)); flow += DFS(s, INF); &#125; return flow; &#125;&#125;DN;int main(void)&#123; int T,i,j,f,t,c,ca=1,n,m; scanf("%d",&amp;T); while(T--)&#123; scanf("%d %d",&amp;n,&amp;m);//点数，边数 DN.init(maxn);//初始化邻接表 for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;f,&amp;t,&amp;c); DN.AddEdge(f,t,c); &#125; printf("Case %d: %d\n",ca++,DN.Maxflow(1,n));//表示源点是1，汇点是n &#125;&#125; 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>MaxFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈QuickSort]]></title>
    <url>%2F2019%2F01%2F06%2FQuickSort%2F</url>
    <content type="text"><![CDATA[快速排序快速排序作为目前我们使用的最常用的排序方式，快速排序平均复杂度的级别和大多数排序都一样为$O(n\log n)​$，但是他的常数相对较小，同时思想也是更加简单，实现过程中也不需要其他的数组进行辅助排序。无论是时间上还是空间上都可以做到极致。但是它的各个细节你真的知道吗？ 原始快速排序在我们初次接触快速排序的时候，我们通常选择给定待排序数组中的第一个元素作为标准，进行左右划分，从而进行递归。下面给出一份基于该思想的代码： 1234567891011121314151617181920212223void QuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; QuickSort(a,L,m-1); QuickSort(a,m+1,R);&#125; 可以看到我们每次都是选择待排序数组的第一个作为标准，进行左右划分的。虽然在部分情况下很好用，但是当数据不配合我们的时候，我们就要承担一定风险，例如当传过来的数组$a$原本就是增序的情况下，这份代码就会出现最糟糕的情况，这时算法需要迭代$n$轮，由于每轮的复杂度都是$O(n)$，那么相应的时间复杂度就变成了$O(n^2)$了。 下面我们讨论的就是快速排序的各种优化方法。 随机快速排序这种想法就非常直接了，听说你想造数据搞我，那我就打乱你的数据，我们为了保证数据和正常情况下的平均复杂度一样，我们最开始就不要选择第一个元素了，我们在$$L,R$$当中随机选一个，这样的话，我们几乎可以保证算法发生最坏情况是不可能的，所以通常情况下我们使用下面这一份代码就足够了： 123456789101112131415161718192021222324void RandomQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x=rand()%(R-L+1); swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; RandomQuickSort(a,L,m-1); RandomQuickSort(a,m+1,R);&#125; 可以看到我们就是新申请了一个变量$x$，存一下应该和哪个元素作为标准即可。 但是这种优化仅仅只是开始，有很多人提出了非常有意思的并且有效的想法。 中位数快速排序这个人呢，他就想啊，我们排序的时候为什么不首先找到中位数，之后使用中位数做标准呢？这样不是很简单的就能保证算法每次都从中间划分吗?于是这个人在随机化取下标的地方做了一些修改，其实也就是借用了利用快速排序找出第k大数的思想，这个的平均时间复杂度也是O(n)，于是这个人在QuickSort中，又套了一层QucikSort。代码的框架如下： 123456789101112131415161718192021222324252627int Find_Median_ID(int a[],int L,int R,int k)&#123;&#125;void MedianQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x=Find_Median_ID(a,L,R,(R+L)/2); swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; MedianQuickSort(a,L,m-1); MedianQuickSort(a,m+1,R);&#125; 从这开始，其他的思想又出现了，到此位置，以上的快速排序的理论的最坏复杂度依然为$O(n^2)$。没有真正做到$O(n\log n)$级别。 中位数的中位数快速排序-BFPRT这个人在前者的思想上又做了一些改变，想法也是很直接，既然你找中位数这么麻烦，我就要求松一点，我不找最好的中位数，我找一个接近中位数的数。于是提出了中位数的中位数这么一个有意思的东西，算法也是十分直接的，其方法是：我们每次把原数组分为按5分组，对每一组中的5个数我们使用插入、选择、堆排等排序方式进行排序，挑出每组的中位数，每次把挑出来的每组的中位数放到当前数组前面作为中位数序列。之后我们对该段进行递归排序，返回中位数。之后使用这中位数的中位数作为标准，对原数组左右进行划分。 举个例子： 我们首先把数组按照5进行划分，最后一个长度不足5的我们舍弃不考虑。经过分组之后，我们再对中位数进行排序，返回中位数的中位数大小。通过这种方式我们还真的得到了一个理论最坏情况下是$O(n\log n)$的排序算法，虽然这个算法常数较大，实际跑起来的时候可能并不如前面的算法，但是这并不影响这个算法的价值。 代码如下： 123456789101112131415161718192021222324252627282930313233343536int BFPRT(int a[],int L,int R)&#123; int i,cnt=L; if((R-L+1)&lt;5)&#123; sort(a+L,a+R+1); return a[(R+L+1)/2]; &#125; for(i=L;i&lt;R;i+=5)&#123; sort(a+i,a+i+5); swap(a[cnt],a[i+2]); cnt++; &#125; int Median=BFPRT(a,0,cnt-1); for(i=L;i&lt;R;i++)&#123; if(a[i]==Median)&#123; swap(a[L],a[i]); break; &#125; &#125; int l=L+1,m=L,r=R; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; BFPRT(a,L,m-1); BFPRT(a,m+1,R); return a[(R-L+1)/2];&#125; 代码写的很丑，但是勉强能看看，也许会有BUG 延迟中位数快速排序由于BFPRT的选择过程实在太麻烦了，于是有人在想，你这个中位数的中位数算来算去不就是一个接近中位数的数吗？那我何必这么麻烦，我们不如随机的选，只不过我选的时候计算一下，如果当前的数在$\lfloor \frac n 4 \rfloor$到$\lfloor \frac {3n} 4 \rfloor$之间，那么我就用这些数来作为标准。由于这些数有$\frac n 2$个，每次选就相当于我们抛硬币，那么期望次数就是2次。这样的话，就算我们每次选到了边缘，也没有关系。我们最坏我们的算法也会在$O(n\log_{\frac 4 3} n)$这个限制下。这真的是一个很有意思很精髓的想法。 代码如下： 12345678910111213141516171819202122232425262728293031323334void LazyRandomQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x,cnt; while(true)&#123; x=rand()%(R-L+1); cnt=0; for(int i=L;i&lt;=R;i++)&#123; if(a[i]&lt;=a[L+x]) cnt++; &#125; if((cnt&gt;=(R-L+1)/4)&amp;&amp;(cnt&lt;=(3*(R-L+1))/4))&#123; break; &#125; &#125; swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; LazyRandomQuickSort(a,L,m-1); LazyRandomQuickSort(a,m+1,R);&#125; 去重复数字的快速排序我们在写快速排序的时候，其实很多时候都浪费了计算资源，比如说我们之前我们每次进行子问题递归的时候，区间都是$L,m-1$和$m+1,R$但是我们这样做的话，对于大量的重复数字出现的情况下，通常会浪费计算资源。但是这种情况我们原本是可以避免的，而实现的想法也十分简单。简单来说：我们每次在调整的过程中我们希望与标准相同的数集中起来，之后递归的时候再精简一下区间即可，这个改进的的确确是非常非常有用的。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748void BoundQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,BoundL,BoundR; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; BoundL=m-1; BoundR=m+1; l=L,r=R; while(true)&#123; while(a[l]!=a[m]&amp;&amp;(l&lt;BoundL)) l++; if(a[l]!=a[m]) l++; while(a[BoundL]==a[m]&amp;&amp;(BoundL&gt;L)) BoundL--; if(l&gt;=BoundL)&#123; break; &#125; else &#123; swap(a[l],a[BoundL]); &#125; &#125; while(true)&#123; while(a[BoundR]==a[m]&amp;&amp;(BoundR&lt;R)) BoundR++; if(a[BoundR]==a[m]) BoundR++; while(a[r]!=a[m]&amp;&amp;(BoundR&lt;r)) r--; if(r&lt;=BoundR)&#123; break; &#125; else &#123; swap(a[r],a[BoundR]); &#125; &#125; BoundQuickSort(a,L,BoundL); BoundQuickSort(a,BoundR,R);&#125; 总结虽然快速排序已经是多年前的算法，但是其思想仍然值得我们仔细思考，细细品味。 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>QuickSort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binomial堆]]></title>
    <url>%2F2019%2F01%2F03%2FBinomial%2F</url>
    <content type="text"><![CDATA[Binomial 堆是什么？首先我们知道Binomial这个单词的意思是二项式，但是为什么取个名字呢？也许读完本文你就明白了。 二叉堆首先我们回顾一下二叉堆，对于节点数为$n$的它，具有下面几个基本性质 树高$\lceil \log n \rceil$ 插入节点时间复杂度$O(\log n)$ 访问根节点(优先级最高的节点)时间复杂度$O(1)$ 删除根节点时间复杂度$O(\log n)$ 合并两棵树时间复杂度： 自底向上合并时间复杂度为$O(n)$ 自顶向下合并时间复杂度为$O(n\log n)$ 虽然说二叉堆具有查找上极其优良的性质，但是在合并的时候（虽然一般情况下用不到合并这个操作），时间复杂度却没有达到$O(\log n)$级别。这就使得有些人在想，我们可不可以放松一下访问根节点的时间到$O(\log n)$，但是加速合并两棵树呢？于是Binomial 堆就被提出来了，同时比预想的更加强大！ 思想由于我们要加速合并，那么肯定是有好几棵树同时存在才需要合并，所以Binomial 堆的本质思想就与二叉堆有很大不同。Binomial 堆是由森林构成的，而二叉堆是一棵树。所以这也导致了合并这个需求在Binomial 堆中是如此的重要。 但是Binomial 又是什么意思呢？直观上我们只知道Binomial 表示$2^0,2^1,2^2,…,2^n$这样的数目。但是这和森林有什么关系呢？实际上这恰恰是Binomial 堆的精髓所在，它表示的是森林中每棵树的节点数为$2^k$这么多，同时我们也称当前这棵树为$B_k$。本文中介绍小根Binomial 堆，如果不懂什么是小根，那么请移步我的另一篇文章。 Binomial 堆简单的例子，表示的是0阶，1阶，2阶，3阶，4阶，5阶树的结构： 一般的结构形式： 注意: 通过上面的描述，我们可以对每棵数得到几个很明显的性质： $|B_k|=2^k$ $height(B_k)=k$ $degree(B_k)=k$ 对于一棵树的每一个节点来说，其第$i$个儿子的度为$i-1$ 对于总节点为$n$的Binomial 堆而言，其具有以下几个性质： 最多有$\lfloor \log n \rfloor +1$棵树，很明显，其实就是n在二进制下有多少个1就有多少棵数 树高最高为$\log n$，因为二进制下最高位一定为$\log n$位 也许这里会有疑惑，但是请务必仔细想想为什么会有这种性质。 合并条件-Union我们规定Binomial 堆中不允许出现结构相同的两棵数，换句话说，假如森林中出现了阶数相同的两棵树，我们需要进行合并。我们合并的方式也很简单，将根节点的值更大的树作为儿子连上根节点的值更小的树。 一个简单的例子： 请注意这是一个递归过程。我们每次合并之后都需要再检查，其实这里本质上就是二进制的进位操作。由于最坏的情况下我们需要进位$\log n$次，所以这一步的复杂度最坏为$O(\log n)$。但是这一步均摊下来其实是很快的，因为其实二进制的进位操作并不会每次都是最坏情况，而且也不可能出每次都是最坏情况，这一步经过证明我不会，平均的时间复杂度为$O(1)$。 插入节点-Insert我们在插入一个节点的时候，想法也是非常简单的，我们首先将其视为$B_0$（也就是单个节点的树），并将其加入到森林中，之后我们使用上一步讲到的合并条件，该步复杂度与合并条件相同,时间复杂度为$O(1)$。 提取最小值-Top提取最小值，我们采用最简单的方法，对森林中每棵树的根节点进行一次遍历，找到根最小的节点的值返回即可。由于最多有$\lfloor \log n \rfloor +1$棵树，故时间复杂度为$O(\log n)​$。 删除最小值-Pop首先我们通过上一步的方法，定位到最小值点在森林的哪一个树中，之后我们将根节点移除，那么此时它应该分裂成了一些更小的树，由于$B_k$这样的一棵树可以分裂为$B_0,B_1,B_2,…,B_{k-1}$这样的一些子树，由于$k$最大为$\log n$,故我们最多插入$\log n$次，每次插入的时间为$O(1)$，故该步的时间复杂度为$O(\log n+\log n)=O(\log n)$。 对比 二叉树 Binomial Union O(n) O(1) Insert O(log n) O(1) Top O(1) O(log n) Pop O(log n) O(log n) 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉堆]]></title>
    <url>%2F2019%2F01%2F02%2Fheap%2F</url>
    <content type="text"><![CDATA[什么是二叉堆？二叉堆是什么呢？我的理解是堆是一个数据结构，为了满足我们的需求所精心设计的数据结构。而二叉堆正是为了满足我们的需求而被提出来的，同时它也叫做优先队列，它所能实现的最重要的功能是： 在$n$个元素中通过$\log n$时间找到优先级最大/最小的元素 我们传统的方式是遍历数组去寻找，这样需要花费$n$级别的时间，那么对于大量查询优先级最大/最小的元素情况下，我们使用二叉堆进行优化可以直接把复杂度的级别下降。 大根堆/小根堆大根堆和小根堆是我们最常用的二叉堆的结构，相应的，它们是为了满足每次查询优先级最大/最小的元素而分别设计的。它们的本质思想都一样，故在本文中我们会详细介绍小根堆，同时本文中的元素定义为int类型的数值。 结构 小根堆是一颗完全二叉树，请注意这一点，因为这一步保证了我们每次查询的复杂度为$\log n$ 小根堆的父亲节点的数值都小于其儿子节点 例如： 这就是一个典型的小根堆，而其右边的就是其使用数组存储的方式，为了简单，我们通常使用数组版本的，当然指针版本的更加优越，但是本文都是基于数组实现的。 注意： 这里有个很简单的技巧，对于每个节点号为$id$的节点而言，其左儿子是$2id$，右儿子是$2id+1$ 同时对于节点号为$id$的，其父亲节点编号为$id/2$ 我们所需要完善的几个函数对于一个堆，我们只需要满足三个函数即可 节点的插入Insert(int x) 根节点的访问Top() 根节点的删除Pop() 但是其实有难度的只有第一个和第三个功能。 请关注为什么函数的名字为什么与队列函数类似。 Insert我们进行插入操作很简单，简单来说只有两步，假如我们需要插入$x$： 首先将$x$放置在数组的最后，这一步保证了插入之后仍然是一棵完全二叉树 向上调整$x$的位置，倘若比其父亲小的话，进行交换，直到无法再进行调整。 一个例子： 左边是首先插入到最后一个位置，之后向上调整到合适的位置。显然，我们插入最多向上交换$\log n$次，故复杂度为$O(\log n)$ Top根节点的访问就很简单了，只要访问数组的第一个位置就行了。故复杂度为$O(1)$ Pop我们进行根节点的删除操作也很简单，简单来说，只需要三步： 将根节点的值和最后一个节点的值交换 删除最后一个节点 调整根节点的位置，每次和最小的儿子进行交换，直到无法进行交换。 请特别注意第三个步骤，并好好想想为什么一定要这样做。 一个例子： 首先和最后一个节点进行交换，之后删除，最后调整根节点的位置。由于交换后的节点最多也是向下交换$\log n$次，故复杂度为$O(\log n)$ 小根堆的代码实现，数组版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include&lt;bits/stdc++.h&gt;using namespace std;int heap[1000005],size=0;void Insert(int x)&#123; int fatherid,now; size++; heap[size]=x; now=size; while(now&gt;1)&#123; fatherid=now/2; if(heap[fatherid]&gt;heap[now])&#123; swap(heap[fatherid],heap[now]); now=fatherid; &#125; else break; &#125;&#125;int Top()&#123; if(size) return heap[1]; else printf("NO element, can't get top\n"); return -1;&#125;void Pop()&#123; int leftsonid,rightsonid,now; if(size)&#123; swap(heap[1],heap[size]); size--; now=1; while(now&lt;=size)&#123; leftsonid=now*2; rightsonid=now*2+1; if(leftsonid&lt;=size&amp;&amp;rightsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[rightsonid])&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; if(heap[rightsonid]&lt;heap[now])&#123; swap(heap[rightsonid],heap[now]); now=rightsonid; &#125; else break; &#125; &#125; else if(leftsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; break; &#125; &#125; &#125; else printf("NO element, can't pop\n");&#125;int main(void)&#123; int i,j,n,m; n=100; for(i=0;i&lt;n;i++)&#123; Insert(rand()); &#125; while(size)&#123; printf("%d\n",Top()); Pop(); &#125;&#125; 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CF1091E-New Year and the Acquaintance Estimation]]></title>
    <url>%2F2018%2F12%2F31%2FCF1091E%2F</url>
    <content type="text"><![CDATA[题目大意给我们$n$个点的度数$a_1,a_2,…,a_n$,同时还有一个点的度$x$尚未确定。我们希望$a_1,a_2,…,a_n,x$构成的点度序列能够形成一个简单图，所谓简单图就是无自环，重边的无向图，连通性无需保证。要求从小到大输出$x$的所有可能解。若没有任何解输出$-1$。 基本知识 Havel–Hakimi algorithm 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$a_2-1,a_3-1,…,a_{a_1+1}-1,a+{a_1+2},..,a_n$$构成一个简单图。即我们可以根据这个进行递归。 人话解释：每次拿一个点度最大的$a_1$出来，将剩余的前$a_1$个点度减一即可。注意，这里每次都是要排序的。故递归求解的话时间复杂度是$n^2logn$的 Erdős–Gallai theorem 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$\sum_{i-1}^ka_i&lt;=(k(k-1)+\sum_{j=k+1}^n\min(a_j,k))$$同时点度和和需要为偶数：$$\sum_i^na_i=even$$这个定理就比较晦涩了，而且很强大，直接不需要递归了，我们只需要对每个$k$维护一下值就行了。这个判断一次的复杂度就直接变成了$n$级别的复杂度。故本题只用这个结论 题目想法很显然，我们只需要找一个上界和一个下界，中间的点度是一个公差为2的等差数列。现在问题变成了怎么找到上界和下界。 下界我们通过二分的方式找下界，我们在使用Erdős–Gallai theorem定理的时候需要对每个$a_i$进行检查。很显然，我们二分$x$为$M$的时候，我们可以得到如下几种情况： 所有$a_i$都满足条件，那么我们很确定的说下界在左边，此时我们令$R=M$ 存在$a_i$不满足条件，那么我们再分为两种情况 $a_i&gt;x$ 同时不满足条件，这个时候我们可以观察Erdős–Gallai theorem定理，很轻松可以想到我们此时应该增加$x$才能使$a_i$满足条件，故此时应该令$L=M+1$ 相应的，$a_i&lt;x$同时不满足条件的情况下，应该令$R=M$ 通过上面几种情况的考虑，我们可以轻松求出下界。 上界由于我们此时知道下界，那么我们求上界就是一个基本的二分了，找第一个不满足的$M$即可。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;bits/stdc++.h&gt;using namespace std;int a[500005],b[500005],N;long long pre[500005];int cmp(int x,int y) &#123; return x&gt;y;&#125;int check(int x) &#123; long long i,res,cur,n,pos; n=N; if(x&gt;n) return 1; for(i=0;i&lt;n;i++)&#123; b[i]=a[i]; &#125; b[n]=x; n++; sort(b,b+n,cmp); res=0; for(i=0;i&lt;n;i++)&#123; pre[i]=b[i]; if(i) pre[i]+=pre[i-1]; &#125; pos=n-1; for(i=0;i&lt;n;i++)&#123; res+=b[i]; cur=(i+1)*i; while(pos&gt;=0&amp;&amp;b[pos]&lt;=(i+1))&#123; pos--; &#125; if(pos&gt;i)&#123; cur+=pre[n-1]-pre[pos]; cur+=(i+1)*(pos-i); &#125; else &#123; cur+=pre[n-1]-pre[i]; &#125; if(res&lt;=cur) continue ; if(b[i]&gt;x) return 0; return 1; &#125; return 2;&#125;int main(void) &#123; int i,x,L,R,M,T,ans,n,all; T=0; scanf("%d",&amp;N); n=N; for(i=0; i&lt;n; i++) &#123; scanf("%d",a+i); T+=(a[i]&amp;1); &#125; T%=2; L=0; R=5e5; ans=-1; while(L&lt;R) &#123; M=(L+R)/2; all=check(T+M*2); if(all==1) &#123; R=M; &#125; else if(all==2)&#123; ans=M; R=M; &#125; else &#123; L=M+1; &#125; &#125; if(ans==-1) &#123; printf("-1\n"); return 0; &#125; L=ans; R=5e5; while(L&lt;R) &#123; M=(L+R)/2; if(check(T+M*2)==2) &#123; L=M+1; &#125; else &#123; R=M; &#125; &#125; for(i=ans; i&lt;L; i++) &#123; printf("%d ",T+i*2); &#125; printf("\n");&#125;]]></content>
      <categories>
        <category>codeforces</category>
      </categories>
      <tags>
        <tag>graph-theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型详述]]></title>
    <url>%2F2018%2F12%2F26%2FHMM%2F</url>
    <content type="text"><![CDATA[什么是隐马尔可夫模型？隐马尔可夫模型实际上是一个双重的随机过程，我们不知道具体的状态序列，只知道转移的概率，即模型的状态转移过程是未知的，而观察事件的随机过程是状态转换过程的随机函数，即我们希望通过可见的事件变换去预测深藏在其背后的本质规律。 请记住上述提到的几个概念： 状态序列(这是我们需要去预测的) 观察序列(这是我们已知的) 状态序列的转移概率(这是我们已知的) 状态序列对观察序列的转移(这是我们已知的) 隐马尔可夫链成立的三个假设（当然可以不需要理解） 状态构成一个隐马尔可夫链$$P(q_i|q_{i-1}q_{i-2}…q_1)=P(q_i|q_{i-1})$$ 不动性假设（状态与具体时间无关）$$P(q_{i+1}|q_i)=P(q_{j+1}|q_j)$$ 输出独立性假设$$P(o_1..o_i|q_1…q_i)=\prod_tP(o_t|q_t)$$ 其实这里的隐马尔可夫链中的链就表示假设了是一阶的！我们假设满足上面的性质。 现在我们考虑一个场景假如我们面对一堆过去的数据，过去的数据当中只有自然生长下的海藻的每天的状态。 我们也有一些最近的数据，最近的数据中有海藻的状态和每天的天气情况。 现在我们给定天气的转移矩阵，天气对海藻状态影响的矩阵。我们希望根据这些数据，去预测过去的天气。 海藻的状态只有4种： 干 稍干 潮湿 湿润 天气的状态只有3种： 晴 阴 雨 天气状态转移矩阵（后面简称为A）： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，明天的天气是雨，阴，晴的概率分布。 天气对海藻干湿的影响(后面简称为B): 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，今天的海藻出现干，稍干，潮湿，湿润的概率分布。 建立模型我们的任务是根据观察序列去推测状态序列。 首先我们根据之前的信息：构建状态集合$s$，状态转移矩阵$A$，初始状态概率分布$\pi$，观察集合$ss$，状态对观测的影响矩阵$B$ 对于我们这个题目而言，状态集合$s$为(晴，阴，雨)，观察集合$ss$为(干，稍干，潮湿，湿润)，请务必注意这个地方！ 那么我们可以得到五元组模型:$(s,ss,\pi,A,B)$，通常我们将模型简写为三元组:$\lambda =(\pi,A,B)$。 对于上述三元组，只有$\pi$是未知的，这个东西怎么统计出来呢？其实很简单，由于我们包含最近的数据，我们根据最近的天气可以统计出来每种天气出现的概率，那么这个分布就是$\pi$，公式表示为：$$\pi_i=P(q_1=s_i)$$当然这仅仅是对于这个任务是这样计算的，别的任务可能会不一样。上述公式表示的就是说第$i$种情况出现的概率 比如说统计后大概长成这个样子： 晴 雨 阴 P 0.5 0.05 0.45 求解过程我们现在已知观察序列$O=o_1….o_n$，模型$\lambda =(\pi,A,B)$ 。现在我们需要求给定模型与观察序列的情况下求状态序列$Q$。有我们需要以下概率最大：$$P(O|Q,\lambda )$$意思也很简单啊，就是我们求这个条件概率，当前模型$\lambda ​$来说，希望找到一个状态序列$Q​$使得观察序列O发生的可能性最大。 我们假设$Q=q_1…q_n$ 我们把上面的式子打开，有：$$P(O|Q,\lambda )=\pi_{q_1}A_{q_1q_2}A_{q_2q_3}…A_{q_{n-1}q_n}B_{q_1o_1}B_{q_2o_2}…B_{q_no_n}$$对上面的式子我们很简单的可以想到枚举每一种$Q$，对每一种都进行计算，之后输出使得$P(O|Q,\lambda )$最大的那个$Q$就可以了。但是这当然是不可以的，因为这个是指级别的，$n$大的话这个方法就凉了。 解释一下上面的公式： $\pi_{q_1}$表示最初天气为$q_1$成立的概率 $A_{ij}​$表示天气从$i​$到$j​$转移的概率 $B_{ij}$表示天气为$i$的情况下海藻状态为$j$的概率，也叫发射概率 viterbi算法维特比算法分为前向后向，但是我感觉会一种就行了，其实都是一样的。这里介绍一下前向算法。 前向算法思想：假设我们已知了对于$o_1…o_{n-1}o_n$最优的状态序列肯定是$o_1…o_{n-1}$的一个序列转移过来的，等于说我们对当前的一层计算来说，对于每个节点只需要考虑前面一层的结果就可以了。即有$$dp_{iq_i}=\max_{k\in s} dp_{(i-1)k}A_{k(q_j)}B_{q_jo_i}$$$dp$表示计算的结果，$q_i$表示当前层的状态，$k$表示枚举前一层的状态，从观察状态集合中枚举。$max$表示存储后面式子的最大值。 算法流程： C++的代码，写起来很简单，每种语言都可以按照这个方式写。但是为了保证精度，我们一般是取$log$的，这份代码的结果应该会很差！ 123456789101112131415161718192021void viterbi(int O[],double PI[],double A[][],double B[][])&#123;//传观察序列和模型 int i,j,back; int path[][]； double dp[][],MX； for(i=1;i&lt;=n;i++)&#123;//初始化 dp[1][i]=PI[i]; &#125; for(i=1;i&lt;=m;i++)&#123; for(j=1;j&lt;=n;j++)&#123; MX=-1; for(k=1;k&lt;=n;k++)&#123;//枚举前一层 if(MX&lt;dp[i-1][k]*A[k][j])&#123; MX=dp[i-1][k]*A[k][j];//找到最大的 back=k; &#125; &#125; path[i][j]=back;//用于回溯 dp[i][j]=MX*B[j][O[i]];//计算结果存起来 &#125; &#125;&#125; 一个简单例子为了结果看起来比较正常，我们把初始分布$\pi$ 设的极端一点，同时我们再次召唤之前的表。 晴 阴 雨 P 1 0 0 天气状态转移矩阵$A$： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 天气对海藻干湿的影响$B$: 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 对于我们给定的观测序列：干，潮湿，湿润。我们计算的结果入上图所示。其中红色括号的结果就是表示结果是上一次的第几个节点过来的。 到此为止，隐马尔可夫模型的这个例子就介绍完了，当然问题有其他变种，一般是在viterbi算法上动刀，不会解决的话可以留言评论。 A和B矩阵怎么求？我们大部分时候，其实只有训练数据和测试数据，没有这些转移矩阵，这个时候我们可以通过最大似然估计来求这些矩阵(看起来吓人)。就好比这一题我没有给出矩阵$A,B$的话怎么办？ 其实我们也可以做出来这个题目，因为我在题目中特意强调了我们还有一些最近的标注数据，这时候我们可以通过最近的数据把矩阵$A,B$给估计出来，做法如下。 其实就是统计一下就行了，对于A矩阵：$$A_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的后面连接了天气状态为$j$的次数除以天气状态为$i$的出现总次数。 对于B矩阵：$$B_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的对应海藻的状态为$j$的次数除以天气状态为$i$的出现的总次数。 我们在求的时候一般加上松弛操作，即$$A_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ $$B_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ 一个小项目为了使本文有一点实用价值，我把我的代码放在这里。一个序列标注的小东西。 参考资料国科大自然语言处理课件 胡玥]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformer模型详解]]></title>
    <url>%2F2018%2F12%2F24%2FAttention%2F</url>
    <content type="text"><![CDATA[背景目前深度学习中用于做NLP的方法，大多都是首先将句子进行分句，之后将每个单词使用与训练好的词向量进行表示(其实这就是一种迁移学习？)，通过这一步我们把一个句子转化为向量的序列。这样的好处是我们可以把一个句子使用一个向量模型来表示，即每个句子我们都对应一个矩阵$x=(x_1,x_2,…x_n)$其中$x_i$表示第$i$个词的词向量，通常我们记为行向量，假如预训练好的向量维度为$d$,也就是说一个句子我们可以映射为$x\in R^{n*d}$。通过这个操作，我们可以将语言化作向量表示，这是一个很好的建模方式。 我们为了处理这些信息主要分为以下几个思路： RNN 可以很好的获得时序信息 CNN 可以很好的获得全局信息 Attention google提出的全新思想，在各大模型上都有提升 RNN第一个思路是RNN，思想是很简单的，进行递归式的信息提取：$$y_t=f(y_{t-1},x_t)$$由于我们在每一步进行计算的时候都需要依赖上一步计算的结果，这导致了RNN无法并行计算，这是这个模型原本就存在的缺陷，但是RNN可以很好的学习到时序信息。相应的，我们很难获取到全局信息，倘若我们想得到全局信息，我们一般使用双向RNN或者双向LSTM。 CNN第二个思路是CNN，思想也是十分简单的，进行窗口式的扫描，对于尺度为3的卷积：$$y_t=f(x_{t-1},x_t,x_{t+1})$$由于CNN的每一步进行计算的时候是独立的，故我们可以很方便的进行并行计算。其实通过这一步我们只能获取到局部的信息，但是我们通过堆叠的方式增大感受野，使得模型能够获得很好的全局信息！但是相应的，CNN的时序信息获取效果不如RNN。 AttentionGoogle在2017年的论文Attention is All You Need提供了第三个思路，这个想法也也是十分简单的，一般而言我们记作：$$y_t=f(q_t,K,V)$$对于全局过程我们记作：$$y=f(Q,K,V)$$其中$Q,K$与$V$相同的时候，我们称作self-attention。计算方式在文章后续会详叙。 Transformer注意力机制 上图看起来十分复杂，我们简化一下。 这其实就是一个Seq2Seq模型，我们左边把一个encoder输入进去，右边就decoder得到输出。由于上图有两个部分，我们不妨将其中的的部分再打开看看： 我们随之而来的问题就是这个encoder是如何把信息传递给右边的呢？因为这个图和上面的压根就不一样好吗？我们再将图片打开拓展: 意思就是说对于每一层的encoder的输出，我们会和decoder的输入进行结合。 我们再取一种的一层做详细展示： 我们可以看到在解码层的做了两次attention，我们在第二次的attention时使用了左边的encoder传递过来的信息。上面这张图我觉得有点小问题，也许是我理解错误。。我认为左边的encoder端上面应该是没有箭头的。 传统Attention 机制定义Attention的翻译过来就是注意力，这表示了人类的偏好，我们在观察一个图片的时候，往往对图像的某一部分有更集中的偏好，就是所谓的”抢镜”。对某一部分有更高的关注度，这使得我们对图片有一个更加准确的感受。 我们拓展到文本中，例如我们在进行翻译任务的时候，翻译当前词的时候一般是对序列的局部信息有偏好，但是对于每个单词而言，对原序列的关注程度肯定是不一样的。attention就是使用最简单的方法实现这个功能。 计算方式我们再次召唤前面的公式:$$y_t=f(q_t,K,V)$$我们简述一下这个$f$是如何进行计算的。 我们输入一组$K$与$V$与查询$Q$，首先我们对每个$K_i$与$Q$计算相似度得到$S_i$,之后将$S$通过softmax函数进行归一化得到分布$a$，之后我们计算$a$与$V$的加权和得到对于查询$Q$的attention向量Att-V。 其中对于$f(Q,K_i)$(都为列向量的话)的计算方式主要分为以下几种： 点乘 dot product : $f(Q,K)=Q^TK$ 权重 general : $f(Q,K)=Q^TWK$ 拼接 concat ：$f(Q,K)=W[Q^T;K]$ 神经网络 perceptron ：$f(Q,K)=V^T\tanh (WQ+UK)$ 对于$Q$,$K$与$V$相同的情况下，我们也称作为自注意力机制，希望寻找文本中内在的联系。也就是说，在序列内部做Attention，寻找序列内部的联系。Google论文的主要贡献之一是它表明了内部注意力在机器翻译（甚至是一般的Seq2Seq任务）的序列编码上是相当重要的。 传统模型有一个非常明显的缺点，就是无法获得时序信息，就算我们把顺序打乱，我们算出来的结果一样是不变的，这个就有点不太合理了！当然我们一般使用attention去辅助RNN和CNN，由于RNN和CNN已经包括了时序信息，可能会好一些。 transformer中的Attention机制在transformer当中的Attention机制与传统的attention机制还是有很大区别的。分别叫做 Scaled Dot-Product Attention 和 Multi-Head Attention。 Scaled Dot-Product Attention其结构图如下图所示： 首先第一个问题,$Q,K,V$从哪里来？按照我的理解，对于我们输入的句子$x=(x_1,x_2,…x_n)$，这一步可以是原本的词向量，也可以是对于输入的词向量做线性变化，例如$Q=xW^q,K=xW^k,V=xW^v$，例如： 我们对于input的词向量Thinking Machines，通过三个矩阵$W^q,W^k,W^v$，得到$Q,K,V$,于是我们依次遍历$Q_i$，计算每个的$Q_i$的注意力向量。 首先对于$q_1$,我们采用dot product进行计算相似度$S_i$ 之后我们将$S_i$ 除以$\sqrt{d_k}$其中$d_k$表示$K$的维度，加上这个是为了防止内积太大，如果内积过大，会导致softmax进入饱和区，就没有注意力这个作用了。接下来我们通过softmax计算概率分布。 最后我们计算加权和得到对于$q_1$的注意力向量$z_1$。直以递归下去，我们可以对每个$q$进行相同的操作即可。听上去这一步好像很麻烦，其实我们用矩阵表示的话，就很简单且简约了： 首先输入词向量$X$(行向量)，计算$Q,K,V$ 这一步就更加直观了，但是我们需要好好理解一下$QK^T$ 这一个计算过程，这一步实际上是计算了一个word2word的attention矩阵。例如我们对”I have a dream”计算的话 其中每个格子$grid_{ij}$ 表示的是第$i$个单词和第$j$个单词的相似度，这肯定是一个对称矩阵啦！现在我们回到transformer的结构图当中，可以很明显的看到有的self-attention前面加上了masked，这又是什么意思呢？ 简单来说就是为了防止程序看见未来的信息，而用灰色区域(0.0)覆盖上。 Multi-Head Attention多头的意思就很简单了，就是将工作重复做几次而已。论文中倒是画的很吓人的样子。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。 我们重复几次，将得到的$Z$矩阵进行拼接即可 首先我们对输入的$X$进行计算多次 将计算完的结果拼接$Z_0+Z_1+….+Z_n$，之后使用一个线性变换为指定维度的$Z$ 整个的框架如下： Position Embedding然而，我们经过思考之后，可以发现这个计算方式和传统的一样啊，我就算位置变了，对attention的计算结果不变。这个问题就很严重了，很有可能我们在机器翻译的任务当中，我们确实算出来了翻译结果应该包含了哪些单词，但是结果很有是乱序的，那么就没法用了。transformer为了解决这个问题，祭出了Position Embedding这个东西。Google是直接给出了一个公式来构造Position Embedding。$$PE_{2i}(p)=\sin(p/10000^{2i/d_{pos}})\PE_{2i+1}(p)=\cos(p/10000^{2i/d_{pos}})$$这里的意思是将position为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。Google在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的。那么为了减少计算复杂度，我们何必自己再训练呢? 原文中提到采用这个公式的另一个原因的是sin和cos，满足一些良好的性质:$$\sin(a+b)=\sin(a)\cos(b)-\cos(a)\sin(b)$$$$\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$$ 这使得位置为p+k的向量可能可以使用位置为p的向量线性表出。$$PE_{2i}(p+k)=\sin((p+k)/10000^{2i/d_{pos}})=$$$$\sin(p/10000^{2i/d_{pos}})\cos(k/10000^{2i/d_{pos}})-\cos(p/10000^{2i/d_{pos}})\sin(k/10000^{2i/d_{pos}})$$ 由于$k$是定值，而$\sin(p/10000^{2i/d_{pos}})$和$\cos(p/10000^{2i/d_{pos}})$确实是知道的，所以确实为线性表出提供了可能性吧。。 我们在input输入的时候加入位置信息，同时我们在decoder的输入过程中也加入了位置信息。 Position-wise Feed-forward Networks在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$值得注意的是encoder的每一层的$W_1$和$W_2$都不相同。 The Residuals由于transformer的结构十分复杂，训练的时候很容易出现梯度消失，导致网络难以训练，文章采用的思想借鉴了何恺明的残差网络的思想，在每一次操作之后为了保证损失能够尽量回传，每层的输入会和输出进行叠加。 其中的Add&amp;Normalize就是叠加过程。公式记作：$$output=LayerNorm(x+Sublayer(x))$$但是这也带来一个小问题，就是每次sublayer输出的结果的为维度和原输入的维度应该一样，带来了一些限制。 Encoder在文章中encoder有6层，而其中的每一层包含两个sub-layer 第一个sub-layer是多头自注意力机制，用来计算文本内部的关联 第二个是全连接Position-wise Feed-forward Networks Decoder文章中decoder也是6层，其中的每一层包含三个sub-layer 第一个sub-layer是masked多头自注意力机制，原因很简单啊后面的结果还没生成呢！ 第二个sub-layer是多头注意力机制，将decoder和encoder结合，注意计算的时候$K,V$是由encoder提供的，decoder提供的是$Q$ 第三个是一个全链接网络Position-wise Feed-forward Networks The Final Linear and Softmax Layer 最后一层就比较简单了，简单来说就是首先把decoder的第6层输出经过一个神经网络对每个单词的可能性计算一个权值，再经过一个softmax是为了反向传播的时候有一个误差回传，使得整个网络能够训练。 这里还需要注意的是，每次输出的结果，都要像RNN一样作为输入传给decoder，依次输出结果。 transformer动画演示过程首先encoder的过程 接着就是decoder的过程 参考文献jalammar’s blog 苏剑林《《Attention is All You Need》浅读（简介+代码） 》]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simplex-单纯形算法介绍]]></title>
    <url>%2F2018%2F12%2F18%2FSimplex%2F</url>
    <content type="text"><![CDATA[单纯形算法（Simplex）有什么用？谈到单纯形算法，我们不得不提线性规划，所谓线性规划，就是在满足一定约束下优化目标函数。下面引用几个例子来进行简单介绍。 Food Energy Ca Price 燕麦 110 4 3 牛奶 160 8 9 猪肉 260 14 19 现在我们需要55个单位的Ca和2000个单位的Energy，问我们应该怎么购买最合适。这就是一类最经典的线性优化问题，我们可以很轻松的写出目标函数和约束条件： ​ $min$ $$3X_1+9X_2+19X_3$$ ​ $s.t.$ ​ $110X_1+160X_2+260X_3&gt;=2000$ ​ $4X_1+8X_2+14X_3&gt;=55$ ​ $X_i&gt;=0，i=1,2,3$ 上述不等式表示的含义很简单，$min$表示最小化后面的目标函数，$s.t.$ 表示subject to的缩写，意思是受限于。 针对这类问题，我们曾经学过的方法是图解法，画出可行域（阴影部分）（a），之后使用目标函数(b)在可行域上移动，之后靠直觉确定最优解。 但是计算机可没有人类的直觉！计算机求解这些问题的时候，需要更加通用的方法来求解。于是在二战时期，为了协助政府协调物资人员，前苏联的坎托诺维奇强行提出了Simplex算法。其实该算法的思想也是来自于图解法，我们观察上面的求解过程，可以很明显的发现，最优解一定是在顶点上的！！！！于是这个人就在想，我们是不是随便从一个顶点开始，之后每次迁移到这个顶点的相邻顶点上，对这个凸包每个点遍历一次就能得到答案了？事实确实也确实是这样的，而且可以更简单，实际上我们只需要随便从一个顶点开始，之后暴力这个点相邻的点，然后选择一个下降的方向迁移点就可以，直到当前的点的值最小，那么这个点就是最终的解。 但是为什么可以这样做呢？按照我的理解，因为可行域是一个凸多边形(为什么是凸的？其实画一画，就知道了。不可能是凹的，可行域一定是凸的！！)，那么这个凸多边形的顶点结果是单调的，因为全局最优解只有一个。那么求解过程就好像下山一样，只要每次向下走就可以走到谷底，类似于梯度下降法在凸二次函数上每次沿着下降的方向走，一直迭代就能得到最优解。当然这些是在扯淡，实际上这玩意是被证明了的，而我没看。。。 单纯形算法适用的情况标准的线性规划格式（也叫标准型）： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A*x&lt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$ 其中$c$表示的就是每一类$x$的花费，$A$表示的是约束矩阵，$x$表示的是擦书的列向量，$x_i$表示的是每一类的量(针对于整数线性规划，要求$x_i$必须为整数，比如$x_i$表示的是人的数量或者物品的份数) 但是有些时候我们可能碰到一些要最大化的最优化问题，比如我们固定钱数，怎么样买到的能量最多？这时候我们的优化目标变成了$max$，针对这类问题我们转换为标准形式的方式也很简单，$c*-1$ 就变为$min$ 了，同样的道理我们对$A$和$x$都可以这样干。 目标函数是$max$的时候，我们将$c$取反 $Ai*x&gt;=b$,我们将$A_i$和$b$取反 $x&lt;=0$,我们将$x$取反 松弛型我们上面介绍了标准型，现在介绍一下松弛型，其实也是一个很简单的东西，由于我们上面都是不等式，实在是太烦了，于是我们想要把不等式优化成等式，于是我们将标准型构造成下面的形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A_i*x+x_{i+n}=b_i$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中$m$表示约束的个数，由于之前的不等式都是$&gt;=$的情况，我们可以减去一个非负的变量使得等号成立。这就是松弛形。 单纯形现在介绍单纯形，单纯形其实就是从松弛型过来的，它是为了单纯形算法求解简便一些而存在的，因为它可以导出单纯形表，一般写成如下形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $x_{i+n}=b_i-A_ix$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中我们称左边的变量为基本变量，右边的变量称为非基本变量，我们很显然有一组基础解，就是令非基本变量为零，这时候基本变量的值都为其对应的$b_i$。我们一般情况下可以认为基础解就是前面说的可行解区域的一个顶点（因为原本就是边界）。 而单纯形表的初始形式很简单，举个例子 ​ $min$ $-x_1-14x_2-6x_3$ ​ $s.t.$ ​ $x_1+x_2+x_3+x_4$ $=4$ ​ $x_1$ $+x_5$ $=2$ ​ $x_3$ $+x_6$ $=3$ 当前的例子的基本变量为${x_4,x_5,x_6}$,非基本变量为${x_1,x_2,x_3}$。 其单纯形表的形式为: 0 -1 -14 -6 0 0 0 4 1 1 1 1 0 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 人话解释，第一行表示的是目标函数的系数，但是其中第0个位置表示的是当前目标函数取当前基础解作为解之后求得的值的相反数。后面的每一行表示的都是约束条件$b_i=a_{i1}x_1+a_{i2}x_2+…+a_{i(n+m)}x_{n+m}$ 求解步骤我们在使用单纯形法进行求解的时候，首先找到第一个目标函数中系数为负的非基本变量$y$，将其增大（这个过程相当于我们在凸包上沿着边缘走到另一个顶点），但是我们怎么确定这个非基本变量$y$最大能增大多少呢？这个其实很简单，只需要令除当前变量的其他变量为0，剩下$b_i$和$y$，我们可以对每个包含$y$的约束条件进行计算约束。找到使其最紧(即在当前的约束条件中让$y$的最大取值最小)的那一个约束即可，再使用当前约束条件下的基本变量与其进行替换即可。由于我们每次可以使约束z向更小的方向迁移，这使得我们的算法不会陷入死循环。 注意，假如对于非基本变量$y$不存在限制最紧的约束条件，那么该组线性规划无解，因为可以无限增大。 再次注意我们的规则(Bland规则)： 找到第一个目标函数中系数为负的非基本变量 找到限制最紧的约束条件 使用当前约束条件下的基本变量(也叫替入变量)与非基本变量（也叫替出变量）进行替换 我们对上述单纯形表进行一次示范操作： 首先我们看到第一个非基本变量$x_1$的系数为负数。那么我们开始对$x_1$进行增大。 对于第一个约束条件有$x_1=4$ 对于第二个约束条件有$x_1=2$ 对于第三个约束条件有$x_1=3$ 由于第二个约束是最紧的，那么我们选择第二个约束中的$x_5$作为替出变量，此时有单纯形表的变化为 2 0 -14 -6 0 1 0 2 0 1 1 1 -1 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 由于我们在替出时将$x_1=2-x_5$带入，很明显可以将单纯形表进行转换。如果没有想清楚的话我们可以手动带入一下即可。 一直重复这个操作，直到所有的非基本变量的系数都大于零（顶点无法再迁移），那么程序就求出了最优解。 注意，本文其实未完待续，因为其实在实现算法中间存在很多细节，但本文的目的是让读者对该算法有个系统的了解，明白算法的原由，至于一些具体的细节，读者可以参考链接进行进一步的了解！ 单纯形Simplex模板给出一份Simplex的模板与其应用，所求解的题目链接：BZOJ 1061： 注意！！该模板求解的是： ​ $max$ $c^Tx$ ​ $s.t.$ ​ $A*x&gt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$这类问题，稍稍转换一下就好了。之所以选择这个作为模板，是因为这份模板直接省去了替出变量，节省了大量空间，而且极其富有技巧性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;cstdio&gt;#include &lt;cmath&gt;typedef double DB;typedef long long LL;const int maxn = 1005, maxm = 10005;const DB inf = 0x3f3f3f3f3f3f3f3f, eps = 1e-7;int n, m;DB b[maxm], c[maxn], cof[maxm][maxn], ans;//约束、目标函数的系数、约束矩阵inline void pivot(int id, int pos) &#123; b[id] /= cof[id][pos];//首先归一化 cof[id][pos] = 1 / cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 for(int i = 1; i &lt;= n; i++) if(i != pos) cof[id][i] *= cof[id][pos];//归一化 for(int i = 1; i &lt;= m; i++) if(i != id &amp;&amp; fabs(cof[i][pos]) &gt; eps) &#123;//当前约束条件包含约束最紧的变量 b[i] -= cof[i][pos] * b[id];//搞死小圆(高斯消元)的方法更新变量的系数和b的值 for(int j = 1; j &lt;= n; j++) if(j != pos) cof[i][j] -= cof[i][pos] * cof[id][j]; cof[i][pos] = -cof[i][pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 &#125; ans += c[pos] * b[id];//对c进行消元 for(int i = 1; i &lt;= n; i++) if(i != pos) c[i] -= c[pos] * cof[id][i]; c[pos] = -c[pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方&#125;inline DB simplex() &#123; while(1) &#123; int pos, id; for(pos = 1; pos &lt;= n; pos++) if(c[pos] &gt; eps) break;//找到第一个系数大于0的变量，进行增大 if(pos == n + 1) return ans;//找不到了结束，返回答案 DB tmp = inf; for(int i = 1; i &lt;= m; i++) if(cof[i][pos] &gt; eps &amp;&amp; b[i] / cof[i][pos] &lt; tmp) tmp = b[i] / cof[i][pos], id = i;//找到约束最紧的那个约束条件 if(tmp == inf) return inf; pivot(id, pos);//进行顶点的迁移（旋转） &#125;&#125;int main() &#123; scanf("%d%d", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) scanf("%lf", &amp;c[i]);//其实将min改为max问题，可以直接采用结论，求对偶问题即可。 for(int i = 1; i &lt;= m; i++) &#123; int x, y; scanf("%d%d", &amp;x, &amp;y); for(int j = x; j &lt;= y; j++) cof[i][j] = 1; scanf("%lf", &amp;b[i]); &#125; printf("%lld\n", LL(simplex() + 0.5)); return 0;&#125; 参考文献hrwhisper]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Simplex</tag>
        <tag>最优化</tag>
        <tag>线性规划</tag>
      </tags>
  </entry>
</search>
