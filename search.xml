<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tree]]></title>
    <url>%2F2018%2F12%2F14%2Ftree%2F</url>
    <content type="text"><![CDATA[理模糊，说明是坏瓜纹理稍糊，还得看其触感，如果触感硬滑为好瓜，触感软粘为坏瓜纹理清晰，则接下来看根蒂….可以看出，决策树是一个可解释性很强的模型，用数据结构里的树来描述的话，是一棵多叉树，其中间结点代表决策步骤，叶子结点代表决策结果（或者说类别标签），而从根结点到叶子结点则描述了我们决策的过程。 不过，如何训练决策树呢？给定训练数据，可能存在多棵能拟合数据的决策树，如果要求解全局最优的决策树，那么是一个NP问题，因此，我们往往采用贪心法建立次优决策树。 采用贪心法建立决策树的框架如下（采用分治法（Divide and conquer）递归的建树）： 上图中，有2种情形使得递归返回， 情形1：即代码2-4行，当前结点的样本都属于同一类别，无需划分，递归返回情形2：即代码5-7行，在当前属性集为空，或是所有样本在所有属性上取值都相同，无法进一步划分，所以返回很简单是不是！我们只需要搞定第8行就可以了，如何选择最优划分属性呢？这就是不同贪心算法采用的不同策略了。 PS: 周志华的西瓜书中，认为第12行应该标记完然后return；我觉得是错误的。因为还有其它取值非空的Dv。因此这里我将图上的return去除了。 ID3与信息增益一般而言，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度越来越高。而ID3中采用信息增益来衡量“纯度”的变化，从而选择划分点。 信息论与概率统计中，熵（Entropy）用来表示随机变量不确定性的大小，熵越大，不确定性越大。 设随机变量D有d个取值{a1,a2,⋯,ad}, 取值取值为ai的概率为pi, 则随机变量的熵定义为：H(D)=−∑i=1dpilogpi(2-1)计算时约定当p = 0时，pilogpi=0 应用在分类数据集中，pi可以表示第i个类别占总样本的比重。 接着介绍条件熵（conditional entropy），即给定随机变量a后，D剩余的不确定性（熵）:H(D|A)=∑i=1dp(A=ai)H(D|A=ai)(2-2)看不懂条件熵的式子？其实就是按特征A的取值划分数据，对取值为ai的数据计算熵，然后乘以取值为ai的概率。 信息增益(information gain)描述的是，给定随机变量A后，X所减少的不确定性(即熵 – 条件熵)：IG(D|A)=H(D)–H(D|A)(2-3)一般而言，信息增益越大，说明选择属性A来进行划分所获得的“纯度”提升越大。因此，著名的ID3决策树学习算法就是用信息增益来划分属性的。 例子以周志华的西瓜书中西瓜数据2.0为例： 该数据共有17条，我们可以用这个数据来学习一棵判断一个瓜是否为好瓜的决策树。 首先看好瓜出现了8次，坏瓜出现了9次，因此可以用式2-1计算出根结点的熵为：H(D)=−∑i=12pilogpi=–(817log2817+917log2917)=0.998接着，查看可选取的特征有：{色泽、根蒂、敲声、纹理、脐部、触感} 先查看色泽特征，有三种取值，{青绿， 乌黑，浅白}， 色泽为青绿的时候，有{1, 4, 6, 10, 13, 17} 6个样例，其中好瓜为{1,4,6}色泽为乌黑时有{2, 3, 7, 8, 9, 15} 6个样例，其中好瓜为{2,3,7,8}色泽为浅白时有{5, 11, 12, 14, 16} 5个样例，其中好瓜为{5}因此我们用式子2-2计算条件熵：H(D|色泽)=∑i=13p(A=ai)H(D|A=ai)=p(A=青绿)H(D|A=青绿)+p(A=乌黑)H(D|A=乌黑)+p(A=浅白)H(D|A=浅白)=617(–(36log236+36log236))+617(–(46log246+26log226))+517(–(15log215+45log245))=617∗1.000+617∗0.918+517∗0.722=0.8893最后用2-3计算给定了色泽特征的信息增益：IG(D|色泽)=H(D)–H(D|色泽)=0.998−0.8893=0.109同理计算出其它特征的信息增益，IG(D|根蒂)=0.143；IG(D|敲声)=0.141IG(D|纹理)=0.381；IG(D|脐部)=0.289IG(D|触感)=0.006；因为纹理的信息增益最大，所以会选纹理作为这次的划分特征。 根据前面讲的决策树算法框架，会将数据集根据纹理的取值划分， 纹理清晰：{1, 2, 3, 4, 5, 6, 8, 10, 15}纹理稍糊: {7, 9, 13, 14, 17}纹理模糊: {11, 12, 16}然后，建立递归的建立三棵子树，不过子树中就没有纹理特征了。 以纹理清晰的子树为例，有9个样例(设为D1用来和原始数据进行区分)，H(D1)=−∑i=12pilogpi=–(79log279+29log229)=0.7642]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F12%2F13%2FMY%2F</url>
    <content type="text"><![CDATA[这是行间公式$$ x=\frac{-b\pm\sqrt{b^2-4ac}}{2a} $$ 这是行内公式这里是行内公式 \(E = mc^2\) 这里是行内公式]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F13%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
