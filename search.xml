<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Numpy函数解析]]></title>
    <url>%2F2019%2F02%2F26%2FNumpy%2F</url>
    <content type="text"><![CDATA[NumpyNumpy作为一个工具包，已经得到了大量的应用，可以说只要你用Pyhton，都必须要接触到这个工具包，但是numpy中的函数你真的了解吗？本文结合我自己的使用过程，对一些函数进行一些总结，这样我们在用时方便查找。 导入我们在使用时为了方便，通常在导入时将numpy重命名为np。 1import numpy as np 版本号1np.__version__ 我现在使用的版本是以下版本： 11.13.0 一般来说，numpy在版本更新后，函数用法都是不会改变的。所以这个也不需要纠结什么，直接往下走就行了。 创建numpy数组创建numpy数组的方式多种多样，不过以下几种基本够用： array()这个函数的参数为list或者array。传入list之后我们可以得到array类型的变量。传入array类型的话，我们会得到一个原array的拷贝。这里都可以是多维的list。 1234567891011&gt;&gt;&gt; x=[1,2,3]&gt;&gt;&gt; x=np.array(x)&gt;&gt;&gt; y=np.array(x)&gt;&gt;&gt; y[0]=4&gt;&gt;&gt; print(x,y)[1 2 3] [4 2 3]&gt;&gt;&gt; x=[[1,2,3],[4,5,6]]&gt;&gt;&gt; x=np.array(x)&gt;&gt;&gt; print(x)[[1 2 3] [4 5 6]] asarray()这个函数的函数是list或者array。传入list之后我们可以得到array类型的变量。不过传入array变量的话，我们只会得到一个原array的引用。这里都可以是多维的list。 12345678910111213&gt;&gt;&gt; x=[1,2,3]&gt;&gt;&gt; x=np.asarray(x)&gt;&gt;&gt; print(x)[1 2 3]&gt;&gt;&gt; y=np.asarray(x)&gt;&gt;&gt; y[0]=4&gt;&gt;&gt; print(x,y)[4 2 3] [4 2 3]&gt;&gt;&gt; x=[[1,2,3],[4,5,6]]&gt;&gt;&gt; x=np.asarray(x)&gt;&gt;&gt; print(x)[[1 2 3] [4 5 6]] 这个例子很好的反应了两者的区别。 zeros()这个函数传入的参数是shape，我们可以得到相应shape的全零矩阵。 123456789101112&gt;&gt;&gt; x=np.zeros([2,2],dtype=np.int32)&gt;&gt;&gt; print(x)[[0 0] [0 0]]&gt;&gt;&gt; x=np.zeros([2,2],dtype=np.float32)&gt;&gt;&gt; print(x)[[ 0. 0.] [ 0. 0.]]&gt;&gt;&gt; x=np.zeros([2,1],dtype=np.float64)&gt;&gt;&gt; print(x)[[ 0.] [ 0.]] ones()这个函数传入的参数是shape，可以得到相应shape全一矩阵。其中dtype是可选参数，其表示的是array里面的数据类型。默认的数据类型是np.float64。支持的参数有np.int32，np.int64 ，np.str，np.float32等。 123456789101112&gt;&gt;&gt; x=np.zeros([2,2],dtype=np.int32)&gt;&gt;&gt; print(x)[[ 1 1] [ 1 1]]&gt;&gt;&gt; x=np.zeros([2,2],dtype=np.float32)&gt;&gt;&gt; print(x)[[ 1. 1.] [ 1. 1.]]&gt;&gt;&gt; x=np.zeros([2,1],dtype=np.float64)&gt;&gt;&gt; print(x)[[ 1.] [ 1.]] full()这个函数功能相对比较简单，传入shape和填充值，之后得到填充后的array 123&gt;&gt;&gt; np.full([2,5],6)array([[6, 6, 6, 6, 6], [6, 6, 6, 6, 6]]) arange()这个函数有几种使用方式。 传入单个整数$x$，我们可以得到0到$x-1$的连续数组成的array。 123&gt;&gt;&gt; x=np.arange(5)&gt;&gt;&gt; print(x)[0 1 2 3 4] 传入两个数$L$与$R$，我们可以得到$L$到$R-1$的连续数组成的array。 123&gt;&gt;&gt; x=np.arange(1,6)&gt;&gt;&gt; print(x)[1 2 3 4 5] 传入三个数$L$,$R$,$step​$,我们根据步长得到相应的array。 123456&gt;&gt;&gt; x=np.arange(1,100,30)&gt;&gt;&gt; print(x)[ 1 31 61 91]&gt;&gt;&gt; x=np.arange(1,101,10)&gt;&gt;&gt; print(x)[ 1 11 21 31 41 51 61 71 81 91] eye()和identity()这两个函数的功能在我看来是一样的，都是得到单位阵。我们只需要传入相应的阶数就好了。 12345678&gt;&gt;&gt; np.eye(3)array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]])&gt;&gt;&gt; np.identity(3)array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) zeros_like()、ones_like()、fulls_like()zeros_like()、ones_like()这两个函数依然是生成全零矩阵或者全一矩阵。其参数是一个array变量。之后会生成相同shape,dtype的全零\一矩阵。 12345678&gt;&gt;&gt; x=np.eye(3)&gt;&gt;&gt; np.zeros_like(x)array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]])&gt;&gt;&gt; x = np.arange(4, dtype=np.int64)&gt;&gt;&gt; np.ones_like(x)array([1, 1, 1, 1], dtype=int64) 对于fulls_like()这个函数的话就是生成一个shape相同的填充矩阵。 123&gt;&gt;&gt; x = np.arange(4, dtype=np.int64)&gt;&gt;&gt; np.full_like(x,6,dtype=np.int64)array([6, 6, 6, 6], dtype=int64) copy()copy函数也很简单，参数就是一个array。我们在这里与上面的array函数和asarray函数做对比。 123456789101112&gt;&gt;&gt; x = np.array([1, 2, 3])&gt;&gt;&gt; print(id(x),x)1993915146032 [1 2 3]&gt;&gt;&gt; y = np.copy(x)&gt;&gt;&gt; print(id(y),y)1993915571216 [1 2 3]&gt;&gt;&gt; y=np.array(x)&gt;&gt;&gt; print(id(y),y)1993901738224 [1 2 3]&gt;&gt;&gt; y = np.asarray(x)&gt;&gt;&gt; print(id(y),y)1993915146032 [1 2 3] 我们可以看到copy后所占用的内存块与原内存块并不相同。也就是说array函数可以替代copy函数。但是asarray函数并不能替代copy函数。 linspace()这个函数可以完成$n$等份分割。参数是$L$,$R$,$n$ 123456&gt;&gt;&gt; np.linspace(3.,10,20)array([ 3. , 3.36842105, 3.73684211, 4.10526316, 4.47368421, 4.84210526, 5.21052632, 5.57894737, 5.94736842, 6.31578947, 6.68421053, 7.05263158, 7.42105263, 7.78947368, 8.15789474, 8.52631579, 8.89473684, 9.26315789, 9.63157895, 10. ]) logspace()老实说，我不知道什么时候用这个函数。。。。参数和上面的一样。 12345678&gt;&gt;&gt; np.logspace(3.,10, 20)array([ 1.00000000e+03, 2.33572147e+03, 5.45559478e+03, 1.27427499e+04, 2.97635144e+04, 6.95192796e+04, 1.62377674e+05, 3.79269019e+05, 8.85866790e+05, 2.06913808e+06, 4.83293024e+06, 1.12883789e+07, 2.63665090e+07, 6.15848211e+07, 1.43844989e+08, 3.35981829e+08, 7.84759970e+08, 1.83298071e+09, 4.28133240e+09, 1.00000000e+10]) diag()和diagflat()两个函数的作用都是生成一个方阵，参数是对角线的元素。 12345678910&gt;&gt;&gt; np.diag([1,2,3,4])array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt;&gt;&gt; np.diagflat([1,2,3,4])array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) 行列操作reshape()reshape函数是把array重新分配行列的大小，参数首先是array对象，之后是shape。注意，我们传参数要保证合法，否则就会报错。 123456789&gt;&gt;&gt; x=[[1,2],[3,4]]&gt;&gt;&gt; x=np.array(x)&gt;&gt;&gt; np.reshape(x,[1,4])array([[1, 2, 3, 4]])&gt;&gt;&gt; np.reshape(x,[4,1])array([[1], [2], [3], [4]]) ravel()和flatten()这两个函数是将二维array按行或者按列拉长为一维array。 123456789101112&gt;&gt;&gt; x=np.arange(1,7,1,dtype=np.int32).reshape(2,3)&gt;&gt;&gt; print(x)[[1 2 3] [4 5 6]]&gt;&gt;&gt; np.ravel(x)array([1, 2, 3, 4, 5, 6])&gt;&gt;&gt; np.ravel(x, order='F')array([1, 4, 2, 5, 3, 6])&gt;&gt;&gt; np.flatten(x)array([1, 2, 3, 4, 5, 6])&gt;&gt;&gt; np.flatten(x, order='F')array([1, 4, 2, 5, 3, 6]) swapaxes()这个函数可以改变交换维度的大小。 123456&gt;&gt;&gt; x=np.zeros([3,4,5])&gt;&gt;&gt; x.shape(3, 4, 5)&gt;&gt;&gt; x=np.swapaxes(x,0,1)&gt;&gt;&gt; x.shape(4, 3, 5) transpose()矩阵的转置，矩阵在二维的时候我们通常用.T来做同样的操作 123456789101112&gt;&gt;&gt; x=np.swapaxes(x,0,1)&gt;&gt;&gt; x.shape(4, 3, 5)&gt;&gt;&gt; x=np.zeros([3,4])&gt;&gt;&gt; x.shape(3, 4)&gt;&gt;&gt; x=x.transpose()&gt;&gt;&gt; x.shape(4, 3)&gt;&gt;&gt; y=x.T&gt;&gt;&gt; y.shape(3, 4) 当然在多维的时候，transpose可以做更多的事，我们可以指定需要转换成的维度。 1234&gt;&gt;&gt; x=np.zeros([3,4,5])&gt;&gt;&gt; x=np.transpose(x,[2,0,1])&gt;&gt;&gt; x.shape(5, 3, 4) expand_dims()这个函数的功能是添加维度，我们可以指定添加到第几个axis后。 1234567&gt;&gt;&gt; x=np.zeros([3,4])&gt;&gt;&gt; x=np.expand_dims(x,axis=1)&gt;&gt;&gt; x.shape(3, 1, 4)&gt;&gt;&gt; x=np.expand_dims(x,axis=0)&gt;&gt;&gt; x.shape(1, 3, 1, 4) squeeze()这个函数的功能很简单，把维度大小为1的维度，全部删除，因为维度大小为1的话本来就没用。 1234&gt;&gt;&gt; x=np.zeros([3,4,1])&gt;&gt;&gt; x=np.squeeze(x)&gt;&gt;&gt; x.shape(3, 4) concatenate()、hstack()、vstack()、dstack()concatenate这个函数在两个array进行拼接时用到，我们可以指定拼接到指定的axis上。有时候我们为了简化掉axis参数使用hstack()，这些函数。其中vstack()表示axis为0，其中hstack()表示axis为1，其中dstack()表示axis为3。 1234567891011121314&gt;&gt;&gt; x=np.arange(1,7,1)&gt;&gt;&gt; x=x.reshape(2,3)&gt;&gt;&gt; y=np.arange(7,13,1)&gt;&gt;&gt; y=y.reshape(2,3)&gt;&gt;&gt; out1=np.concatenate((x,y),axis=1)&gt;&gt;&gt; out2=np.hstack((x,y))&gt;&gt;&gt; assert np.allclose(out1,out2)&gt;&gt;&gt; print(x,"\n",y,"\n",out1)[[1 2 3] [4 5 6]] [[ 7 8 9] [10 11 12]] [[ 1 2 3 7 8 9] [ 4 5 6 10 11 12]] split()split函数可以对维度进行再切分。将维度扩充。 1234&gt;&gt;&gt; x=np.arange(1,10,1)&gt;&gt;&gt; x=np.split(x,[4,6])#在维度为第4和6的地方进行切分&gt;&gt;&gt; print(x)[array([1, 2, 3, 4]), array([5, 6]), array([7, 8, 9])] split函数还可以在指定的axis上进行切分。 123456789101112&gt;&gt;&gt; x=np.arange(16).reshape(2,2,4)&gt;&gt;&gt; x=np.split(x,[3],axis=2)#在第三维的第三个位置切分&gt;&gt;&gt; print(x)[array([[[ 0, 1, 2], [ 4, 5, 6]], [[ 8, 9, 10], [12, 13, 14]]]), array([[[ 3], [ 7]], [[11], [15]]])] tile()这个函数就是把array多重复几次。重复的次数用shape来表示。 1234&gt;&gt;&gt; x=np.arange(3)&gt;&gt;&gt; np.tile(x,[2,2])array([[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]]) repeat()这个函数相比tile就是在重复的方式上不一样，是依次重复的。而且参数是整型数，而不是shape 123&gt;&gt;&gt; x=np.arange(3)&gt;&gt;&gt; np.repeat(x,2)array([0, 0, 1, 1, 2, 2]) trim_zeros()作用就是去掉array上头尾多余的0 1234&gt;&gt;&gt; x=np.array([0, 0, 0, 1, 2, 3, 0, 2, 1, 0])&gt;&gt;&gt; x=np.trim_zeros(x)&gt;&gt;&gt; print(x)[1 2 3 0 2 1] unique()这个函数就是返回一个排序的无重复元素的array。但是其有可选项return_counts，表示是否返回每个元素原先出现多少次。 1234&gt;&gt;&gt; x=np.array([2, 2, 1, 5, 4, 5, 1, 2, 3])&gt;&gt;&gt; uni,cnt=np.unique(x,return_counts=True)&gt;&gt;&gt; print(uni,cnt)[1 2 3 4 5] [2 3 1 1 2] fliplr()、flipud()两个函数表示把array里的左右顺序改变或者是上下顺序改变 1234567&gt;&gt;&gt; x=np.arange(1,9).reshape(2,4)&gt;&gt;&gt; np.fliplr(x)array([[4, 3, 2, 1], [8, 7, 6, 5]])&gt;&gt;&gt; np.flipud(x)array([[5, 6, 7, 8], [1, 2, 3, 4]]) rot90()表示将将array逆时针旋转90度。这个实际中有什么用？？？？ 1234&gt;&gt;&gt; x=np.arange(1,5).reshape(2,2)&gt;&gt;&gt; np.rot90(x)array([[2, 4], [1, 3]]) roll()可以在指定维度上循环array。 1234&gt;&gt;&gt; x=np.arange(1,9).reshape(2,4)&gt;&gt;&gt; np.roll(x,1,axis=1)array([[4, 1, 2, 3], [8, 5, 6, 7]]) 逻辑函数any()原array中有一个为1。则为真。 1234567&gt;&gt;&gt; x = np.array([1,0,0])&gt;&gt;&gt; print(np.any(x))True&gt;&gt;&gt;&gt;&gt;&gt; x = np.array([0,0,0])&gt;&gt;&gt; print(np.any(x))False all()原array必须全为1。才为真。 1234567&gt;&gt;&gt; x = np.array([1,2,3])&gt;&gt;&gt; print(np.all(x))True&gt;&gt;&gt;&gt;&gt;&gt; x = np.array([1,0,3])&gt;&gt;&gt; print(np.all(x))False allclose()、array_equal()allclose这个是用来比较两个array是否一样的，默认误差alot为1e-5。而array_equal则需要完全一样才可以。 1234&gt;&gt;&gt; print(np.allclose([3], [2.999999]))True&gt;&gt;&gt; print(np.array_equal([3], [2.999999]))False isclose()、equal()这两个函数是进行逐个位置进行比较。 123456&gt;&gt;&gt; print(np.array_equal([3], [2.999999]))False&gt;&gt;&gt; print(np.equal([1, 2], [1, 2.000001]))[ True False]&gt;&gt;&gt; print(np.isclose([1, 2], [1, 2.000001]))[ True True] 运算函数一一对应运算12345678910111213141516x=[[1,2,3],[4,5,6]]x=np.array(x)y=[[1,1,1],[2,2,2]]y=np.array(y)z=np.add(x, y)#一一对应的加上print(z)z=np.subtract(x, y)#一一对应的减去print(z)z=np.multiply(x, y)#一一对应的乘上print(z)z=np.divide(x, y)#一一对应的除上print(z)z=np.power(x, y)#一一对应的次方print(z)z=np.mod(x, y)#一一对应的取模print(z) matmul()这个函数完成的功能是矩阵的乘法。做乘法的时候最好不要用dot函数。 123456&gt;&gt;&gt; x=[2,3]&gt;&gt;&gt; y=[[1,0],[4,0]]&gt;&gt;&gt; x=np.array(x)&gt;&gt;&gt; y=np.array(y)&gt;&gt;&gt; np.matmul(x,y)array([14, 0]) 我的一些笔记123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224import numpy as np#生成函数x=[3,4,5]y=np.array(x)z=np.asarray(x)#注意此时array和asarray没有区别，都新申请了内存来存储x[1]=111print(x)print(y)print(z)x=np.zeros(3) #生成一个1*3维的全零矩阵y=np.array(x)z=np.asarray(x)#这时体现出array和asarray的区别，即x若为ndarray的话，asarray不在申请新内存了x[1]=999z[2]=888print(x)print(y)print(z)x=[2,3,4]y=np.array(x,dtype=np.float64)#转换为浮点数z=np.array(x,dtype=np.complex64)#转换为复数print(y)print(z)x=np.ones(3) #生成一个1*3的全1矩阵,默认为float64类型的print(x)print(type(x[1]))x=np.zeros(3, dtype=np.float32) #生成一个1*3的全0（float32类型）矩阵print(x)y=np.ones_like(x)#生成一个全为1的矩阵，矩阵的shape和传入的是一样的，数据类型也是一样的print(y)print(type(y[1]))x=np.eye(3)print(x)#生成一个3*3维的单位矩阵x=np.identity(3)#两个函数的功能一样print(x)#生成一个3*3维的单位矩阵x=np.arange(3)#生成一个0-2的矩阵print(x)x=np.arange(3,5)#生成一个3-4的矩阵print(x)x=np.arange(3,7,2)#生成一个3-6的步长为2的矩阵print(x)x=np.zeros(3)y=np.ones(3)z=np.where(0,x,y)print(z)z=np.where(1,x,y)print(z)x=[3,4,5,6,7,8]y=[3,3,6,6,7,8]x=np.array(x,dtype=np.float32)print(type(x[1]))print(np.in1d(x,y))#两个矩阵中的元素一一对应看看值是否相同，相同true，不同flase，且只与x的长度有关系x=[3,4,5]x=np.diag(x)#传进去的x表示的是对角线上的元素，生成的是一个方阵print(x)x=[[3,4,5],[5,6,7]] #这样就错了，只对一维的ndarray有效果的x=[3,4,5]x=np.array(x)x=np.diag(x)print(x)x=[2,3]y=[[1,0],[4,0]]x=np.array(x)y=np.array(y)#矩阵的乘法#(2.3)*(1,0) = (14,0)# (4,0)z=np.dot(x,y)print(z)x=np.diag([1,2,3,-4])print(np.trace(x)) #计算方阵的对角线元素的和x=[[4,3,2,1],[8,7,5,6]]x=np.array(x)print(x)x=np.sort(x)#对矩阵按维度进行排序print(x)x=[[4,4,3,2],[2,2,2,7]]x=np.array(x)x=np.unique(x)#不管多少维的矩阵，将其拉伸为一维，去重，排序，返回print(x)x=[[3,3,-3],[-4,-5,7]]x=np.array(x)x=np.abs(x)#不管多少维的矩阵，将其中每个元素取绝对值后返回int类型的x=np.fabs(x)#不管多少维的矩阵，将其中每个元素取绝对值后返回float类型的print(x)x=[[3,3,-3],[-4,-5,7]]x=np.array(x)print(np.mean(x))#不管多少维的，元素全部相加后，除以所有的个数，输出结果x=[[4,9,16],[4,25,100]]x=np.array(x)x=np.sqrt(x)#对每个元素开根号之后返回，注意元素不能为负数，否则结果会出错，而且这里自动将类型转换为了float了print(x)x=[[4,9,16],[4,25,100]]x=np.array(x)x=np.square(x)#对每个元素平方，不转换类型的##类似的还有np.exp(ndarray) 计算e^x 后返回##log、log10、log2、log1p ,计算ln,log10,log2,log(1+x)的log值print(x)x=[[3,3,-3],[-4,-5,7]]x=np.array(x)print(np.sign(x)) #查看矩阵中的每个元素的正负情况np.ceil(x)np.floor(x)np.rint(x)# 计算大于等于改值的最小整数# 计算小于等于该值的最大整数# 四舍五入到最近的整数，保留dtypex=[[1,2,3],[4,5,6]]x=np.array(x)y=[[1,1,1],[2,2,2]]y=np.array(y)z=np.add(x, y)#一一对应的加上print(z)z=np.subtract(x, y)#一一对应的减去print(z)z=np.multiply(x, y)#一一对应的乘上print(z)z=np.divide(x, y)#一一对应的除上print(z)z=np.power(x, y)#一一对应的次方print(z)z=np.mod(x, y)#一一对应的取模print(z)#np的常用的属性print(x.T)#矩阵的转置print(x.ndim)#获取ndarray的维度print(x.shape)#获取ndarray的各个维度的长度，若每一维的长度相同会返回(x,y)，若存在不同的会返回(x,)print(x.dtype)#值的类型print(x.T)#矩阵的转置x.T[2][1]=1#会修改的！！！print(x)#矩阵的转置y=xprint(x)y[1][1]=-1print(x)#这样的话会改变值的结果y=np.copy(x)#重新拷贝一个xy[0][0]=-100#这样的话不会改变结果了，这个还是很关键的呀print(x)print(y)x=[[1,2,3],[4,5,6]]x=np.array(x)print(x.T)#转置和reshape不一样的！！reshape是依次填格子。转置是XIJ=XJI！！！！！两个不一样的！x=x.reshape(3,2)print(x)x=[[1,2,3],[4,5,6]]x=np.array(x)y=[[1,1,1]]y=np.array(y)print(x.mean( axis=0 )) #按照列进行求平均值，返回的是一个一维的矩阵print(x.mean( axis=1 )) #按照行进行求平均值，返回的是一个一维的矩阵print(x.sum( axis=0 )) #按照列进行求和，返回的是一个一维的矩阵print(x.sum( axis=1 )) #按照行进行求和，返回的是一个一维的矩阵print(x.max( axis=0 )) #按照列返回最大的值，返回的是一个一维的矩阵print(x.max( axis=1 )) #按照行返回最大的值，返回的是一个一维的矩阵print(x.min( axis=0 )) #按照列返回最小的值，返回的是一个一维的矩阵print(x.min( axis=1 )) #按照行返回最小的值，返回的是一个一维的矩阵print(x.argmax( axis=0 )) #按照列返回最大的值的索引，返回的是一个一维的矩阵，在列的下标print(x.argmax( axis=1 )) #按照行返回最大的值的索引，返回的是一个一维的矩阵，在行的下标print(x.argmin( axis=0 )) #按照列返回最小的值的索引，返回的是一个一维的矩阵，在列的下标print(x.argmin( axis=1 )) #按照行返回最小的值的索引，返回的是一个一维的矩阵，在行的下标z=x.dot(y.T)#也是求矩阵的相乘print(z)#一维情况下x=np.arange(7)print(x)print(x[1]) #选取下标为1的元素print(x[1:3]) #选取下标为1的到下标为2的元素print(x[:]) #选取全部的元素print(x[1:]) #选取下标1开始的元素print(x[:5]) #选取下标0到下标4的元素#二维情况下x=[[1,2,3],[4,5,6]]x=np.array(x)print(x[1][1]) #选择第[1][1]个元素np.random.seed(4)#确定随机数种子x=np.random.permutation(6)#随机一个排列出来print(x)x=[1,2,4,4,5]x=np.random.permutation(x)#返回一个序列的随机排列print(x)x=np.random.randn(3,3)#生成一个3*3的正态分布（平均值为0,标准差为1）print(x)x=np.random.normal(size=(3,3))#生成一个3*3的正态（高斯）分布,注意这里有size参数的print(x)#NumPy.linalgx=np.diag([3,4,5])y=np.array([3,4,5])print(np.linalg.det(x)) #计算方阵的行列式的值print(np.linalg.eig(x)) #计算方阵的本征值和本征向量？？？特征值特征向量？print(np.linalg.inv(x)) #计算矩阵的逆print(np.linalg.pinv(x)) #计算矩阵的伪逆print(np.linalg.qr(x)) #计算矩阵的qr分解print(np.linalg.svd(x)) #计算矩阵的奇异值分解print(np.linalg.solve(x,y.T)) #解线性方程组 ???print(np.linalg.lstsq(x,y.T)) #解线性方程组 ??? 最小二乘解]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[素数/质数]]></title>
    <url>%2F2019%2F02%2F24%2FPrime%2F</url>
    <content type="text"><![CDATA[素数/质数素数/质数是一个非常容易被考察的点，其定义很简单：只有1和其本身能够将其整除。关于素数，衍生出了很多理论，这些理论对求解问题时能够极大的加速算法。本文我们探讨素数判定问题。 暴力判断这个就很简单了，由于除数肯定比被除数要小，所以我们对每个数进行枚举判断即可。时间复杂度$O(n)$ 1234567bool Is_prime(int n)&#123; for(int i=2;i&lt;n;i++)&#123; if(n%i==0) return false; &#125; return true;&#125; 常用判断我们通常对上面的算法进行一些改进，假如说一个数不是素数。那么其肯定可以写成$a*b$的形式。我们规定$a$比$b$小。那么$a$最大为$\sqrt n$。之后我们再枚举所有的$a$，若能找到满足的$b$，那么就不是素数，对所有的$a$都不满组的话，就是素数。时间复杂度$O(\sqrt n)$ 12345678bool Is_prime(int n)&#123; int N=sqrt(n); for(int i=2;i&lt;=N;i++)&#123; if(n%i==0) return false; &#125; return true;&#125; 技巧判断我最近看到了一个有意思的idea，这也是写这篇文章的初心所在。我们将$n$记作为$6x+k$的形式。那么从5开始，所有的数可以被记作为：…$6x-1$,$6x$,$6x+1$,$6x+2$,$6x+3$,$6x+4$…的形式。 首先我们证明当$n$为素数的时候，$6x+k$中的$k​$只可能为-1和1。 当$k$为0的时候，$n​$可以被3整除。 当$k$为2的时候，$n$可以被2整除。 当$k$为3的时候，$n$可以被3整除。 当$k$为4的时候，$n$可以被2整除。 根据常识我们可以知道$6x+1$与$6x-1$是不可能被3整除(因为$6x$被3整除，其相邻的数必不可能被3整除)，同时为奇数。那么等于说$n$为素数的时候形式，只有可能是$6x+1$与$6x-1​$。其实这就是孪生素数，有兴趣的话，可以百深入了解一下。 于是我们可以很轻松的枚举出小于$\sqrt n$中的所有素数了，虽然会有大量的非素数被枚举到，但是还是能起到不错的效果，也许未来会出现更强的算法？ 123456789bool Is_prime(int n)&#123; if(n==1) return false; if(n==2||n==3) return true; if(n%6!=1&amp;&amp;n%6!=5) return false; for(register int i=5;i*i&lt;=n;i+=6) if(n%i==0||n%(i+2)==0) return false; return true;&#125; 素数测试我们使用费马小定理的逆定理来进行测试，若每次测试都满足的话，我们认为这个数就是素数。这个是个坑，以后以后机会补上。代码先搬一下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#include &lt;iostream&gt; #include &lt;cstdio&gt; #include &lt;algorithm&gt; #include &lt;cmath&gt; #include &lt;cstring&gt; #include &lt;map&gt; #define ll long longusing namespace std;const int times = 20;int number = 0;map&lt;ll, int&gt;m;ll Random(ll n)//生成[ 0 , n ]的随机数&#123; return ((double)rand()/RAND_MAX*n+0.5);&#125;ll q_mul(ll a, ll b, ll mod)//快速计算 (a*b) % mod&#123; ll ans=0; while(b) &#123; if(b&amp;1) &#123; b--; ans=(ans+a)%mod; &#125; b/=2; a=(a+a)%mod; &#125; return ans;&#125;ll q_pow(ll a,ll b,ll mod)//快速计算 (a^b) % mod&#123; ll ans=1; while(b) &#123; if(b&amp;1) &#123; ans=q_mul(ans,a,mod ); &#125; b/=2; a=q_mul(a,a,mod); &#125; return ans;&#125;bool witness(ll a,ll n)//miller_rabin算法的精华&#123;//用检验算子a来检验n是不是素数 ll tem=n-1; int j=0; while(tem%2==0) &#123; tem/=2; j++; &#125; //将n-1拆分为a^r * s ll x=q_pow(a,tem,n); //得到a^r mod n if(x==1||x==n-1) return true;//余数为1则为素数 while(j--) //否则试验条件2看是否有满足的 j &#123; x=q_mul(x,x,n); if(x==n-1)return true; &#125; return false;&#125;bool miller_rabin(ll n)//检验n是否是素数&#123; if(n==2)return true; if(n&lt;2||n%2==0)return false;//如果是2则是素数，如果&lt;2或者是&gt;2的偶数则不是素数 for(register int i=1;i&lt;=times;i++)//做times次随机检验 &#123; ll a=Random(n-2)+1;//得到随机检验算子 a if(!witness(a,n))return false;//用a检验n是否是素数 &#125; return true;&#125;int main()&#123; ll x; while(cin&gt;&gt;x) &#123; if(miller_rabin(x)) cout&lt;&lt;"Yes"&lt;&lt;endl; else cout &lt;&lt;"No"&lt;&lt;endl; &#125; return 0;&#125; 埃拉托斯特尼筛法我们可以通过筛法的预处理使用$O(n\log\log n)$的时间得到小于$n​$的所有素数，思想很简单，素数的倍数不是素数。我们每次得到素数之后，将其的倍数全部舍弃就好了。 12345678910111213int p[100005],prime[100005],len;void init(int n)&#123; len=0; memset(prime,0,sizeof(prime)); for(int i=2;i&lt;=n;i++)&#123; if(prime[i]==0)&#123; p[len++]=i; for(j=i+i;j&lt;=n;j+=i)&#123; prime[j]=1; &#125; &#125; &#125;&#125; 动态图的话就是这样的： the Meissel, Lehmer, Lagarias, Miller, Odlyzko method这个是一个神奇的算法，我是照搬过来的，没有仔细研究过，只是当模板用过。。估计也研究不明白。这个算法可以快速的求1e18内有多少个素数小于某个数。算法的时间复杂度也是相当低，复杂度只有$O(\frac {n^\frac 3 4} {\log n})$。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include&lt;cstdio&gt;#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;#include&lt;cstdlib&gt;#include&lt;ctime&gt;#pragma warning(disable:4996)#define INF 2000000005#define lowbit(a) ((a)&amp;-(a))#define FAIL -INFconst long long MAXN=6893911;long long p[MAXN], cnt;bool mark[MAXN];int pi[MAXN];void init()&#123; long long i,j; for (i=2;i&lt;MAXN;i++) &#123; if (!mark[i]) p[cnt++]=i; pi[i]=pi[i-1]+!mark[i]; for (j=0;p[j]*i&lt;MAXN&amp;&amp;j&lt;cnt;j++) &#123; mark[p[j]*i]=true; if (i%p[j]==0) break; &#125; &#125;&#125;int f(long long n,int m)&#123; if (n==0)return 0; if (m==0)return n-n/2; return f(n,m-1)-f(n/p[m],m-1);&#125;int Pi(long long N);int p2(long long n, int m)&#123; int ans=0; for (int i=m+1;(long long)p[i]*p[i]&lt;=n;i++) ans+=Pi(n/p[i])-Pi(p[i])+1; return ans;&#125;int p3(long long n, int m)&#123; int ans=0; for (int i=m+1;(long long)p[i]*p[i]*p[i]&lt;=n;i++) ans+=p2(n/p[i],i-1); return ans;&#125;int Pi(long long N)&#123; if (N&lt;MAXN)return pi[N]; int lim=f(N,0.25)+1; int i; for (i=0;p[i]&lt;=lim;i++); int ans=i+f(N,i-1)-1-p2(N,i-1)-p3(N,i-1); return ans;&#125;int main()&#123; long long L,R; scanf("%lld %lld",&amp;L,&amp;R); init(); printf("%d",Pi(R)-Pi(L-1)); return 0;&#125; 参考文献洛谷日报-质数判定]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Prime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unordered_map & set]]></title>
    <url>%2F2019%2F02%2F06%2FUnordered-mapset%2F</url>
    <content type="text"><![CDATA[Unordered_map &amp; setC++11发布后，出现了一些更加有用的容器，性能超过了STL中自带的容器，我们常用的有unordered_map,unordered_set,unordered_multiset。我们主要介绍一下unordered_multiset，通过我们前一篇文章的介绍，我们知道map和set是按照红黑树实现的，且内部按照键值排序的，即内部依然是有序的，这就导致了我们的查询是$O(\log n)$，但是我们很多时候都不需要这些多余的性质，于是出现了unordered这样的关键字，表示内部是无序的，这种数据结构本质借助了hash的思想。实现了$O(1)$ 的查询。map相当于java中的TreeMap，unordered_map相当于HashMap。无论从查找、插入上来说，unordered_map的效率都优于hash_map，更优于map；而空间复杂度方面，hash_map最低，unordered_map次之，map最大。 unordered_map与map的对比： 存储时是根据key的hash值判断元素是否相同，即unordered_map内部元素是无序的，而map中的元素是按照二叉搜索树存储（用红黑树实现），进行中序遍历会得到有序遍历。所以使用时map的key需要定义operator&lt;。而unordered_map需要定义hash_value函数并且重载operator==。但是很多系统内置的数据类型都自带这些。 总结：结构体用map重载&lt;运算符，结构体用unordered_map重载==运算符。 unordered_map与hash_map对比： unordered_map原来属于boost分支和std::tr1中，而hash_map属于非标准容器。 unordered_map感觉速度和hash_map差不多，但是支持string做key，也可以使用复杂的对象作为key。 unordered_map编译时gxx需要添加编译选项：–std=c++11 使用方式123456789101112131415161718192021#include&lt;unordered_set&gt;#include&lt;unordered_map&gt;#include&lt;bits/stdc++.h&gt;using namespace std;/*//不支持C++11的时候#include&lt;tr1/unordered_set&gt;#include&lt;tr1/unordered_map&gt;#include&lt;bits/stdc++.h&gt;using namespace std;using namespace tr1;*/unordered_set&lt;int&gt; s;unordered_multiset&lt;int&gt; ms;unordered_map&lt;int,int&gt; ii;int main(void)&#123; s.insert(6); ms.insert(7); ii[34]=178; cout&lt;&lt;ii[34]&lt;&lt;endl;&#125; 通过上面的代码，我们可以看到，和正常的使用方式一样，只不过效率更高了。但是由于内部是由hash实现的，所以肯定有投机取巧的素数可以卡掉unordered_map。$x=126271$就是一个神奇的素数，若将其和其的倍数插入到unordered_map当中，就会相当慢，甚至不如一般的map。 参考文献洛谷日报 neal’s blog]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>set</tag>
        <tag>map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[STL用法及其时间复杂度分析]]></title>
    <url>%2F2019%2F02%2F05%2FSTL%2F</url>
    <content type="text"><![CDATA[STLSTL是C++语言中一个非常实用的代码库，叫做标准模板库，通常我们使用&lt;bits/stdc++.h&gt;这个头文件即可导入STL。本文立足与C++，但是python其实也是大同小异。 setset正如其名，表示的是一个集合，其分为两类，set为数学上的集合，即不含重复元素，multiset为可重集合，即可以包含重复元素我理解这个就是像比于set，insert的时候增加了一维时间戳。两者其内部的实现为一颗红黑树。 set multiset 定义12set&lt;类型&gt; 名字;multiset&lt;类型&gt; 名字; 举个例子来说: 12345set&lt;int&gt; s;set&lt;set&lt;int&gt; &gt; ss;set&lt;pair&lt;int,int&gt; &gt; ps;set&lt;vector&lt;int&gt; &gt; vs;multiset&lt;double&gt; ms; 如果我们需要自己定义类型的话需要自己定义友元函数来重载运算符，与排序是一样的，例如: 123456789101112131415161718192021222324252627#include&lt;bits/stdc++.h&gt;using namespace std;struct node &#123; int x,y; friend bool operator &lt; (node A,node B)&#123; //和 sort 定义 cmp 函数是一样的 if(A.x==B.x)&#123; return A.y&lt;B.y; &#125; return A.x&lt;B.x; &#125;&#125;;int main(void) &#123; node a,b; a.x=1;a.y=2; b.x=1;b.y=3; set&lt;node&gt; A; A.insert(a); A.insert(b); auto x=*A.begin(); cout&lt;&lt;x.x&lt;&lt;" "&lt;&lt;x.y&lt;&lt;endl; x=*(++A.begin()); cout&lt;&lt;x.x&lt;&lt;" "&lt;&lt;x.y&lt;&lt;endl;&#125;output:1 21 3 size函数表示set中的元素个数，返回一个整形变量，时间复杂度为$O(1)$ 使用方式： 1printf("%d",(int)s.size()); clear函数清空一个集合，无返回值 使用方式: 1s.clear() count函数返回set中的值为$x$的元素个数，时间按复杂度为$O(\log n+an)$ 使用方式： 1printf("%d\n",s.count(x)); 迭代器set的迭代器是双向的，支持–,++两种操作。若把 ite++ ，则 ite 将会指向“下一个”元素。这里的下一个是指在 key从小到大排序的结果中，排在ite 下一名的元素。同理，若把ite– ，则 ite会指向排在上一个的元素。“++”，“–”操作的复杂度均为$O (\log n)$。 使用方式： 123456789101112131415161718#include&lt;bits/stdc++.h&gt;using namespace std;set&lt;int&gt; s;set&lt;int&gt;::iterator ite;int main(void) &#123; s.insert(4); s.insert(5); ite=s.begin(); cout&lt;&lt;*ite&lt;&lt;endl; ite++; cout&lt;&lt;*ite&lt;&lt;endl; ite--; cout&lt;&lt;*ite&lt;&lt;endl;&#125;output:454 begin函数表示集合的首迭代器，时间复杂度$O(1)​$ 使用方式： 1cout&lt;&lt;*s.begin()&lt;&lt;endl; end函数由于STL都是左闭右开的，那么end返回的位置其实是类似于string中\0的位置，所以我们需要–，才能得到最大的元素的迭代器。其实s.end()存储的是集合中的元素个数 使用方式： 1cout&lt;&lt;*(--s.end())&lt;&lt;endl; 遍历我们通过++运算符以及begin函数和end函数来进行遍历整个集合 1234567891011#include&lt;bits/stdc++.h&gt;using namespace std;set&lt;int&gt; s;set&lt;int&gt;::iterator ite;int main(void) &#123; s.insert(4); s.insert(5); for(ite=s.begin();ite!=s.end();ite++)&#123; cout&lt;&lt;*ite&lt;&lt;endl; &#125;&#125; insert函数向set中插入元素，返回值为插入地址的迭代器以及是否插入成功所组成的pair，时间复杂度为$O(\log n)$使用方式： 1s.insert(5); erase函数删除，参数可以是元素或者迭代器，返回下一个元素的迭代器，时间复杂度为 O(log n)，注意在 multiset 中 s.erase(x)会删除所有值为 $x$ 的元素。所以在multiset中一般是和find函数一起使用的。 使用方式: 12s.erase(4);s.erase(s.find(5)); find函数在集合中找到第一个元素值等于$x$的迭代器，若不存在的话，返回s.end()。时间复杂度为$O(\log n)$ 123if(s.find(3)!=s.end())&#123; cout&lt;&lt;"yes"&lt;&lt;endl;&#125; lower_bound和upper_bound函数用法与$find$类似，但查找的条件略有不同，时间复杂度 $O(\log n)$。和vector当中的几乎一样。 $s.lower_ bound(x)$表示查找 $&gt;=x​$的元素中最小的一个，并返回指向该元素的迭代器。 $s.upper_ bound(x)$表示查找 $&gt;x$ 的元素中最小的一个，并返回指向该元素的迭代器。 例如： 12345678910111213141516171819202122232425262728293031323334353637#include&lt;bits/stdc++.h&gt;using namespace std;set&lt;int&gt; s;set&lt;int&gt; ::iterator ite;int main(void) &#123; int a[]=&#123;3,4,6,7,9&#125;; for(int i=0;i&lt;5;i++)&#123; s.insert(a[i]); &#125; ite=s.lower_bound(4); cout&lt;&lt;*ite&lt;&lt;endl;//4 ite=s.upper_bound(4); cout&lt;&lt;*ite&lt;&lt;endl;//6 ite=s.lower_bound(8); cout&lt;&lt;*ite&lt;&lt;endl;//9 ite=s.upper_bound(8); cout&lt;&lt;*ite&lt;&lt;endl;//9 ite=s.lower_bound(9); cout&lt;&lt;*ite&lt;&lt;endl;//9 ite=s.upper_bound(9); cout&lt;&lt;*ite&lt;&lt;endl;//s.end(); 5 ite=s.lower_bound(99); cout&lt;&lt;*ite&lt;&lt;endl;//s.end(); 5 ite=s.upper_bound(99); cout&lt;&lt;*ite&lt;&lt;endl;//s.end(); 5&#125;/*output:46999555*/ mapmap表示一个映射，这是非常有用的东西。其主要分为两部分&lt;key,value&gt;表示的是一个偏序对，左边key为键值，右边value为映射值。 定义1map&lt;key类型，value类型&gt; 名字 举个例子： 1234map&lt;int,int&gt; ii;map&lt;int,double&gt; id;map&lt;long long,double&gt; Ld;map&lt;vector&lt;int&gt; ,vector&lt;int&gt; &gt; vv; 如果我们需要自定义类型的话，和前面的set一样自己定义友元函数重载运算符即可。 size函数表示map中存储了多少对映射，时间复杂度$O( 1 )$ 使用方式： 1cout&lt;&lt;ma.size()&lt;&lt;endl; 插入方式类似数组一样的使用方式，使用[]来进行插入。使劲按复杂度是$O(\log n )$ 1234567891011#include&lt;bits/stdc++.h&gt;using namespace std;map&lt;string,int&gt; ma;int main(void) &#123; ma["a"]=3;//表示"a"映射为3 ma["b"]=312;//表示"a"映射为312 ma["c"]=43;//表示"a"映射为43 cout&lt;&lt;ma["a"]&lt;&lt;endl; cout&lt;&lt;ma["b"]&lt;&lt;endl; cout&lt;&lt;ma["c"]&lt;&lt;endl;&#125; 迭代器使用与set一样的方式进行遍历，定义迭代器，但是相应不同的地方就是first与second的用法。其中++,–操作的时间复杂度也是$O(\log n)$的。 注意，map中的end迭代器是真的没有用的。我们必须要进行特判 123456789101112#include&lt;bits/stdc++.h&gt;using namespace std;map&lt;string,int&gt; ma;map&lt;string,int&gt;::iterator ite;int main(void) &#123; ma["a"]=3;//表示"a"映射为3 ma["b"]=312;//表示"a"映射为312 ma["c"]=43;//表示"a"映射为43 for(ite=ma.begin();ite!=ma.end();ite++)&#123; cout&lt;&lt;ite-&gt;first&lt;&lt;" "&lt;&lt;ite-&gt;second&lt;&lt;endl; &#125;&#125; lower_bound和upper_bound函数map在调用这些函数的时候，是二分查找键值。 1234567891011121314151617#include&lt;bits/stdc++.h&gt;using namespace std;map&lt;int,int&gt; ma;map&lt;int,int&gt;::iterator ite;int main(void) &#123; int i,a[]=&#123;3,4,6,7,9&#125;; for(i=0;i&lt;5;i++)&#123; ma[a[i]]=i; &#125; ite=ma.lower_bound(4); if(ite!=ma.end()) cout&lt;&lt;ite-&gt;first&lt;&lt;" "&lt;&lt;ite-&gt;second&lt;&lt;endl; ite=ma.upper_bound(4); if(ite!=ma.end()) cout&lt;&lt;ite-&gt;first&lt;&lt;" "&lt;&lt;ite-&gt;second&lt;&lt;endl;&#125;output:4 16 2 vectorvector是stl中封装好的动态数组实现方式。其有一些特性，我们不妨看看。其中我们需要知道以下两个函数。 size() capacity() clear() 前者表示当前的vector存储了多少个数值，时间复杂度为$O(1)​$，中者表示的是当前的vector申请类多少内存，后者表示清空当前的vector。 123456789101112131415161718192021222324252627282930#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; a;int main(void) &#123; int i; for(i=0;i&lt;15;i++)&#123; cout&lt;&lt;"now size "&lt;&lt;a.size()&lt;&lt;" now capacity "&lt;&lt;a.capacity()&lt;&lt;endl; a.push_back(i); &#125; a.clear(); cout&lt;&lt;"now size "&lt;&lt;a.size()&lt;&lt;" now capacity "&lt;&lt;a.capacity()&lt;&lt;endl;&#125;/*now size 0 now capacity 0now size 1 now capacity 1now size 2 now capacity 2now size 3 now capacity 4now size 4 now capacity 4now size 5 now capacity 8now size 6 now capacity 8now size 7 now capacity 8now size 8 now capacity 8now size 9 now capacity 16now size 10 now capacity 16now size 11 now capacity 16now size 12 now capacity 16now size 13 now capacity 16now size 14 now capacity 16now size 0 now capacity 16*/ 我们运行上面的代码可以发现vector数组是倍增的，同时clear函数不会改变申请的内存的容量。 sort函数排序的方式相对也比较简单，具体来说有几种方式。 默认的类型时使用greater方法 1234567891011121314#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; a;int main(void) &#123; int i; for(i=0;i&lt;15;i++)&#123; a.push_back(rand()); &#125; //sort(a.begin(),a.end());//增序 sort(a.begin(),a.end(),greater&lt;int&gt;());//降序 for(i=0;i&lt;15;i++)&#123; cout&lt;&lt;a[i]&lt;&lt;endl; &#125;&#125; 定义cmp函数 1234567891011121314151617181920#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; a;int cmp1(int x,int y)&#123; return x&lt;y;//增序&#125;int cmp2(int x,int y)&#123; return x&gt;y;//降序&#125;int main(void) &#123; int i; for(i=0;i&lt;15;i++)&#123; a.push_back(rand()); &#125; //sort(a.begin(),a.end(),cmp1); sort(a.begin(),a.end(),cmp2); for(i=0;i&lt;15;i++)&#123; cout&lt;&lt;a[i]&lt;&lt;endl; &#125;&#125; 友元函数重载运算符 12345678910111213141516171819202122#include&lt;bits/stdc++.h&gt;using namespace std;struct node &#123; int x,y; friend bool operator &lt; (node A,node B)&#123; return A.x&lt;B.x; &#125;&#125;;vector&lt;node&gt; a;int main(void) &#123; int i; node t; for(i=0;i&lt;15;i++)&#123; t.x=rand(); t.y=rand(); a.push_back(t); &#125; sort(a.begin(),a.end()); for(i=0;i&lt;15;i++)&#123; cout&lt;&lt;a[i].x&lt;&lt;" "&lt;&lt;a[i].y&lt;&lt;endl; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进制HASH]]></title>
    <url>%2F2019%2F02%2F05%2FHash%2F</url>
    <content type="text"><![CDATA[HASH为了加速字符串的检索。我们通常采用的方法是对字符串进行加密，加密的方式有很多种，有单纯为了检索是否原串存在的，有加密后再解密的算法(典型的就是RSA公钥私钥算法)。本文讨论第一种，就是检索原串是否存在。 Hash的基本思想哈希算法其实是一种概率算法，它希望我们对串进行一系列操作，使得每一个串都能对应一个独一无二的数。我们知道64位计算机最大能存储的数为18446744073709551615，也就是$2^{64}-1$，这其实已经是一个相当大的数了。假如我们只有$10^{15}$个串的话，我们希望每个串对应上面中的一个数，冲突的概率其实是无限接近于0的。但是我们要知道$10^{15}$其实是一个相当大的数了，基本上完全可以满足我们的需求。于是我们希望对字符串设计一种方法，使得字符串的对应的值尽量唯一。 进制hash一种十分经典的思想就是进制哈希，这种哈希方式写起来十分简单，意思就是我们将原字符串就视为一个其他进制的数，之后我们用二进制表示这个数，就得到了一个很好的hash结果。我们在操作过程中通常取原串的进制为131。采用unsigned long long方式存储，自然溢出得到结果，当然不自然溢出也是可以的。因为从概率来说几乎都是不存在冲突的可能。除非刻意构造数据。 1234567891011121314151617181920#include&lt;bits/stdc++.h&gt;using namespace std;typedef unsigned long long ull;ull prime=1000000007,base=131;ull mod=212370440130137957ll;ull HASH(string s)&#123; ull ans=0; for(int i=0;s[i];i++)&#123; ans=(ans*base+(ull)s[i])%mod+prime; &#125; return ans;&#125;int main(void)&#123; ull a; string s; while(1)&#123; cin&gt;&gt;s; cout&lt;&lt;HASH(s)&lt;&lt;endl; &#125;&#125; 多重进制hash有时候我们为了进一步减少冲突，往往采用好几组$base$和$prime$。如果对每一组$base$和$prime$都一样的话，我们才认为相同，正确性也是显然的。在采用多组的$base$和$prime$之后，几乎是找不到冲突的数据的。实现起来也很简单。 1234567891011121314151617181920212223ull ans[100005][5];ull p[]=&#123;&#125;,BASE[]=&#123;&#125;;void MULTIHASH(int x,string s)&#123; int i,j; for(i=0;i&lt;5;i++)&#123; ans[x][i]=0; &#125; for(i=0;s[i];i++)&#123; for(j=0;j&lt;5;j++)&#123; ans[x][j]=(ans[x][j]*base[j]+(ull)s[i])%mod+prime; &#125; &#125;&#125;int main(void)&#123; ull a; string s; int n; cin&gt;&gt;n; for(i=0;i&lt;n;i++)&#123; cin&gt;&gt;s; MULTIHASH(i,s); &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>HASH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性基]]></title>
    <url>%2F2019%2F02%2F03%2FLinearBase%2F</url>
    <content type="text"><![CDATA[线性基我们在碰到XOR问题的时候，通常难以下笔。这是因为XOR问题的解法通常难以构造，直观上建模比较困难。为了解决这一类问题，线性基由此而提出。通过我们所熟知的XOR的一些完美的性质，于是我们可以把一堆数压缩成64个数，可以保证这64个数的异或结果的值域与原数组的值域一样。而这个压缩的过程就是线性基的构造过程，我们理解起来也很简单。 基上面所说的压缩成64个数也许你会有一些疑问，我们不妨设想，现在有一组二进制基:1，10，100，1000，…。我们其实可以通过这一些基相互异或得到$[0,2^{64}-1]$区间内所有的数。但是我们需要构造与原数组所有子集异或值域一样的基，就被称作线性基，线性基保证了产生的基的个数最少。类似于线性代数中的极大无关组。所谓基的概念也是从线性代数中引用过来的。 构造方法1首先我们申请一个数组$base[64]$，对于$base[i]$而言存储的是二进制中最高位的1在第$i$位的数。64表示二进制的数位，最高为64位，可以向上叠加。 我们对原数组中每一个数$x$，从高位到低位遍历，若当前最高位$i$为1，我们检查$base[i]$是否有值，有值的话，我们令$x=x$^ $base[i]$，否则我们令$base[i]=x$ 构造代码1123456789101112131415long long base[100];scanf("%lld",&amp;n);for(i=0; i&lt;n; i++) &#123; scanf("%lld",&amp;a); for(j=63; j&gt;=0; j--) &#123; if(a&amp;(1LL&lt;&lt;j)) &#123; if(base[j]) &#123; a^=base[j]; &#125; else &#123; base[j]=a; break; &#125; &#125; &#125;&#125; 构造方法2使用高斯消元的方法进行求线性基，这种构造线性基的方式相对麻烦一些，但是它保证了一个非常良好的性质，就是保证了，线性基的异或值从高位到低位的结果是单增的，我们使用这种线性基的话，很容易解决XOR求取第$k​$大问题。构造过程和构造方法1十分类似。 构造代码21234567891011121314151617181920212223242526long long a[10005];int n;scanf("%d", &amp;n);for (int i = 0; i &lt; n; i++) &#123; scanf("%lld", &amp;a[i]);&#125;int m = 60;int dim = 0;for (int i = m - 1; i &gt;= 0; i--) &#123; int t = -1; for (int j = dim; j &lt; n; j++) &#123; if (a[j] &gt;&gt; i &amp; 1) &#123; t = j; break; &#125; &#125; if (t == -1) continue; if (t != dim) swap(a[dim], a[t]); for (int j = 0; j &lt; n; j++) &#123; if (j != dim &amp;&amp; (a[j] &gt;&gt; i &amp; 1)) &#123; a[j] ^= a[dim]; &#125; &#125; dim++;&#125;//最后的0-dim维就是求得的线性基，这里升序排序。 线性基的性质线性基的任何一个非空子集都不会使得其$xor​$和为0，证明方法使用反证法即可。这个性质可以保证线性基的算法的合法性。 来解决一些问题吧异或最大值问题给我们一堆数，我们希望知道哪些数相互异或后结果最大，输出最大的值即可。 解法我们使用首先构造线性基，初始化$ans=0$，之后对于线性基从高位向低位扫，若当前的$base[i]$与ans异或后结果更大，那么就异或。否则跳过这一位即可。 若我们使用第二种线性基构造方式，将得到的线性基全部异或起来就好了。 异或判断问题给我们一堆数，再给我们一个数$x$，判断$x$是否可以由这些数的一个子集异或得到。 解法我们首先构造线性基，之后对于$x$，我们只需要对于$x$从高位到低位每个1与$base[i]$异或即可，若最后$x$的值为0，那么就可以被构造出来，否则无法被构造出来。其实意是也是很简单的，就相当于插入过程，如果$x$不需要导出新的$base[i]$，说明$x$无用，那么就一定可以被构造出来。否则需要导出新的$base[i]$来辅助构造$x$。 CF1011G将原数组分为几段，每段内异或得到一个值，且任何一些连续段的异或值都不为0，最多能够划分多少段？ 解法这个题目用到了上文介绍到的性质，线性基的任何一个非空子集都不会使得其$xor$和为0。于是本题的答案就是构造线性基的结果中,$base[i]$不为0的个数。 异或种数问题给我们一堆数，问这些数的所有子集异或所得到的数有多少种？ 解法答案就是2^线性基中基的个数。 一堆数中第k大的异或值问题给我们一堆数，我们可以从中挑一些数出来，进行异或，这样的话我们可以得到很多不同的异或值，问我们第k大的结果是什么？ 解法这个题目我们只能使用第二种线性基构造方式构造线性基，特判0的存在之后，将k的每一位为1的相应的线性基对应的值异或起来就好了。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>LinearBase</tag>
        <tag>codeforces</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小问题集合-长期更新]]></title>
    <url>%2F2019%2F01%2F31%2FProblemSet%2F</url>
    <content type="text"><![CDATA[这是一些有意思的小问题，长期更新题目 现在有$n$类钞票，每种有无限张，我们从中带走一些钞票。给定一个数$k$，希望我们保证$[1,k]$这个面值区间的每一个数我们都可以从带走的钞票中凑出来。问对于给定的$k$，我们至少要需要带走几张钞票，不能满足输出$-1$。$n&lt;1e5,k&lt;1e9$ 给定一个$1$到$n$的排列，求一个子序列使得其逆序对数与长度比值最大，输出这个比值。$n&lt;100$ 给定两个1到$n$的排列，求这两个排列的最长公共序列。$n&lt;1e5$ 给定一个$n*n$矩阵，其中数字各不相同，找到一个矩阵的局部最小(大)值的坐标输出即可。$n&lt;1e5$ 给出$n$个数，这些数互不相同，我们希望删除其中的一个数，使得剩余的所有数异或和最大。$n&lt;1e 5$ 给出$n$个数，这些数互不相同，我们希望删除其中的两个数，使得剩余的所有数异或和最大。$n&lt;1e 5​$ 给出$n+1$个数，每个数都在$[1,n]$之间，且只有一个数出现两次，要求$O(n)$时间$O(1)$空间找出。 给$n$个数，保证存在唯一一个出现奇数次的数，找到这个数。$n&lt;1e5$ 给$n$个数求最大最小数。比较次数控制在$1.5n$次之内。 在一大堆数中求top100，要求时间复杂度小于$O(nlogn)$ 解法 我们使用贪心的思想，有一个小trick就是我们对于选中的数集的和$sum$而言，我们下一次选的数一定是在$sum+1$之内的，所以只需要每次贪心的选取就可以了。时间复杂度$O(\log k)$。 遇到这种比值的且$n$较小的，一般都是最大密度子图问题，这个就是一个裸题，但是很多时候我们往往想不到。时间复杂度$O(E100^2)$ 由于排列当中没有相同的数字，那么我们可以把问题变成最长上升序列问题。时间复杂度$O(n\log n)$ 这个题目明显是一道分治的题目，我们可以每次递归问题到一个小矩阵，做法很简单，我们至于需要见检查中间一列，中间一行是否有数满足条件，若满足条件，输出。不满足条件，向检查过程中最小值(一定在小矩阵内部)出现的那一个矩阵进行递归即可。时间复杂度$O(n)$ 我们在原数组中删除一个数$x$，相当于在数组中添加一个数$x$。这是异或有意思的性质。于是我们直接拿最初的异或结果$sum$，向原数组中的每个数异或一遍得到最大的结果就可以了。时间复杂度$O(n)$ 我们在原数组中删除两个数$x，y$，相当于在数组中添加两个数$x，y$。我们首先构造一个01字典树,树高为64。于是我们直接拿最初的异或结果$sum$，向原数组中的每个数异或，接下来我们只需要按照异或结果的反方向在树上走一遍就好了，之后我们可以得到答案。时间复杂度$O(64n)$ 我们只需要对原数组求和之后减去$n*(1+n)/2$即可。时间复杂度$O( n )$ 我们只需要对原数组的所有数异或一边即可。时间复杂度$O( n )​$ 我们对$n$进行两两分组，之后两个数之间先确定大小。再与当前最大值与当前最小值进行比较，即这里可以省去不必要的比较。 我们固定一个大小为100的小根堆，每次插入，调整，删除即可。时间复杂度$O(nlog100)$]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Nice-Problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最短路问题]]></title>
    <url>%2F2019%2F01%2F11%2FMinDistance%2F</url>
    <content type="text"><![CDATA[最短路问题最短路问题是一类十分典型的图论问题，给定一个图，求起点$s$至终点$t$的最短路径。使用数学语言描述为： 给定图$G(V,E)$ $e=(from,to,distance)\in E$ 表示边集$E$中的边有三个参数表示起点，终点，距离 $s,t\in V$ 起点终点都属于点集$V$，求最短距离$MinDis(s,t)$ 这类问题理解起来也很简单，典型的问题就是火车买票问题，求上海到北京怎么买票花费最少，或者用时最短，这都可以建模为最短路问题。关于最短路问题，分为两类： 单源最短路 多源最短路 虽然多源最短路可以通过多次调用单源最短路的算法达到效果，但是对于一些算法的出发点是不同的，所以还是归为两类。对于多源最短路问题，最典型的算法是Floyd算法，通过$O(V^3)$的时间可以求出来任意两点的最短路径。对于单源最短路问题，方法多种多样，代表性的算法是Bellman-ford、Spfa、Dijkstra。本文将会依次介绍上述4种算法。 Bellman-ford我们按照时间顺序介绍上述算法，在1956年，提出了第一个基于动态规划思想的最短路算法Bellman-ford。这个算法为后面的所有算法打下了理论基础，所以Bellman-ford算法是一个非常值得我们仔细思考的算法。 首先回到我们的问题，我们希望求$s-t$的最短路径，由于我们中间最多有$V-2$个节点，经过$V-1$条不同路径，那么我们相当于我们从$s$出发，每次尝试一下$V$种节点，那么我们总共会有$(V-1)^{V}$可能的路径，这个估计明显是个上界，显然是不可解的。于是我们采用动态规划化简上面的求解过程，仔细想想，这个过程和和隐马尔可夫模型中的Viterbi算法的过程一样。，所以我认为Viterbi算法(1967年提出)极大的受到了Bellman-ford算法的启发现在看来神经网络还真是玄学啊，感觉和之前的算法没什么共通性我们定义状态转移方程：$$dp[i][step]=min(dp[i][step-1],dp[j][step-1]+dis[j][i])\quad j=1,2,3,…,n$$上面的dis数组表示节点$j$与节点$i$之间的距离。step表示当前最多允许通过几个节点。 注意：我们使用Bellman-ford算法还有一个原因，就是Bellman-ford可以用来判断负环，负环的意思就是说网络中存在一个环，环上的边的所有权值相加为负数。这样的话，我们找最短路，就可以一直沿着这个环走，从而得到一个无穷小的值，但是这显然是不合理的。所以我们可以通过Bellman-ford算法进行检测，检查的方式也很简单，我们在运行了$V-1$次程序之后，再运行一次，如果存在负环的话，那么我们对每个环至少可以走一圈，也就是说，如果还有节点的最短距离可以继续缩短的话，那么就存在负环，否则就不存在。请好好思考为什么，如果不明白这个，那么我只能说你还没有掌握到Bellman-ford算法的精髓。 代码实现本文的代码能够AC HDU-2544。 12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;pair&lt;int,int&gt; &gt; vec[1005];//end-distanceint dp[1005][1005],n,m,INF=1e9+7;int BellmanFord(int s,int t) &#123; int i,j,step; for(i=0; i&lt;n; i++) &#123; for(j=0; j&lt;n; j++) &#123; dp[i][j]=INF; &#125; &#125; for(i=0; i&lt;n; i++) &#123; dp[s][i]=0; &#125; for(step=1; step&lt;n; step++) &#123; for(i=0; i&lt;n; i++) &#123; for(auto x : vec[i]) &#123; dp[i][step]=min(dp[i][step],min(dp[i][step-1],dp[x.first][step-1]+x.second)); &#125; &#125; &#125; return dp[t][n-1];&#125;int main(void) &#123; int i,j,a,b,c; while(scanf("%d %d",&amp;n,&amp;m)!=EOF) &#123; if(n==0&amp;&amp;m==0) break; for(i=0; i&lt;n; i++) &#123; vec[i].clear(); &#125; for(i=0; i&lt;m; i++) &#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); a--; b--; vec[a].push_back(make_pair(b,c)); vec[b].push_back(make_pair(a,c)); &#125; printf("%d\n",BellmanFord(0,n-1)); &#125;&#125; 时间复杂度分析很明显Bellman-ford算法需要更新$V-1$次，同时每一次我们需要对每条边进行检查，时间复杂度为$O(VE)$ SpfaSpfa的全称是shortest path faster algorithm。这个算法的名字倒还是蛮中肯，并没有说是fastest。这个算法也是一种单源最短路径算法，其本质是对Bellman-ford算法的队列优化。它省去了一些冗余操作。其想法主要在Bellman-ford的计算方式上。我们再次看看状态转移方程：$$dp[i][step]=min(dp[i][step-1],dp[j][step-1]+dis[j][i])\quad j=1,2,3,…,n$$我们很可能存在这样一种情况，对于第$step$轮循环来说$dp[j][step-1]$的值和第$step-1$的$dp[j][step-1]$是一样的，同时我们在上一轮已经计算过了，那么对于这一轮来说，再计算完全是浪费的计算，我们可不可以减少这样的冗余计算呢？ 其实改进的方式也很简单，我们在递推过程中，如果当前的最短距离的值减小了，我们才用这个减小的值去更新别的值，直到所有的节点的距离不再减小。换句话说，当我们检查到当前的节点的最短距离变小了，我们才用这个最短距离去检查是不是能让其相邻节点的最短路也减小距离，请注意我们并不是立即更新，而是放入队列进行等待，依次更新，这样是有很大好处的，请仔细想想为什么我们会这样操作。我们每次更新的这种操作也叫松弛操作。 注意：Spfa也是可以判断负环的，判断的方式也很简单，只需要对每个节点检查入队次数即可，因为一个节点入队的次数是不会超过$n$次的，所以当一个点入队次数超过$V$，就肯定存在负环。因为一个点到源点的距离不可能被更新$V$次，因为路径上最多只有$V-1$个节点，不存在还有其他节点能够再松弛距离。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;pair&lt;int,int&gt; &gt; vec[1005];int dp[1005],n,m,INF=1e9+7,vis[1005];queue&lt;int&gt; que;int Spfa(int s,int t)&#123; int i,T; for(i=0; i&lt;n; i++) &#123; dp[i]=INF; &#125; dp[s]=0; que.push(s); vis[s]=1; while(que.size())&#123; T=que.front(); vis[T]=0; que.pop(); for(auto x : vec[T])&#123; if(dp[x.first]&gt;dp[T]+x.second)&#123; dp[x.first]=dp[T]+x.second; if(!vis[x.first])&#123; vis[x.first]=1; que.push(x.first); &#125; &#125; &#125; &#125; return dp[t];&#125;int main(void) &#123; int i,j,a,b,c; while(scanf("%d %d",&amp;n,&amp;m)!=EOF) &#123; if(n==0&amp;&amp;m==0) break; for(i=0; i&lt;n; i++) &#123; vec[i].clear(); &#125; for(i=0; i&lt;m; i++) &#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); a--; b--; vec[a].push_back(make_pair(b,c)); vec[b].push_back(make_pair(a,c)); &#125; printf("%d\n",Spfa(0,n-1)); &#125;&#125; 时间复杂度分析Spfa的时间复杂度不是很好分析。。虽然Spfa大部分时候都比较快，但是某些精心构造的图上可以让spfa跑的很慢，目前证明的结果是Spfa的时间复杂度和Bellman-ford是一样的，最坏情况下时间复杂度为$O(VE)$。但是对于大部分含有噪声的数据上跑的还是很快，这也是我们为什么说噪声是个好东西，实际会让你的算法效果变的更优。 Flody弗洛依德算法是一个多源最短路径算法，这个算法的发明人曾是一个文科专业毕业的学生，由于难以找到工作，转行计算机，但是却在计算机行业干的风生水起，最后走上人生巅峰拿了图灵奖。废话说了一堆，让我们聊一聊这个大名鼎鼎的算法，这个算法美就美在它只用了几行代码就实现了多源最短路。废话不多说，先看代码。 代码实现1234567int floyd(int s,int t)&#123; for(k=0;k&lt;n;k++) for(i=0;i&lt;n;i++) for(j=0;j&lt;n;j++) dis[i][j]=min(dis[i][j],dis[i][k]+dis[k][j]); return dis[s][t];&#125; 没错，就是这么简单粗暴，代码就是这么精简，但是却实现多源最短路这个神奇的任务，这是为什么呢？很多人都会说Flody算法没什么啊，很显然啊，就是对每个点都中转一次而已，没什么难度。但是事实真的是这么显然吗？为什么我没看出来？既然都是中转一遍，为什么k一定要在外层，我可以把k放在里层吗？也请读者你再带着怀疑的眼光再观察这份代码，好好想想，我觉得很有可能出现$dis[i][k]$并没有达到最优情况或者$dis[k][j]$没有达到最优情况。请在思考后再看下面的解释，我认为这会更加帮助你理解Floyd那精妙的想法。 时间复杂度分析简单粗暴的$O(V^ 3)$的时间复杂度，其实对于多源最短路来说，这个复杂度并不糟糕，我们调用$V$次Bellman-ford所需要花的时间是$O(V^2E)$但是$E$在稠密图的话，量级是趋于$V^2$的，所以对比最坏情况来说，Floyd算法不仅在数量级上取得了质的改变，同时代码的量也极其精简。 正确性证明Floyd算法的正确性真的就这么显然吗？我不是这么认为的，现在我们使用数学方法来说明为什么这样写是正确的。 我们假设源点$i$到汇点$j$中最优路径需要经过的最大节点为$x$。我们需要说明在$k=x$这一轮结束后，$dis[i][j]$一定得到了最小值。也就是说$dis[i][k],dis[k][j]$已经为最小值了。其实到此为止我们就已经发现最优子结构了，原本是不需要再往下解释的，但是我们再详细解释一下。 假设我们此时对$dis[i][k]$继续进行递归。而此时最大的为$x1$。我们可以知道$x1&lt;x$，再进行递归下去。 最后我们可以得到$x_n$是最小的需要经过的节点，而这个节点肯定是与$s$直接连接，而无法被减小的。那么我们可以回到上一层，通过这样的归纳，我们可以证明$dis[i][k]$与$dis[k][j]$一定是最小值。那么$dis[i][j]$通过$k=x$这一轮循环，一定可以得到最小值。 DijkstraDijkstra算法，就是目前最常使用的单源最短路算法，他的想法也是从Bellman-ford算法得到的，他通过仔细观察Bellman-ford算法的计算过程，总结出了一个极其优美的规律，那就是我们每一轮的松弛操作只需要对当前距离源点$s$最近的且没有被访问过的节点开始松弛就可以了，其他的节点我们不需要松弛。但是这是为什么呢？我的理解是因为我们既然做Bellman-ford的松弛操作。我们肯定是希望和spfa那样，选择最有可能减小其他节点的最短路的点去进行松弛操作，而这个点就是最近的且没有被访问过的节点。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;cstdio&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;int dis[1005],vis[1005],ma[1005][1005],INF=1e9+7,n,m;int dijstra(int x,int y)&#123; int i,j,t; for(j=1;j&lt;=n;j++) dis[j]=ma[x][j]; memset(vis,0,sizeof(vis)); vis[x]=1; while(true)&#123; t=-1; for(i=1;i&lt;=n;i++)&#123; if(dis[i]!=INF&amp;&amp;!vis[i])&#123; if(t==-1||(dis[t]&gt;dis[i])) t=i; &#125; &#125; if(t==-1) break; vis[t]=1; for(i=1;i&lt;=n;i++)&#123; dis[i]=min(dis[i],dis[t]+ma[t][i]); &#125; &#125; return dis[y];&#125;int main(void)&#123; int T,i,j,a,b,c,t,res; while(scanf("%d %d",&amp;n,&amp;m)!=EOF)&#123; if(n==0&amp;&amp;m==0) break; for(i=0;i&lt;1005;i++)&#123; for(j=0;j&lt;1005;j++)&#123; ma[i][j]=INF; &#125; &#125; for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;a,&amp;b,&amp;c); ma[a][b]=min(ma[a][b],c); ma[b][a]=ma[a][b]; &#125; res=dijstra(1,n); printf("%d\n",res); &#125;&#125; 时间复杂度分析上面的代码是最简单的dijkstra实现的方法，采用的是邻接矩阵存储。时间复杂度为$O(n^2)$。当然我们可以采用二叉堆、Binomial堆、斐波那契堆进行优化，得到$O(n\log n)​$级别的Dijkstra算法。在此不再赘述。]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>graph-theory</tag>
        <tag>Bellman-ford</tag>
        <tag>Dijkstra</tag>
        <tag>Spfa</tag>
        <tag>Floyd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大流问题]]></title>
    <url>%2F2019%2F01%2F09%2FMaxFlow%2F</url>
    <content type="text"><![CDATA[最大流问题最大流问题是算法竞赛中经常考察的问题，其目标是解决这样一类问题：给定源点$s$和汇点$t$，给定包含源点汇点的网络，网络中每条边有其相应的所能经过的最大流量，求源点至汇点最多能有多少流量。用数学语言描述为： 给定有向图$G(V,E)$ $V$表示点集 $E$表示边集，其中每条单向边由三元组组成，$(from,to,cap)\in E$。表示起点、终点、每条边的容量。 给定源点$s$，汇点$t$，求$s$到$t$的最大流量$MaxFlow(s,t)$ 目前已经有相对较为快速的算法（EdmondKarp算法，Dinic算法）而其难点主要在于如何构建网络模型。相对常见的问题主要分为几类： 最小点/边独立集 最小点/边覆盖集 最小割问题 拆点建图 本文将介绍Ford-Fulkerson方法，EdmondKarp算法和Dinic算法。 Ford-Fulkerson方法Ford-Fulkerson算法是最早提出来的用于解决最大流问题的方法，之所以称为方法，是因为它并不严谨，它仅仅是一颗seed，它发展壮大形成了很多有趣的算法，许多科研前辈为了培养这颗seed，花费了毕生精力。其思想十分简单，每次在残余网络中寻找从源点$s$到$t$的增广路，若存在，那我们就从中任取一条，之后答案添加相应的流量，同时添加反向边再形成残余网络，若不存在，那么算法结束，我们已经得到了最大流。 什么是残余网络我们举个例子来说： 上面的这个图，其中边上的参数$a/b$表示：目前通过流量和该边的流量最大限制。由图可知，我们可以得到这两种调度方案。第一种方案的所能运输的最大流量为1，而第二种方案所能运输的最大流量为2。这个问题就很尴尬了，对于给定的图，我们怎么确保算法可以得到最优解呢？这个时候残余网络的出现给了我们一个可靠的解决方案。对于第一个图，我们选则之后，相应的添加反向边，得到下面的图： 我们可以在选择了这条路径之后，可以添加红色的反向边得到残余网络，之后在残余网络中再寻找增广路，仍然可以得到最大流为2。 所谓残余网络其实就是在中途建立退货边，让算法有一个反悔的机会，这真是一个很好的思想！很多时候这个思想可以帮助我们解决很多问题。 时间复杂度分析Ford-Fulkerson方法确实很妙，但是它的时间复杂度分析就不忍直视了，我们假设每次考虑最坏情况，首先花$O(E)$的时间BFS得到所有$s-t$的增广路，但是不巧每次挑选的增广路径的流量都为1。那么，算法的时间复杂度就为$O(FE)$，其中$F$为最大流量。当然这个时间复杂度很高，但是它跑起来还是很快的，因为实际生活中几乎是碰不到这样的图的，相对而言，这个方法还是可以用的。 Ford-Fulkerson的扩展对于Ford-Fulkerson方法的改进主要有两种。 每次找一个$s-t$流量最大的增广路 每次找一个$s-t$距离最短的增广路 对于每次找一个流量最大的增广路，想法就很简单了，就是为了更快的结束算法，但是完成这个目的太慢了。我们考虑一个更简单的问题，每次找一个尽量大的增广路，为了完成这个目标，我们仅仅只需要套一个二分就行了。 对于每次找一个最短的增广路，产生了很多分支，接下来我们介绍EdmondsKarp算法和Dinic算法，同时也解决了最小费用最大流的问题。 EdmondsKarp算法该算法是1972年提出的，它的效率比1970年提出的Dinic算法要差，而这个算法也是Dinic玩剩下的东西，但是为什么paper还是能发出来呢？这真是一个有意思的问题。当时美国和苏联冷战，科研成果并不互通，美国人还不知道Dinic那精妙的思想。 EdmondsKarp算法改善了Ford-Fulkerson中在残余网络中挑选增广路的过程，EdmondsKarp遵循的原则是每次挑选$s- t$最短的增广路，正确性是显而易见的。通过这样的改进，算法的复杂度从$O(FE)$降低到了$O(VE^2)$。 时间复杂度分析这个时间复杂度的分析其实就是Dinic算法的精髓所在，为了解释这个时间复杂度，我们引入分层网络这个概念。 分层网络我们举个例子来说： 左边是我们当前的残余网络，中间的图表示的就是从源点$s$到$t$中经过的分层后的形状。其中相同颜色的节点表示的是到$s$的距离相同，也就是节点的深度$d$相同，由于我们每次找的是$s - t$的最短路这样的话，我们每次最优的增广路肯定是不含相同颜色的，因为若含相同颜色，肯定不是最短的路径了。同样的，最右边的图表示去掉与$t$相同颜色的点，同时将不同层的边补齐，同时也想想为什么相同层不补齐呢？。 现在我们考虑这样一个情况。 我们找到了一条$s - t$的增广路，但是限制最紧的边为$u-v$这条边，那么我们在建立完反向边之后，$v$在分层图中肯定是向后延伸或者不变的，于是我们得到$d_{f’}(v)&gt;=d_f(v)$。但是如果红色的边想再次做限制最紧的点的话，$v$的层次肯定比$u$的层次低一级，于是当这条边再做限制最紧的边时，必然满足$d_{f’’’}(u)&gt;=d_f(u)+2$,$d_{f’’’}(v)&gt;=d_f(v)+2$。等于说，完成这样的一次交换，$u$和$v$的层次都向后延伸了2。由于层次大于等于汇点$t$的层次的话，这个点肯定就不在最短路上了，同时$t$的层次最大就是$V$。那么前面的操作最多只有$\frac V 2$次。相当于每条边最多只有$\frac V 2$次机会做限制最紧的边。 时间复杂度的计算那么对于每条边而言，这一部分的时间复杂度为$O(\frac V 2 E)$。同时我们每次BFS找最短路的时间复杂度为$O(E)$。那么总的时间复杂度就是$O(VE^2)$。通过上面的分析，实际上这个复杂度也是虚高的，通常不需要跑这么久，而且对于某些图跑的比Dinic还要快，但是为什么会快一些呢？如果后面你的Dinic你真正理解的话就再回头想想为什么。 Dinic算法这个算法是真的牛逼，因为EdmondKarp算法其实是Dinic的副产物。通过上面的介绍，我们可以知道，EK算法每次BFS得到最短路径，之后计算的结果就抛弃了，马上构建反向边，在残余网络中重新计算。但是实际上，我们可以利用之前的结果，我们可以通过一次BFS得到每个节点的深度，之后补上相邻层的边之后得到一个分层网络。我们通过上面的分析，$s - t$的增广路的最短路的长度肯定是每次肯定是非递减的！虽然直观上也是这样，那么这也就意味着，我们构建一次分层网络，可以直接把当前分层网络的最短路全部构建反向边，因为不这样做，下次我们找到的最短路还是这些增广路，因为它们已经是最短的了！！！(仔细想想和Dijkstra算法的思想有共同之处啊)我们通过这样做，简单来说我们一次BFS的结果找到了多条路径。 再次看看这个图，我们构建了一次分层网络，可以一次性找到3条增广路呢，我们依次构建反向边即可。要注意每次边权都要更新。 时间复杂度分析按照Dinic的思想，我们每次构建分层网络的时间是边数$O(E)$，每次我们对最短路建反向边只需要对每个节点遍历一边即可$O(V)$，由于每次这样操作一次后，$t$的深度，也就是$d_t$至少会向后延伸1，也就是说$d’_t&gt;=d_t+1$。同样的道理，深度最多是一条链，那么$d_t$最多增长$V$次。所以总时间就是$O(V^2E)$。而这个复杂度确实是相对比较满的，我们的确可以构造出来这样的图，让Dinic跑满，当然Dinic有弧优化技巧尽量避免这些情况，当然这都是后话了，本文暂不做讨论。 代码实现最大流入门题目，HDU-3549，以下是EK和Dinic的代码实现。 EdmondsKarp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;bits/stdc++.h&gt;using namespace std;const int INF = 0x3f3f3f3f;const int maxn = 5000;struct Edge&#123; int from, to, cap, flow; Edge(int u, int v, int c, int f):from(u), to(v), cap(c), flow(f) &#123;&#125;&#125;;struct EdmondsKarp&#123; int n, m; vector&lt;Edge&gt; edges; vector&lt;int&gt; G[maxn]; int a[maxn]; int p[maxn]; void init(int n) &#123; for (int i=0; i&lt;n; i++) G[i].clear(); edges.clear(); &#125; void AddEdge(int from, int to, int cap) &#123; edges.push_back(Edge(from, to, cap, 0)); edges.push_back(Edge(to, from, 0, 0)); m = edges.size(); G[from].push_back(m-2); G[to].push_back(m-1); &#125; int Maxflow(int s, int t) &#123; int flow = 0; for (;;) &#123; memset(a, 0, sizeof(a)); queue&lt;int&gt; Q; Q.push(s); a[s] = INF; while (!Q.empty()) &#123; int x = Q.front(); Q.pop(); for (int i=0; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (!a[e.to] &amp;&amp; e.cap &gt; e.flow) &#123; p[e.to] = G[x][i]; a[e.to] = min(a[x], e.cap-e.flow); Q.push(e.to); &#125; &#125; if (a[t]) break; &#125; if (!a[t]) break; for (int u=t; u!=s; u=edges[p[u]].from) &#123; edges[p[u]].flow += a[t]; edges[p[u]^1].flow -= a[t]; &#125; flow += a[t]; &#125; return flow; &#125;&#125;EK;int main(void)&#123; int T,i,j,f,t,c,ca=1,n,m; scanf("%d",&amp;T); while(T--)&#123; scanf("%d %d",&amp;n,&amp;m); EK.init(maxn); for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;f,&amp;t,&amp;c); EK.AddEdge(f,t,c); &#125; printf("Case %d: %d\n",ca++,EK.Maxflow(1,n)); &#125;&#125; Dinic123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;queue&gt;#include &lt;vector&gt;using namespace std;const int INF = 0x3f3f3f3f;const int maxn = 50000;struct Edge&#123; int from, to, cap, flow; Edge(int u, int v, int c, int f):from(u), to(v), cap(c), flow(f) &#123;&#125;&#125;;struct Dinic&#123; int n, m, s, t; vector&lt;Edge&gt; edges; vector&lt;int&gt; G[maxn]; int d[maxn]; bool vis[maxn]; int cur[maxn]; void init(int n) &#123; for (int i=0; i&lt;n; i++) G[i].clear(); edges.clear(); &#125; void AddEdge(int from, int to, int cap) &#123; edges.push_back(Edge(from, to, cap, 0)); edges.push_back(Edge(to, from, 0, 0)); m = edges.size(); G[from].push_back(m-2); G[to].push_back(m-1); &#125; bool BFS() &#123; memset(vis, false, sizeof(vis)); queue&lt;int&gt; Q; Q.push(s); d[s] = 0; vis[s] = true; while (!Q.empty()) &#123; int x = Q.front(); Q.pop(); for (int i=0; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (!vis[e.to] &amp;&amp; e.cap&gt;e.flow) &#123; vis[e.to] = 1; d[e.to] = d[x]+1; Q.push(e.to); &#125; &#125; &#125; return vis[t]; &#125; int DFS(int x, int a) &#123; if (x == t || a == 0) return a; int flow = 0, f; for (int&amp; i=cur[x]; i&lt;G[x].size(); i++) &#123; Edge&amp; e = edges[G[x][i]]; if (d[x]+1==d[e.to] &amp;&amp; (f=DFS(e.to, min(a, e.cap-e.flow)))&gt;0) &#123; e.flow += f; edges[G[x][i]^1].flow -= f; flow += f; a -= f; if (a == 0) break; &#125; &#125; return flow; &#125; int Maxflow(int s, int t) &#123; this-&gt;s = s; this-&gt;t = t; int flow = 0; while (BFS()) &#123; memset(cur, 0, sizeof(cur)); flow += DFS(s, INF); &#125; return flow; &#125;&#125;DN;int main(void)&#123; int T,i,j,f,t,c,ca=1,n,m; scanf("%d",&amp;T); while(T--)&#123; scanf("%d %d",&amp;n,&amp;m);//点数，边数 DN.init(maxn);//初始化邻接表 for(i=0;i&lt;m;i++)&#123; scanf("%d %d %d",&amp;f,&amp;t,&amp;c); DN.AddEdge(f,t,c); &#125; printf("Case %d: %d\n",ca++,DN.Maxflow(1,n));//表示源点是1，汇点是n &#125;&#125; 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>MaxFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈QuickSort]]></title>
    <url>%2F2019%2F01%2F06%2FQuickSort%2F</url>
    <content type="text"><![CDATA[快速排序快速排序作为目前我们使用的最常用的排序方式，快速排序平均复杂度的级别和大多数排序都一样为$O(n\log n)​$，但是他的常数相对较小，同时思想也是更加简单，实现过程中也不需要其他的数组进行辅助排序。无论是时间上还是空间上都可以做到极致。但是它的各个细节你真的知道吗？ 原始快速排序在我们初次接触快速排序的时候，我们通常选择给定待排序数组中的第一个元素作为标准，进行左右划分，从而进行递归。下面给出一份基于该思想的代码： 1234567891011121314151617181920212223void QuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; QuickSort(a,L,m-1); QuickSort(a,m+1,R);&#125; 可以看到我们每次都是选择待排序数组的第一个作为标准，进行左右划分的。虽然在部分情况下很好用，但是当数据不配合我们的时候，我们就要承担一定风险，例如当传过来的数组$a$原本就是增序的情况下，这份代码就会出现最糟糕的情况，这时算法需要迭代$n$轮，由于每轮的复杂度都是$O(n)$，那么相应的时间复杂度就变成了$O(n^2)$了。 下面我们讨论的就是快速排序的各种优化方法。 随机快速排序这种想法就非常直接了，听说你想造数据搞我，那我就打乱你的数据，我们为了保证数据和正常情况下的平均复杂度一样，我们最开始就不要选择第一个元素了，我们在$$L,R$$当中随机选一个，这样的话，我们几乎可以保证算法发生最坏情况是不可能的，所以通常情况下我们使用下面这一份代码就足够了： 123456789101112131415161718192021222324void RandomQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x=rand()%(R-L+1); swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; RandomQuickSort(a,L,m-1); RandomQuickSort(a,m+1,R);&#125; 可以看到我们就是新申请了一个变量$x$，存一下应该和哪个元素作为标准即可。 但是这种优化仅仅只是开始，有很多人提出了非常有意思的并且有效的想法。 中位数快速排序这个人呢，他就想啊，我们排序的时候为什么不首先找到中位数，之后使用中位数做标准呢？这样不是很简单的就能保证算法每次都从中间划分吗?于是这个人在随机化取下标的地方做了一些修改，其实也就是借用了利用快速排序找出第k大数的思想，这个的平均时间复杂度也是O(n)，于是这个人在QuickSort中，又套了一层QucikSort。代码的框架如下： 123456789101112131415161718192021222324252627int Find_Median_ID(int a[],int L,int R,int k)&#123;&#125;void MedianQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x=Find_Median_ID(a,L,R,(R+L)/2); swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; MedianQuickSort(a,L,m-1); MedianQuickSort(a,m+1,R);&#125; 从这开始，其他的思想又出现了，到此位置，以上的快速排序的理论的最坏复杂度依然为$O(n^2)$。没有真正做到$O(n\log n)$级别。 中位数的中位数快速排序-BFPRT这个人在前者的思想上又做了一些改变，想法也是很直接，既然你找中位数这么麻烦，我就要求松一点，我不找最好的中位数，我找一个接近中位数的数。于是提出了中位数的中位数这么一个有意思的东西，算法也是十分直接的，其方法是：我们每次把原数组分为按5分组，对每一组中的5个数我们使用插入、选择、堆排等排序方式进行排序，挑出每组的中位数，每次把挑出来的每组的中位数放到当前数组前面作为中位数序列。之后我们对该段进行递归排序，返回中位数。之后使用这中位数的中位数作为标准，对原数组左右进行划分。 举个例子： 我们首先把数组按照5进行划分，最后一个长度不足5的我们舍弃不考虑。经过分组之后，我们再对中位数进行排序，返回中位数的中位数大小。通过这种方式我们还真的得到了一个理论最坏情况下是$O(n\log n)$的排序算法，虽然这个算法常数较大，实际跑起来的时候可能并不如前面的算法，但是这并不影响这个算法的价值。 代码如下： 123456789101112131415161718192021222324252627282930313233343536int BFPRT(int a[],int L,int R)&#123; int i,cnt=L; if((R-L+1)&lt;5)&#123; sort(a+L,a+R+1); return a[(R+L+1)/2]; &#125; for(i=L;i&lt;R;i+=5)&#123; sort(a+i,a+i+5); swap(a[cnt],a[i+2]); cnt++; &#125; int Median=BFPRT(a,0,cnt-1); for(i=L;i&lt;R;i++)&#123; if(a[i]==Median)&#123; swap(a[L],a[i]); break; &#125; &#125; int l=L+1,m=L,r=R; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; BFPRT(a,L,m-1); BFPRT(a,m+1,R); return a[(R-L+1)/2];&#125; 代码写的很丑，但是勉强能看看，也许会有BUG 延迟中位数快速排序由于BFPRT的选择过程实在太麻烦了，于是有人在想，你这个中位数的中位数算来算去不就是一个接近中位数的数吗？那我何必这么麻烦，我们不如随机的选，只不过我选的时候计算一下，如果当前的数在$\lfloor \frac n 4 \rfloor$到$\lfloor \frac {3n} 4 \rfloor$之间，那么我就用这些数来作为标准。由于这些数有$\frac n 2$个，每次选就相当于我们抛硬币，那么期望次数就是2次。这样的话，就算我们每次选到了边缘，也没有关系。我们最坏我们的算法也会在$O(n\log_{\frac 4 3} n)$这个限制下。这真的是一个很有意思很精髓的想法。 代码如下： 12345678910111213141516171819202122232425262728293031323334void LazyRandomQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,x,cnt; while(true)&#123; x=rand()%(R-L+1); cnt=0; for(int i=L;i&lt;=R;i++)&#123; if(a[i]&lt;=a[L+x]) cnt++; &#125; if((cnt&gt;=(R-L+1)/4)&amp;&amp;(cnt&lt;=(3*(R-L+1))/4))&#123; break; &#125; &#125; swap(a[L],a[x+L]); while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; LazyRandomQuickSort(a,L,m-1); LazyRandomQuickSort(a,m+1,R);&#125; 去重复数字的快速排序我们在写快速排序的时候，其实很多时候都浪费了计算资源，比如说我们之前我们每次进行子问题递归的时候，区间都是$L,m-1$和$m+1,R$但是我们这样做的话，对于大量的重复数字出现的情况下，通常会浪费计算资源。但是这种情况我们原本是可以避免的，而实现的想法也十分简单。简单来说：我们每次在调整的过程中我们希望与标准相同的数集中起来，之后递归的时候再精简一下区间即可，这个改进的的确确是非常非常有用的。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748void BoundQuickSort(int a[],int L,int R)&#123; if(L&gt;=R) return ; if((L+1)==R)&#123; if(a[L]&gt;a[R]) swap(a[L],a[R]); return ; &#125; int m=L,l=L+1,r=R,BoundL,BoundR; while(l&lt;r)&#123; while(a[l]&lt;=a[m]&amp;&amp;(l&lt;R)) l++; if(a[l]&lt;=a[m]) l++; while(a[r]&gt;=a[m]&amp;&amp;(r&gt;L)) r--; if(l&gt;r)&#123; swap(a[m],a[l-1]); m=l-1; break; &#125; else &#123; swap(a[l],a[r]); &#125; &#125; BoundL=m-1; BoundR=m+1; l=L,r=R; while(true)&#123; while(a[l]!=a[m]&amp;&amp;(l&lt;BoundL)) l++; if(a[l]!=a[m]) l++; while(a[BoundL]==a[m]&amp;&amp;(BoundL&gt;L)) BoundL--; if(l&gt;=BoundL)&#123; break; &#125; else &#123; swap(a[l],a[BoundL]); &#125; &#125; while(true)&#123; while(a[BoundR]==a[m]&amp;&amp;(BoundR&lt;R)) BoundR++; if(a[BoundR]==a[m]) BoundR++; while(a[r]!=a[m]&amp;&amp;(BoundR&lt;r)) r--; if(r&lt;=BoundR)&#123; break; &#125; else &#123; swap(a[r],a[BoundR]); &#125; &#125; BoundQuickSort(a,L,BoundL); BoundQuickSort(a,BoundR,R);&#125; 总结虽然快速排序已经是多年前的算法，但是其思想仍然值得我们仔细思考，细细品味。 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>QuickSort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Binomial堆]]></title>
    <url>%2F2019%2F01%2F03%2FBinomial%2F</url>
    <content type="text"><![CDATA[Binomial 堆是什么？首先我们知道Binomial这个单词的意思是二项式，但是为什么取个名字呢？也许读完本文你就明白了。 二叉堆首先我们回顾一下二叉堆，对于节点数为$n$的它，具有下面几个基本性质 树高$\lceil \log n \rceil$ 插入节点时间复杂度$O(\log n)$ 访问根节点(优先级最高的节点)时间复杂度$O(1)$ 删除根节点时间复杂度$O(\log n)$ 合并两棵树时间复杂度： 自底向上合并时间复杂度为$O(n)$ 自顶向下合并时间复杂度为$O(n\log n)$ 虽然说二叉堆具有查找上极其优良的性质，但是在合并的时候（虽然一般情况下用不到合并这个操作），时间复杂度却没有达到$O(\log n)$级别。这就使得有些人在想，我们可不可以放松一下访问根节点的时间到$O(\log n)$，但是加速合并两棵树呢？于是Binomial 堆就被提出来了，同时比预想的更加强大！ 思想由于我们要加速合并，那么肯定是有好几棵树同时存在才需要合并，所以Binomial 堆的本质思想就与二叉堆有很大不同。Binomial 堆是由森林构成的，而二叉堆是一棵树。所以这也导致了合并这个需求在Binomial 堆中是如此的重要。 但是Binomial 又是什么意思呢？直观上我们只知道Binomial 表示$2^0,2^1,2^2,…,2^n$这样的数目。但是这和森林有什么关系呢？实际上这恰恰是Binomial 堆的精髓所在，它表示的是森林中每棵树的节点数为$2^k$这么多，同时我们也称当前这棵树为$B_k$。本文中介绍小根Binomial 堆，如果不懂什么是小根，那么请移步我的另一篇文章。 Binomial 堆简单的例子，表示的是0阶，1阶，2阶，3阶，4阶，5阶树的结构： 一般的结构形式： 注意: 通过上面的描述，我们可以对每棵数得到几个很明显的性质： $|B_k|=2^k$ $height(B_k)=k$ $degree(B_k)=k$ 对于一棵树的每一个节点来说，其第$i$个儿子的度为$i-1$ 对于总节点为$n$的Binomial 堆而言，其具有以下几个性质： 最多有$\lfloor \log n \rfloor +1$棵树，很明显，其实就是n在二进制下有多少个1就有多少棵数 树高最高为$\log n$，因为二进制下最高位一定为$\log n$位 也许这里会有疑惑，但是请务必仔细想想为什么会有这种性质。 合并条件-Union我们规定Binomial 堆中不允许出现结构相同的两棵数，换句话说，假如森林中出现了阶数相同的两棵树，我们需要进行合并。我们合并的方式也很简单，将根节点的值更大的树作为儿子连上根节点的值更小的树。 一个简单的例子： 请注意这是一个递归过程。我们每次合并之后都需要再检查，其实这里本质上就是二进制的进位操作。由于最坏的情况下我们需要进位$\log n$次，所以这一步的复杂度最坏为$O(\log n)$。但是这一步均摊下来其实是很快的，因为其实二进制的进位操作并不会每次都是最坏情况，而且也不可能出每次都是最坏情况，这一步经过证明我不会，平均的时间复杂度为$O(1)$。 插入节点-Insert我们在插入一个节点的时候，想法也是非常简单的，我们首先将其视为$B_0$（也就是单个节点的树），并将其加入到森林中，之后我们使用上一步讲到的合并条件，该步复杂度与合并条件相同,时间复杂度为$O(1)$。 提取最小值-Top提取最小值，我们采用最简单的方法，对森林中每棵树的根节点进行一次遍历，找到根最小的节点的值返回即可。由于最多有$\lfloor \log n \rfloor +1$棵树，故时间复杂度为$O(\log n)​$。 删除最小值-Pop首先我们通过上一步的方法，定位到最小值点在森林的哪一个树中，之后我们将根节点移除，那么此时它应该分裂成了一些更小的树，由于$B_k$这样的一棵树可以分裂为$B_0,B_1,B_2,…,B_{k-1}$这样的一些子树，由于$k$最大为$\log n$,故我们最多插入$\log n$次，每次插入的时间为$O(1)$，故该步的时间复杂度为$O(\log n+\log n)=O(\log n)$。 对比 二叉树 Binomial Union O(n) O(1) Insert O(log n) O(1) Top O(1) O(log n) Pop O(log n) O(log n) 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉堆]]></title>
    <url>%2F2019%2F01%2F02%2FHeap%2F</url>
    <content type="text"><![CDATA[什么是二叉堆？二叉堆是什么呢？我的理解是堆是一个数据结构，为了满足我们的需求所精心设计的数据结构。而二叉堆正是为了满足我们的需求而被提出来的，同时它也叫做优先队列，它所能实现的最重要的功能是： 在$n$个元素中通过$\log n$时间找到优先级最大/最小的元素 我们传统的方式是遍历数组去寻找，这样需要花费$n$级别的时间，那么对于大量查询优先级最大/最小的元素情况下，我们使用二叉堆进行优化可以直接把复杂度的级别下降。 大根堆/小根堆大根堆和小根堆是我们最常用的二叉堆的结构，相应的，它们是为了满足每次查询优先级最大/最小的元素而分别设计的。它们的本质思想都一样，故在本文中我们会详细介绍小根堆，同时本文中的元素定义为int类型的数值。 结构 小根堆是一颗完全二叉树，请注意这一点，因为这一步保证了我们每次查询的复杂度为$\log n$ 小根堆的父亲节点的数值都小于其儿子节点 例如： 这就是一个典型的小根堆，而其右边的就是其使用数组存储的方式，为了简单，我们通常使用数组版本的，当然指针版本的更加优越，但是本文都是基于数组实现的。 注意： 这里有个很简单的技巧，对于每个节点号为$id$的节点而言，其左儿子是$2id$，右儿子是$2id+1$ 同时对于节点号为$id$的，其父亲节点编号为$id/2$ 我们所需要完善的几个函数对于一个堆，我们只需要满足三个函数即可 节点的插入Insert(int x) 根节点的访问Top() 根节点的删除Pop() 但是其实有难度的只有第一个和第三个功能。 请关注为什么函数的名字为什么与队列函数类似。 Insert我们进行插入操作很简单，简单来说只有两步，假如我们需要插入$x$： 首先将$x$放置在数组的最后，这一步保证了插入之后仍然是一棵完全二叉树 向上调整$x$的位置，倘若比其父亲小的话，进行交换，直到无法再进行调整。 一个例子： 左边是首先插入到最后一个位置，之后向上调整到合适的位置。显然，我们插入最多向上交换$\log n$次，故复杂度为$O(\log n)$ Top根节点的访问就很简单了，只要访问数组的第一个位置就行了。故复杂度为$O(1)$ Pop我们进行根节点的删除操作也很简单，简单来说，只需要三步： 将根节点的值和最后一个节点的值交换 删除最后一个节点 调整根节点的位置，每次和最小的儿子进行交换，直到无法进行交换。 请特别注意第三个步骤，并好好想想为什么一定要这样做。 一个例子： 首先和最后一个节点进行交换，之后删除，最后调整根节点的位置。由于交换后的节点最多也是向下交换$\log n$次，故复杂度为$O(\log n)$ 小根堆的代码实现，数组版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include&lt;bits/stdc++.h&gt;using namespace std;int heap[1000005],size=0;void Insert(int x)&#123; int fatherid,now; size++; heap[size]=x; now=size; while(now&gt;1)&#123; fatherid=now/2; if(heap[fatherid]&gt;heap[now])&#123; swap(heap[fatherid],heap[now]); now=fatherid; &#125; else break; &#125;&#125;int Top()&#123; if(size) return heap[1]; else printf("NO element, can't get top\n"); return -1;&#125;void Pop()&#123; int leftsonid,rightsonid,now; if(size)&#123; swap(heap[1],heap[size]); size--; now=1; while(now&lt;=size)&#123; leftsonid=now*2; rightsonid=now*2+1; if(leftsonid&lt;=size&amp;&amp;rightsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[rightsonid])&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; if(heap[rightsonid]&lt;heap[now])&#123; swap(heap[rightsonid],heap[now]); now=rightsonid; &#125; else break; &#125; &#125; else if(leftsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; break; &#125; &#125; &#125; else printf("NO element, can't pop\n");&#125;int main(void)&#123; int i,j,n,m; n=100; for(i=0;i&lt;n;i++)&#123; Insert(rand()); &#125; while(size)&#123; printf("%d\n",Top()); Pop(); &#125;&#125; 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CF1091E-New Year and the Acquaintance Estimation]]></title>
    <url>%2F2018%2F12%2F31%2FCF1091E%2F</url>
    <content type="text"><![CDATA[题目大意给我们$n$个点的度数$a_1,a_2,…,a_n$,同时还有一个点的度$x$尚未确定。我们希望$a_1,a_2,…,a_n,x$构成的点度序列能够形成一个简单图，所谓简单图就是无自环，重边的无向图，连通性无需保证。要求从小到大输出$x$的所有可能解。若没有任何解输出$-1$。 基本知识 Havel–Hakimi algorithm 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$a_2-1,a_3-1,…,a_{a_1+1}-1,a+{a_1+2},..,a_n$$构成一个简单图。即我们可以根据这个进行递归。 人话解释：每次拿一个点度最大的$a_1$出来，将剩余的前$a_1$个点度减一即可。注意，这里每次都是要排序的。故递归求解的话时间复杂度是$n^2logn$的 Erdős–Gallai theorem 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$\sum_{i-1}^ka_i&lt;=(k(k-1)+\sum_{j=k+1}^n\min(a_j,k))$$同时点度和和需要为偶数：$$\sum_i^na_i=even$$这个定理就比较晦涩了，而且很强大，直接不需要递归了，我们只需要对每个$k$维护一下值就行了。这个判断一次的复杂度就直接变成了$n$级别的复杂度。故本题只用这个结论 题目想法很显然，我们只需要找一个上界和一个下界，中间的点度是一个公差为2的等差数列。现在问题变成了怎么找到上界和下界。 下界我们通过二分的方式找下界，我们在使用Erdős–Gallai theorem定理的时候需要对每个$a_i$进行检查。很显然，我们二分$x$为$M$的时候，我们可以得到如下几种情况： 所有$a_i$都满足条件，那么我们很确定的说下界在左边，此时我们令$R=M$ 存在$a_i$不满足条件，那么我们再分为两种情况 $a_i&gt;x$ 同时不满足条件，这个时候我们可以观察Erdős–Gallai theorem定理，很轻松可以想到我们此时应该增加$x$才能使$a_i$满足条件，故此时应该令$L=M+1$ 相应的，$a_i&lt;x$同时不满足条件的情况下，应该令$R=M$ 通过上面几种情况的考虑，我们可以轻松求出下界。 上界由于我们此时知道下界，那么我们求上界就是一个基本的二分了，找第一个不满足的$M$即可。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;bits/stdc++.h&gt;using namespace std;int a[500005],b[500005],N;long long pre[500005];int cmp(int x,int y) &#123; return x&gt;y;&#125;int check(int x) &#123; long long i,res,cur,n,pos; n=N; if(x&gt;n) return 1; for(i=0;i&lt;n;i++)&#123; b[i]=a[i]; &#125; b[n]=x; n++; sort(b,b+n,cmp); res=0; for(i=0;i&lt;n;i++)&#123; pre[i]=b[i]; if(i) pre[i]+=pre[i-1]; &#125; pos=n-1; for(i=0;i&lt;n;i++)&#123; res+=b[i]; cur=(i+1)*i; while(pos&gt;=0&amp;&amp;b[pos]&lt;=(i+1))&#123; pos--; &#125; if(pos&gt;i)&#123; cur+=pre[n-1]-pre[pos]; cur+=(i+1)*(pos-i); &#125; else &#123; cur+=pre[n-1]-pre[i]; &#125; if(res&lt;=cur) continue ; if(b[i]&gt;x) return 0; return 1; &#125; return 2;&#125;int main(void) &#123; int i,x,L,R,M,T,ans,n,all; T=0; scanf("%d",&amp;N); n=N; for(i=0; i&lt;n; i++) &#123; scanf("%d",a+i); T+=(a[i]&amp;1); &#125; T%=2; L=0; R=5e5; ans=-1; while(L&lt;R) &#123; M=(L+R)/2; all=check(T+M*2); if(all==1) &#123; R=M; &#125; else if(all==2)&#123; ans=M; R=M; &#125; else &#123; L=M+1; &#125; &#125; if(ans==-1) &#123; printf("-1\n"); return 0; &#125; L=ans; R=5e5; while(L&lt;R) &#123; M=(L+R)/2; if(check(T+M*2)==2) &#123; L=M+1; &#125; else &#123; R=M; &#125; &#125; for(i=ans; i&lt;L; i++) &#123; printf("%d ",T+i*2); &#125; printf("\n");&#125;]]></content>
      <categories>
        <category>codeforces</category>
      </categories>
      <tags>
        <tag>graph-theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型详述]]></title>
    <url>%2F2018%2F12%2F26%2FHMM%2F</url>
    <content type="text"><![CDATA[什么是隐马尔可夫模型？隐马尔可夫模型实际上是一个双重的随机过程，我们不知道具体的状态序列，只知道转移的概率，即模型的状态转移过程是未知的，而观察事件的随机过程是状态转换过程的随机函数，即我们希望通过可见的事件变换去预测深藏在其背后的本质规律。 请记住上述提到的几个概念： 状态序列(这是我们需要去预测的) 观察序列(这是我们已知的) 状态序列的转移概率(这是我们已知的) 状态序列对观察序列的转移(这是我们已知的) 隐马尔可夫链成立的三个假设（当然可以不需要理解） 状态构成一个隐马尔可夫链$$P(q_i|q_{i-1}q_{i-2}…q_1)=P(q_i|q_{i-1})$$ 不动性假设（状态与具体时间无关）$$P(q_{i+1}|q_i)=P(q_{j+1}|q_j)$$ 输出独立性假设$$P(o_1..o_i|q_1…q_i)=\prod_tP(o_t|q_t)$$ 其实这里的隐马尔可夫链中的链就表示假设了是一阶的！我们假设满足上面的性质。 现在我们考虑一个场景假如我们面对一堆过去的数据，过去的数据当中只有自然生长下的海藻的每天的状态。 我们也有一些最近的数据，最近的数据中有海藻的状态和每天的天气情况。 现在我们给定天气的转移矩阵，天气对海藻状态影响的矩阵。我们希望根据这些数据，去预测过去的天气。 海藻的状态只有4种： 干 稍干 潮湿 湿润 天气的状态只有3种： 晴 阴 雨 天气状态转移矩阵（后面简称为A）： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，明天的天气是雨，阴，晴的概率分布。 天气对海藻干湿的影响(后面简称为B): 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，今天的海藻出现干，稍干，潮湿，湿润的概率分布。 建立模型我们的任务是根据观察序列去推测状态序列。 首先我们根据之前的信息：构建状态集合$s$，状态转移矩阵$A$，初始状态概率分布$\pi$，观察集合$ss$，状态对观测的影响矩阵$B$ 对于我们这个题目而言，状态集合$s$为(晴，阴，雨)，观察集合$ss$为(干，稍干，潮湿，湿润)，请务必注意这个地方！ 那么我们可以得到五元组模型:$(s,ss,\pi,A,B)$，通常我们将模型简写为三元组:$\lambda =(\pi,A,B)$。 对于上述三元组，只有$\pi$是未知的，这个东西怎么统计出来呢？其实很简单，由于我们包含最近的数据，我们根据最近的天气可以统计出来每种天气出现的概率，那么这个分布就是$\pi$，公式表示为：$$\pi_i=P(q_1=s_i)$$当然这仅仅是对于这个任务是这样计算的，别的任务可能会不一样。上述公式表示的就是说第$i$种情况出现的概率 比如说统计后大概长成这个样子： 晴 雨 阴 P 0.5 0.05 0.45 求解过程我们现在已知观察序列$O=o_1….o_n$，模型$\lambda =(\pi,A,B)$ 。现在我们需要求给定模型与观察序列的情况下求状态序列$Q$。有我们需要以下概率最大：$$P(O|Q,\lambda )$$意思也很简单啊，就是我们求这个条件概率，当前模型$\lambda ​$来说，希望找到一个状态序列$Q​$使得观察序列O发生的可能性最大。 我们假设$Q=q_1…q_n$ 我们把上面的式子打开，有：$$P(O|Q,\lambda )=\pi_{q_1}A_{q_1q_2}A_{q_2q_3}…A_{q_{n-1}q_n}B_{q_1o_1}B_{q_2o_2}…B_{q_no_n}$$对上面的式子我们很简单的可以想到枚举每一种$Q$，对每一种都进行计算，之后输出使得$P(O|Q,\lambda )$最大的那个$Q$就可以了。但是这当然是不可以的，因为这个是指级别的，$n$大的话这个方法就凉了。 解释一下上面的公式： $\pi_{q_1}$表示最初天气为$q_1$成立的概率 $A_{ij}​$表示天气从$i​$到$j​$转移的概率 $B_{ij}$表示天气为$i$的情况下海藻状态为$j$的概率，也叫发射概率 viterbi算法维特比算法分为前向后向，但是我感觉会一种就行了，其实都是一样的。这里介绍一下前向算法。 前向算法思想：假设我们已知了对于$o_1…o_{n-1}o_n$最优的状态序列肯定是$o_1…o_{n-1}$的一个序列转移过来的，等于说我们对当前的一层计算来说，对于每个节点只需要考虑前面一层的结果就可以了。即有$$dp_{iq_i}=\max_{k\in s} dp_{(i-1)k}A_{k(q_j)}B_{q_jo_i}$$$dp$表示计算的结果，$q_i$表示当前层的状态，$k$表示枚举前一层的状态，从观察状态集合中枚举。$max$表示存储后面式子的最大值。 算法流程： C++的代码，写起来很简单，每种语言都可以按照这个方式写。但是为了保证精度，我们一般是取$log$的，这份代码的结果应该会很差！ 123456789101112131415161718192021void viterbi(int O[],double PI[],double A[][],double B[][])&#123;//传观察序列和模型 int i,j,back; int path[][]； double dp[][],MX； for(i=1;i&lt;=n;i++)&#123;//初始化 dp[1][i]=PI[i]; &#125; for(i=1;i&lt;=m;i++)&#123; for(j=1;j&lt;=n;j++)&#123; MX=-1; for(k=1;k&lt;=n;k++)&#123;//枚举前一层 if(MX&lt;dp[i-1][k]*A[k][j])&#123; MX=dp[i-1][k]*A[k][j];//找到最大的 back=k; &#125; &#125; path[i][j]=back;//用于回溯 dp[i][j]=MX*B[j][O[i]];//计算结果存起来 &#125; &#125;&#125; 一个简单例子为了结果看起来比较正常，我们把初始分布$\pi$ 设的极端一点，同时我们再次召唤之前的表。 晴 阴 雨 P 1 0 0 天气状态转移矩阵$A$： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 天气对海藻干湿的影响$B$: 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 对于我们给定的观测序列：干，潮湿，湿润。我们计算的结果入上图所示。其中红色括号的结果就是表示结果是上一次的第几个节点过来的。 到此为止，隐马尔可夫模型的这个例子就介绍完了，当然问题有其他变种，一般是在viterbi算法上动刀，不会解决的话可以留言评论。 A和B矩阵怎么求？我们大部分时候，其实只有训练数据和测试数据，没有这些转移矩阵，这个时候我们可以通过最大似然估计来求这些矩阵(看起来吓人)。就好比这一题我没有给出矩阵$A,B$的话怎么办？ 其实我们也可以做出来这个题目，因为我在题目中特意强调了我们还有一些最近的标注数据，这时候我们可以通过最近的数据把矩阵$A,B$给估计出来，做法如下。 其实就是统计一下就行了，对于A矩阵：$$A_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的后面连接了天气状态为$j$的次数除以天气状态为$i$的出现总次数。 对于B矩阵：$$B_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的对应海藻的状态为$j$的次数除以天气状态为$i$的出现的总次数。 我们在求的时候一般加上松弛操作，即$$A_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ $$B_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ 一个小项目为了使本文有一点实用价值，我把我的代码放在这里。一个序列标注的小东西。 参考资料国科大自然语言处理课件 胡玥]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transformer模型详解]]></title>
    <url>%2F2018%2F12%2F24%2FAttention%2F</url>
    <content type="text"><![CDATA[背景目前深度学习中用于做NLP的方法，大多都是首先将句子进行分句，之后将每个单词使用与训练好的词向量进行表示(其实这就是一种迁移学习？)，通过这一步我们把一个句子转化为向量的序列。这样的好处是我们可以把一个句子使用一个向量模型来表示，即每个句子我们都对应一个矩阵$x=(x_1,x_2,…x_n)$其中$x_i$表示第$i$个词的词向量，通常我们记为行向量，假如预训练好的向量维度为$d$,也就是说一个句子我们可以映射为$x\in R^{n*d}$。通过这个操作，我们可以将语言化作向量表示，这是一个很好的建模方式。 我们为了处理这些信息主要分为以下几个思路： RNN 可以很好的获得时序信息 CNN 可以很好的获得全局信息 Attention google提出的全新思想，在各大模型上都有提升 RNN第一个思路是RNN，思想是很简单的，进行递归式的信息提取：$$y_t=f(y_{t-1},x_t)$$由于我们在每一步进行计算的时候都需要依赖上一步计算的结果，这导致了RNN无法并行计算，这是这个模型原本就存在的缺陷，但是RNN可以很好的学习到时序信息。相应的，我们很难获取到全局信息，倘若我们想得到全局信息，我们一般使用双向RNN或者双向LSTM。 CNN第二个思路是CNN，思想也是十分简单的，进行窗口式的扫描，对于尺度为3的卷积：$$y_t=f(x_{t-1},x_t,x_{t+1})$$由于CNN的每一步进行计算的时候是独立的，故我们可以很方便的进行并行计算。其实通过这一步我们只能获取到局部的信息，但是我们通过堆叠的方式增大感受野，使得模型能够获得很好的全局信息！但是相应的，CNN的时序信息获取效果不如RNN。 AttentionGoogle在2017年的论文Attention is All You Need提供了第三个思路，这个想法也也是十分简单的，一般而言我们记作：$$y_t=f(q_t,K,V)$$对于全局过程我们记作：$$y=f(Q,K,V)$$其中$Q,K$与$V$相同的时候，我们称作self-attention。计算方式在文章后续会详叙。 Transformer注意力机制 上图看起来十分复杂，我们简化一下。 这其实就是一个Seq2Seq模型，我们左边把一个encoder输入进去，右边就decoder得到输出。由于上图有两个部分，我们不妨将其中的的部分再打开看看： 我们随之而来的问题就是这个encoder是如何把信息传递给右边的呢？因为这个图和上面的压根就不一样好吗？我们再将图片打开拓展: 意思就是说对于每一层的encoder的输出，我们会和decoder的输入进行结合。 我们再取一种的一层做详细展示： 我们可以看到在解码层的做了两次attention，我们在第二次的attention时使用了左边的encoder传递过来的信息。上面这张图我觉得有点小问题，也许是我理解错误。。我认为左边的encoder端上面应该是没有箭头的。 传统Attention 机制定义Attention的翻译过来就是注意力，这表示了人类的偏好，我们在观察一个图片的时候，往往对图像的某一部分有更集中的偏好，就是所谓的”抢镜”。对某一部分有更高的关注度，这使得我们对图片有一个更加准确的感受。 我们拓展到文本中，例如我们在进行翻译任务的时候，翻译当前词的时候一般是对序列的局部信息有偏好，但是对于每个单词而言，对原序列的关注程度肯定是不一样的。attention就是使用最简单的方法实现这个功能。 计算方式我们再次召唤前面的公式:$$y_t=f(q_t,K,V)$$我们简述一下这个$f$是如何进行计算的。 我们输入一组$K$与$V$与查询$Q$，首先我们对每个$K_i$与$Q$计算相似度得到$S_i$,之后将$S$通过softmax函数进行归一化得到分布$a$，之后我们计算$a$与$V$的加权和得到对于查询$Q$的attention向量Att-V。 其中对于$f(Q,K_i)$(都为列向量的话)的计算方式主要分为以下几种： 点乘 dot product : $f(Q,K)=Q^TK$ 权重 general : $f(Q,K)=Q^TWK$ 拼接 concat ：$f(Q,K)=W[Q^T;K]$ 神经网络 perceptron ：$f(Q,K)=V^T\tanh (WQ+UK)$ 对于$Q$,$K$与$V$相同的情况下，我们也称作为自注意力机制，希望寻找文本中内在的联系。也就是说，在序列内部做Attention，寻找序列内部的联系。Google论文的主要贡献之一是它表明了内部注意力在机器翻译（甚至是一般的Seq2Seq任务）的序列编码上是相当重要的。 传统模型有一个非常明显的缺点，就是无法获得时序信息，就算我们把顺序打乱，我们算出来的结果一样是不变的，这个就有点不太合理了！当然我们一般使用attention去辅助RNN和CNN，由于RNN和CNN已经包括了时序信息，可能会好一些。 transformer中的Attention机制在transformer当中的Attention机制与传统的attention机制还是有很大区别的。分别叫做 Scaled Dot-Product Attention 和 Multi-Head Attention。 Scaled Dot-Product Attention其结构图如下图所示： 首先第一个问题,$Q,K,V$从哪里来？按照我的理解，对于我们输入的句子$x=(x_1,x_2,…x_n)$，这一步可以是原本的词向量，也可以是对于输入的词向量做线性变化，例如$Q=xW^q,K=xW^k,V=xW^v$，例如： 我们对于input的词向量Thinking Machines，通过三个矩阵$W^q,W^k,W^v$，得到$Q,K,V$,于是我们依次遍历$Q_i$，计算每个的$Q_i$的注意力向量。 首先对于$q_1$,我们采用dot product进行计算相似度$S_i$ 之后我们将$S_i$ 除以$\sqrt{d_k}$其中$d_k$表示$K$的维度，加上这个是为了防止内积太大，如果内积过大，会导致softmax进入饱和区，就没有注意力这个作用了。接下来我们通过softmax计算概率分布。 最后我们计算加权和得到对于$q_1$的注意力向量$z_1$。直以递归下去，我们可以对每个$q$进行相同的操作即可。听上去这一步好像很麻烦，其实我们用矩阵表示的话，就很简单且简约了： 首先输入词向量$X$(行向量)，计算$Q,K,V$ 这一步就更加直观了，但是我们需要好好理解一下$QK^T$ 这一个计算过程，这一步实际上是计算了一个word2word的attention矩阵。例如我们对”I have a dream”计算的话 其中每个格子$grid_{ij}$ 表示的是第$i$个单词和第$j$个单词的相似度，这肯定是一个对称矩阵啦！现在我们回到transformer的结构图当中，可以很明显的看到有的self-attention前面加上了masked，这又是什么意思呢？ 简单来说就是为了防止程序看见未来的信息，而用灰色区域(0.0)覆盖上。 Multi-Head Attention多头的意思就很简单了，就是将工作重复做几次而已。论文中倒是画的很吓人的样子。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。 我们重复几次，将得到的$Z$矩阵进行拼接即可 首先我们对输入的$X$进行计算多次 将计算完的结果拼接$Z_0+Z_1+….+Z_n$，之后使用一个线性变换为指定维度的$Z$ 整个的框架如下： Position Embedding然而，我们经过思考之后，可以发现这个计算方式和传统的一样啊，我就算位置变了，对attention的计算结果不变。这个问题就很严重了，很有可能我们在机器翻译的任务当中，我们确实算出来了翻译结果应该包含了哪些单词，但是结果很有是乱序的，那么就没法用了。transformer为了解决这个问题，祭出了Position Embedding这个东西。Google是直接给出了一个公式来构造Position Embedding。$$PE_{2i}(p)=\sin(p/10000^{2i/d_{pos}})\PE_{2i+1}(p)=\cos(p/10000^{2i/d_{pos}})$$这里的意思是将position为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。Google在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的。那么为了减少计算复杂度，我们何必自己再训练呢? 原文中提到采用这个公式的另一个原因的是sin和cos，满足一些良好的性质:$$\sin(a+b)=\sin(a)\cos(b)-\cos(a)\sin(b)$$$$\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$$ 这使得位置为p+k的向量可能可以使用位置为p的向量线性表出。$$PE_{2i}(p+k)=\sin((p+k)/10000^{2i/d_{pos}})=$$$$\sin(p/10000^{2i/d_{pos}})\cos(k/10000^{2i/d_{pos}})-\cos(p/10000^{2i/d_{pos}})\sin(k/10000^{2i/d_{pos}})$$ 由于$k$是定值，而$\sin(p/10000^{2i/d_{pos}})$和$\cos(p/10000^{2i/d_{pos}})$确实是知道的，所以确实为线性表出提供了可能性吧。。 我们在input输入的时候加入位置信息，同时我们在decoder的输入过程中也加入了位置信息。 Position-wise Feed-forward Networks在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$值得注意的是encoder的每一层的$W_1$和$W_2$都不相同。 The Residuals由于transformer的结构十分复杂，训练的时候很容易出现梯度消失，导致网络难以训练，文章采用的思想借鉴了何恺明的残差网络的思想，在每一次操作之后为了保证损失能够尽量回传，每层的输入会和输出进行叠加。 其中的Add&amp;Normalize就是叠加过程。公式记作：$$output=LayerNorm(x+Sublayer(x))$$但是这也带来一个小问题，就是每次sublayer输出的结果的为维度和原输入的维度应该一样，带来了一些限制。 Encoder在文章中encoder有6层，而其中的每一层包含两个sub-layer 第一个sub-layer是多头自注意力机制，用来计算文本内部的关联 第二个是全连接Position-wise Feed-forward Networks Decoder文章中decoder也是6层，其中的每一层包含三个sub-layer 第一个sub-layer是masked多头自注意力机制，原因很简单啊后面的结果还没生成呢！ 第二个sub-layer是多头注意力机制，将decoder和encoder结合，注意计算的时候$K,V$是由encoder提供的，decoder提供的是$Q$ 第三个是一个全链接网络Position-wise Feed-forward Networks The Final Linear and Softmax Layer 最后一层就比较简单了，简单来说就是首先把decoder的第6层输出经过一个神经网络对每个单词的可能性计算一个权值，再经过一个softmax是为了反向传播的时候有一个误差回传，使得整个网络能够训练。 这里还需要注意的是，每次输出的结果，都要像RNN一样作为输入传给decoder，依次输出结果。 transformer动画演示过程首先encoder的过程 接着就是decoder的过程 参考文献jalammar’s blog 苏剑林《《Attention is All You Need》浅读（简介+代码） 》]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simplex-单纯形算法介绍]]></title>
    <url>%2F2018%2F12%2F18%2FSimplex%2F</url>
    <content type="text"><![CDATA[单纯形算法（Simplex）有什么用？谈到单纯形算法，我们不得不提线性规划，所谓线性规划，就是在满足一定约束下优化目标函数。下面引用几个例子来进行简单介绍。 Food Energy Ca Price 燕麦 110 4 3 牛奶 160 8 9 猪肉 260 14 19 现在我们需要55个单位的Ca和2000个单位的Energy，问我们应该怎么购买最合适。这就是一类最经典的线性优化问题，我们可以很轻松的写出目标函数和约束条件： ​ $min$ $$3X_1+9X_2+19X_3$$ ​ $s.t.$ ​ $110X_1+160X_2+260X_3&gt;=2000$ ​ $4X_1+8X_2+14X_3&gt;=55$ ​ $X_i&gt;=0，i=1,2,3$ 上述不等式表示的含义很简单，$min$表示最小化后面的目标函数，$s.t.$ 表示subject to的缩写，意思是受限于。 针对这类问题，我们曾经学过的方法是图解法，画出可行域（阴影部分）（a），之后使用目标函数(b)在可行域上移动，之后靠直觉确定最优解。 但是计算机可没有人类的直觉！计算机求解这些问题的时候，需要更加通用的方法来求解。于是在二战时期，为了协助政府协调物资人员，前苏联的坎托诺维奇强行提出了Simplex算法。其实该算法的思想也是来自于图解法，我们观察上面的求解过程，可以很明显的发现，最优解一定是在顶点上的！！！！于是这个人就在想，我们是不是随便从一个顶点开始，之后每次迁移到这个顶点的相邻顶点上，对这个凸包每个点遍历一次就能得到答案了？事实确实也确实是这样的，而且可以更简单，实际上我们只需要随便从一个顶点开始，之后暴力这个点相邻的点，然后选择一个下降的方向迁移点就可以，直到当前的点的值最小，那么这个点就是最终的解。 但是为什么可以这样做呢？按照我的理解，因为可行域是一个凸多边形(为什么是凸的？其实画一画，就知道了。不可能是凹的，可行域一定是凸的！！)，那么这个凸多边形的顶点结果是单调的，因为全局最优解只有一个。那么求解过程就好像下山一样，只要每次向下走就可以走到谷底，类似于梯度下降法在凸二次函数上每次沿着下降的方向走，一直迭代就能得到最优解。当然这些是在扯淡，实际上这玩意是被证明了的，而我没看。。。 单纯形算法适用的情况标准的线性规划格式（也叫标准型）： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A*x&lt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$ 其中$c$表示的就是每一类$x$的花费，$A$表示的是约束矩阵，$x$表示的是擦书的列向量，$x_i$表示的是每一类的量(针对于整数线性规划，要求$x_i$必须为整数，比如$x_i$表示的是人的数量或者物品的份数) 但是有些时候我们可能碰到一些要最大化的最优化问题，比如我们固定钱数，怎么样买到的能量最多？这时候我们的优化目标变成了$max$，针对这类问题我们转换为标准形式的方式也很简单，$c*-1$ 就变为$min$ 了，同样的道理我们对$A$和$x$都可以这样干。 目标函数是$max$的时候，我们将$c$取反 $Ai*x&gt;=b$,我们将$A_i$和$b$取反 $x&lt;=0$,我们将$x$取反 松弛型我们上面介绍了标准型，现在介绍一下松弛型，其实也是一个很简单的东西，由于我们上面都是不等式，实在是太烦了，于是我们想要把不等式优化成等式，于是我们将标准型构造成下面的形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A_i*x+x_{i+n}=b_i$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中$m$表示约束的个数，由于之前的不等式都是$&gt;=$的情况，我们可以减去一个非负的变量使得等号成立。这就是松弛形。 单纯形现在介绍单纯形，单纯形其实就是从松弛型过来的，它是为了单纯形算法求解简便一些而存在的，因为它可以导出单纯形表，一般写成如下形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $x_{i+n}=b_i-A_ix$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中我们称左边的变量为基本变量，右边的变量称为非基本变量，我们很显然有一组基础解，就是令非基本变量为零，这时候基本变量的值都为其对应的$b_i$。我们一般情况下可以认为基础解就是前面说的可行解区域的一个顶点（因为原本就是边界）。 而单纯形表的初始形式很简单，举个例子 ​ $min$ $-x_1-14x_2-6x_3$ ​ $s.t.$ ​ $x_1+x_2+x_3+x_4$ $=4$ ​ $x_1$ $+x_5$ $=2$ ​ $x_3$ $+x_6$ $=3$ 当前的例子的基本变量为${x_4,x_5,x_6}$,非基本变量为${x_1,x_2,x_3}$。 其单纯形表的形式为: 0 -1 -14 -6 0 0 0 4 1 1 1 1 0 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 人话解释，第一行表示的是目标函数的系数，但是其中第0个位置表示的是当前目标函数取当前基础解作为解之后求得的值的相反数。后面的每一行表示的都是约束条件$b_i=a_{i1}x_1+a_{i2}x_2+…+a_{i(n+m)}x_{n+m}$ 求解步骤我们在使用单纯形法进行求解的时候，首先找到第一个目标函数中系数为负的非基本变量$y$，将其增大（这个过程相当于我们在凸包上沿着边缘走到另一个顶点），但是我们怎么确定这个非基本变量$y$最大能增大多少呢？这个其实很简单，只需要令除当前变量的其他变量为0，剩下$b_i$和$y$，我们可以对每个包含$y$的约束条件进行计算约束。找到使其最紧(即在当前的约束条件中让$y$的最大取值最小)的那一个约束即可，再使用当前约束条件下的基本变量与其进行替换即可。由于我们每次可以使约束z向更小的方向迁移，这使得我们的算法不会陷入死循环。 注意，假如对于非基本变量$y$不存在限制最紧的约束条件，那么该组线性规划无解，因为可以无限增大。 再次注意我们的规则(Bland规则)： 找到第一个目标函数中系数为负的非基本变量 找到限制最紧的约束条件 使用当前约束条件下的基本变量(也叫替入变量)与非基本变量（也叫替出变量）进行替换 我们对上述单纯形表进行一次示范操作： 首先我们看到第一个非基本变量$x_1$的系数为负数。那么我们开始对$x_1$进行增大。 对于第一个约束条件有$x_1=4$ 对于第二个约束条件有$x_1=2$ 对于第三个约束条件有$x_1=3$ 由于第二个约束是最紧的，那么我们选择第二个约束中的$x_5$作为替出变量，此时有单纯形表的变化为 2 0 -14 -6 0 1 0 2 0 1 1 1 -1 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 由于我们在替出时将$x_1=2-x_5$带入，很明显可以将单纯形表进行转换。如果没有想清楚的话我们可以手动带入一下即可。 一直重复这个操作，直到所有的非基本变量的系数都大于零（顶点无法再迁移），那么程序就求出了最优解。 注意，本文其实未完待续，因为其实在实现算法中间存在很多细节，但本文的目的是让读者对该算法有个系统的了解，明白算法的原由，至于一些具体的细节，读者可以参考链接进行进一步的了解！ 单纯形Simplex模板给出一份Simplex的模板与其应用，所求解的题目链接：BZOJ 1061： 注意！！该模板求解的是： ​ $max$ $c^Tx$ ​ $s.t.$ ​ $A*x&gt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$这类问题，稍稍转换一下就好了。之所以选择这个作为模板，是因为这份模板直接省去了替出变量，节省了大量空间，而且极其富有技巧性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;cstdio&gt;#include &lt;cmath&gt;typedef double DB;typedef long long LL;const int maxn = 1005, maxm = 10005;const DB inf = 0x3f3f3f3f3f3f3f3f, eps = 1e-7;int n, m;DB b[maxm], c[maxn], cof[maxm][maxn], ans;//约束、目标函数的系数、约束矩阵inline void pivot(int id, int pos) &#123; b[id] /= cof[id][pos];//首先归一化 cof[id][pos] = 1 / cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 for(int i = 1; i &lt;= n; i++) if(i != pos) cof[id][i] *= cof[id][pos];//归一化 for(int i = 1; i &lt;= m; i++) if(i != id &amp;&amp; fabs(cof[i][pos]) &gt; eps) &#123;//当前约束条件包含约束最紧的变量 b[i] -= cof[i][pos] * b[id];//搞死小圆(高斯消元)的方法更新变量的系数和b的值 for(int j = 1; j &lt;= n; j++) if(j != pos) cof[i][j] -= cof[i][pos] * cof[id][j]; cof[i][pos] = -cof[i][pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 &#125; ans += c[pos] * b[id];//对c进行消元 for(int i = 1; i &lt;= n; i++) if(i != pos) c[i] -= c[pos] * cof[id][i]; c[pos] = -c[pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方&#125;inline DB simplex() &#123; while(1) &#123; int pos, id; for(pos = 1; pos &lt;= n; pos++) if(c[pos] &gt; eps) break;//找到第一个系数大于0的变量，进行增大 if(pos == n + 1) return ans;//找不到了结束，返回答案 DB tmp = inf; for(int i = 1; i &lt;= m; i++) if(cof[i][pos] &gt; eps &amp;&amp; b[i] / cof[i][pos] &lt; tmp) tmp = b[i] / cof[i][pos], id = i;//找到约束最紧的那个约束条件 if(tmp == inf) return inf; pivot(id, pos);//进行顶点的迁移（旋转） &#125;&#125;int main() &#123; scanf("%d%d", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) scanf("%lf", &amp;c[i]);//其实将min改为max问题，可以直接采用结论，求对偶问题即可。 for(int i = 1; i &lt;= m; i++) &#123; int x, y; scanf("%d%d", &amp;x, &amp;y); for(int j = x; j &lt;= y; j++) cof[i][j] = 1; scanf("%lf", &amp;b[i]); &#125; printf("%lld\n", LL(simplex() + 0.5)); return 0;&#125; 参考文献hrwhisper]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Simplex</tag>
        <tag>最优化</tag>
        <tag>线性规划</tag>
      </tags>
  </entry>
</search>
