<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Binomial堆]]></title>
    <url>%2F2019%2F01%2F03%2FBinomial%2F</url>
    <content type="text"><![CDATA[Binomial 堆是什么？首先我们知道Binomial这个单词的意思是二项式，但是为什么取个名字呢？也许读完本文你就明白了。 二叉堆首先我们回顾一下二叉堆，对于节点数为$n$的它，具有下面几个基本性质 树高$\lceil \log n \rceil$ 插入节点时间复杂度$O(\log n)$ 访问根节点(优先级最高的节点)时间复杂度$O(1)$ 删除根节点时间复杂度$O(\log n)$ 合并两棵树时间复杂度： 自底向上合并时间复杂度为$O(n)$ 自顶向下合并时间复杂度为$O(n\log n)$ 虽然说二叉堆具有查找上极其优良的性质，但是在合并的时候（虽然一般情况下用不到合并这个操作），时间复杂度却没有达到$O(\log n)$级别。这就使得有些人在想，我们可不可以放松一下访问根节点的时间到$O(\log n)$，但是加速合并两棵树呢？于是Binomial 堆就被提出来了，同时比预想的更加强大！ 思想由于我们要加速合并，那么肯定是有好几棵树同时存在才需要合并，所以Binomial 堆的本质思想就与二叉堆有很大不同。Binomial 堆是由森林构成的，而二叉堆是一棵树。所以这也导致了合并这个需求在Binomial 堆中是如此的重要。 但是Binomial 又是什么意思呢？直观上我们只知道Binomial 表示$2^0,2^1,2^2,…,2^n$这样的数目。但是这和森林有什么关系呢？实际上这恰恰是Binomial 堆的精髓所在，它表示的是森林中每棵树的节点数为$2^k$这么多，同时我们也称当前这棵树为$B_k$。本文中介绍小根Binomial 堆，如果不懂什么是小根，那么请移步我的另一篇文章。 Binomial 堆简单的例子，表示的是0阶，1阶，2阶，3阶，4阶，5阶树的结构： 一般的结构形式： 注意: 通过上面的描述，我们可以对每棵数得到几个很明显的性质： $|B_k|=2^k$ $height(B_k)=k$ $degree(B_k)=k$ 对于一棵树的每一个节点来说，其第$i$个儿子的度为$i-1$ 对于总节点为$n$的Binomial 堆而言，其具有以下几个性质： 最多有$\lfloor \log n \rfloor +1$棵树，很明显，其实就是n在二进制下有多少个1就有多少棵数 树高最高为$\log n$，因为二进制下最高位一定为$\log n$位 也许这里会有疑惑，但是请务必仔细想想为什么会有这种性质。 合并条件-Union我们规定Binomial 堆中不允许出现结构相同的两棵数，换句话说，假如森林中出现了阶数相同的两棵树，我们需要进行合并。我们合并的方式也很简单，将根节点的值更大的树作为儿子连上根节点的值更小的树。 一个简单的例子： 请注意这是一个递归过程。我们每次合并之后都需要再检查，其实这里本质上就是二进制的进位操作。由于最坏的情况下我们需要进位$\log n$次，所以这一步的复杂度最坏为$O(\log n)$。但是这一步均摊下来其实是很快的，因为其实二进制的进位操作并不会每次都是最坏情况，而且也不可能出每次都是最坏情况，这一步经过证明我不会，平均的时间复杂度为$O(1)$。 插入节点-Insert我们在插入一个节点的时候，想法也是非常简单的，我们首先将其视为$B_0$（也就是单个节点的树），并将其加入到森林中，之后我们使用上一步讲到的合并条件，该步复杂度与合并条件相同,时间复杂度为$O(1)$。 提取最小值-Top提取最小值，我们采用最简单的方法，对森林中每棵树的根节点进行一次遍历，找到根最小的节点的值返回即可。由于最多有$\lfloor \log n \rfloor +1$棵树，故时间复杂度为$O(\log n)$。 删除最小值-Pop首先我们通过上一步的方法，定位到最小值点在森林的哪一个树中，之后我们将根节点移除，那么此时它应该分裂成了一些更小的树，由于$B_k$这样的一棵树可以分裂为$B_0,B_1,B_2,…,B_{k-1}$这样的一些子树，由于$k$最大为$\log n$,故我们最多插入$\log n$次，每次插入的时间为$O(1)$，故该步的时间复杂度为$O(\log n+\log n)=O(\log n)$。 对比 二叉树 Binomial Union O(n) O(1) Insert O(log n) O(1) Top O(1) O(log n) Pop O(log n) O(log n) 参考资料国科大-算法设计-卜东波 Binomial dui]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉堆]]></title>
    <url>%2F2019%2F01%2F02%2Fheap%2F</url>
    <content type="text"><![CDATA[什么是二叉堆？二叉堆是什么呢？我的理解是堆是一个数据结构，为了满足我们的需求所精心设计的数据结构。而二叉堆正是为了满足我们的需求而被提出来的，同时它也叫做优先队列，它所能实现的最重要的功能是： 在$n$个元素中通过$\log n$时间找到优先级最大/最小的元素 我们传统的方式是遍历数组去寻找，这样需要花费$n$级别的时间，那么对于大量查询优先级最大/最小的元素情况下，我们使用二叉堆进行优化可以直接把复杂度的级别下降。 大根堆/小根堆大根堆和小根堆是我们最常用的二叉堆的结构，相应的，它们是为了满足每次查询优先级最大/最小的元素而分别设计的。它们的本质思想都一样，故在本文中我们会详细介绍小根堆，同时本文中的元素定义为int类型的数值。 结构 小根堆是一颗完全二叉树，请注意这一点，因为这一步保证了我们每次查询的复杂度为$\log n$ 小根堆的父亲节点的数值都小于其儿子节点 例如： 这就是一个典型的小根堆，而其右边的就是其使用数组存储的方式，为了简单，我们通常使用数组版本的，当然指针版本的更加优越，但是本文都是基于数组实现的。 注意： 这里有个很简单的技巧，对于每个节点号为$id$的节点而言，其左儿子是$2id$，右儿子是$2id+1$ 同时对于节点号为$id$的，其父亲节点编号为$id/2$ 我们所需要完善的几个函数对于一个堆，我们只需要满足三个函数即可 节点的插入Insert(int x) 根节点的访问Top() 根节点的删除Pop() 但是其实有难度的只有第一个和第三个功能。 请关注为什么函数的名字为什么与队列函数类似。 Insert我们进行插入操作很简单，简单来说只有两步，假如我们需要插入$x$： 首先将$x$放置在数组的最后，这一步保证了插入之后仍然是一棵完全二叉树 向上调整$x$的位置，倘若比其父亲小的话，进行交换，直到无法再进行调整。 一个例子： 左边是首先插入到最后一个位置，之后向上调整到合适的位置。显然，我们插入最多向上交换$\log n$次，故复杂度为$O(\log n)$ Top根节点的访问就很简单了，只要访问数组的第一个位置就行了。故复杂度为$O(1)$ Pop我们进行根节点的删除操作也很简单，简单来说，只需要三步： 将根节点的值和最后一个节点的值交换 删除最后一个节点 调整根节点的位置，每次和最小的儿子进行交换，直到无法进行交换。 请特别注意第三个步骤，并好好想想为什么一定要这样做。 一个例子： 首先和最后一个节点进行交换，之后删除，最后调整根节点的位置。由于交换后的节点最多也是向下交换$\log n$次，故复杂度为$O(\log n)$ 小根堆的代码实现，数组版本： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include&lt;bits/stdc++.h&gt;using namespace std;int heap[1000005],size=0;void Insert(int x)&#123; int fatherid,now; size++; heap[size]=x; now=size; while(now&gt;1)&#123; fatherid=now/2; if(heap[fatherid]&gt;heap[now])&#123; swap(heap[fatherid],heap[now]); now=fatherid; &#125; else break; &#125;&#125;int Top()&#123; if(size) return heap[1]; else printf("NO element, can't get top\n"); return -1;&#125;void Pop()&#123; int leftsonid,rightsonid,now; if(size)&#123; swap(heap[1],heap[size]); size--; now=1; while(now&lt;=size)&#123; leftsonid=now*2; rightsonid=now*2+1; if(leftsonid&lt;=size&amp;&amp;rightsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[rightsonid])&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; if(heap[rightsonid]&lt;heap[now])&#123; swap(heap[rightsonid],heap[now]); now=rightsonid; &#125; else break; &#125; &#125; else if(leftsonid&lt;=size)&#123; if(heap[leftsonid]&lt;heap[now])&#123; swap(heap[leftsonid],heap[now]); now=leftsonid; &#125; else break; &#125; else &#123; break; &#125; &#125; &#125; else printf("NO element, can't pop\n");&#125;int main(void)&#123; int i,j,n,m; n=100; for(i=0;i&lt;n;i++)&#123; Insert(rand()); &#125; while(size)&#123; printf("%d\n",Top()); Pop(); &#125;&#125; 参考资料国科大-算法设计-卜东波]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CF1091E-New Year and the Acquaintance Estimation]]></title>
    <url>%2F2018%2F12%2F31%2FCF1091E%2F</url>
    <content type="text"><![CDATA[题目大意给我们$n$个点的度数$a_1,a_2,…,a_n$,同时还有一个点的度$x$尚未确定。我们希望$a_1,a_2,…,a_n,x$构成的点度序列能够形成一个简单图，所谓简单图就是无自环，重边的无向图，连通性无需保证。要求从小到大输出$x$的所有可能解。若没有任何解输出$-1$。 基本知识 Havel–Hakimi algorithm 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$a_2-1,a_3-1,…,a_{a_1+1}-1,a+{a_1+2},..,a_n$$构成一个简单图。即我们可以根据这个进行递归。 人话解释：每次拿一个点度最大的$a_1$出来，将剩余的前$a_1$个点度减一即可。注意，这里每次都是要排序的。故递归求解的话时间复杂度是$n^2logn$的 Erdős–Gallai theorem 给定一个点度序列$a_1,a_2,…,a_n$能够形成一个简单图的充要条件是:$$\sum_{i-1}^ka_i&lt;=(k(k-1)+\sum_{j=k+1}^n\min(a_j,k))$$同时点度和和需要为偶数：$$\sum_i^na_i=even$$这个定理就比较晦涩了，而且很强大，直接不需要递归了，我们只需要对每个$k$维护一下值就行了。这个判断一次的复杂度就直接变成了$n$级别的复杂度。故本题只用这个结论 题目想法很显然，我们只需要找一个上界和一个下界，中间的点度是一个公差为2的等差数列。现在问题变成了怎么找到上界和下界。 下界我们通过二分的方式找下界，我们在使用Erdős–Gallai theorem定理的时候需要对每个$a_i$进行检查。很显然，我们二分$x$为$M$的时候，我们可以得到如下几种情况： 所有$a_i$都满足条件，那么我们很确定的说下界在左边，此时我们令$R=M$ 存在$a_i$不满足条件，那么我们再分为两种情况 $a_i&gt;x$ 同时不满足条件，这个时候我们可以观察Erdős–Gallai theorem定理，很轻松可以想到我们此时应该增加$x$才能使$a_i$满足条件，故此时应该令$L=M+1$ 相应的，$a_i&lt;x$同时不满足条件的情况下，应该令$R=M$ 通过上面几种情况的考虑，我们可以轻松求出下界。 上界由于我们此时知道下界，那么我们求上界就是一个基本的二分了，找第一个不满足的$M$即可。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#include&lt;bits/stdc++.h&gt;using namespace std;int a[500005],b[500005],N;long long pre[500005];int cmp(int x,int y) &#123; return x&gt;y;&#125;int check(int x) &#123; long long i,res,cur,n,pos; n=N; if(x&gt;n) return 1; for(i=0;i&lt;n;i++)&#123; b[i]=a[i]; &#125; b[n]=x; n++; sort(b,b+n,cmp); res=0; for(i=0;i&lt;n;i++)&#123; pre[i]=b[i]; if(i) pre[i]+=pre[i-1]; &#125; pos=n-1; for(i=0;i&lt;n;i++)&#123; res+=b[i]; cur=(i+1)*i; while(pos&gt;=0&amp;&amp;b[pos]&lt;=(i+1))&#123; pos--; &#125; if(pos&gt;i)&#123; cur+=pre[n-1]-pre[pos]; cur+=(i+1)*(pos-i); &#125; else &#123; cur+=pre[n-1]-pre[i]; &#125; if(res&lt;=cur) continue ; if(b[i]&gt;x) return 0; return 1; &#125; return 2;&#125;int main(void) &#123; int i,x,L,R,M,T,ans,n,all; T=0; scanf("%d",&amp;N); n=N; for(i=0; i&lt;n; i++) &#123; scanf("%d",a+i); T+=(a[i]&amp;1); &#125; T%=2; L=0; R=5e5; ans=-1; while(L&lt;R) &#123; M=(L+R)/2; all=check(T+M*2); if(all==1) &#123; R=M; &#125; else if(all==2)&#123; ans=M; R=M; &#125; else &#123; L=M+1; &#125; &#125; if(ans==-1) &#123; printf("-1\n"); return 0; &#125; L=ans; R=5e5; while(L&lt;R) &#123; M=(L+R)/2; if(check(T+M*2)==2) &#123; L=M+1; &#125; else &#123; R=M; &#125; &#125; for(i=ans; i&lt;L; i++) &#123; printf("%d ",T+i*2); &#125; printf("\n");&#125;]]></content>
      <categories>
        <category>codeforces</category>
      </categories>
      <tags>
        <tag>graph-theory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型详述]]></title>
    <url>%2F2018%2F12%2F26%2FHMM%2F</url>
    <content type="text"><![CDATA[什么是隐马尔可夫模型？隐马尔可夫模型实际上是一个双重的随机过程，我们不知道具体的状态序列，只知道转移的概率，即模型的状态转移过程是未知的，而观察事件的随机过程是状态转换过程的随机函数，即我们希望通过可见的事件变换去预测深藏在其背后的本质规律。 请记住上述提到的几个概念： 状态序列(这是我们需要去预测的) 观察序列(这是我们已知的) 状态序列的转移概率(这是我们已知的) 状态序列对观察序列的转移(这是我们已知的) 隐马尔可夫链成立的三个假设（当然可以不需要理解） 状态构成一个隐马尔可夫链$$P(q_i|q_{i-1}q_{i-2}…q_1)=P(q_i|q_{i-1})$$ 不动性假设（状态与具体时间无关）$$P(q_{i+1}|q_i)=P(q_{j+1}|q_j)$$ 输出独立性假设$$P(o_1..o_i|q_1…q_i)=\prod_tP(o_t|q_t)$$ 其实这里的隐马尔可夫链中的链就表示假设了是一阶的！我们假设满足上面的性质。 现在我们考虑一个场景假如我们面对一堆过去的数据，过去的数据当中只有自然生长下的海藻的每天的状态。 我们也有一些最近的数据，最近的数据中有海藻的状态和每天的天气情况。 现在我们给定天气的转移矩阵，天气对海藻状态影响的矩阵。我们希望根据这些数据，去预测过去的天气。 海藻的状态只有4种： 干 稍干 潮湿 湿润 天气的状态只有3种： 晴 阴 雨 天气状态转移矩阵（后面简称为A）： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，明天的天气是雨，阴，晴的概率分布。 天气对海藻干湿的影响(后面简称为B): 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 注意：上表表示的是今天的天气分别是雨，阴，晴的情况下，今天的海藻出现干，稍干，潮湿，湿润的概率分布。 建立模型我们的任务是根据观察序列去推测状态序列。 首先我们根据之前的信息：构建状态集合$s$，状态转移矩阵$A$，初始状态概率分布$\pi$，观察集合$ss$，状态对观测的影响矩阵$B$ 对于我们这个题目而言，状态集合$s$为(晴，阴，雨)，观察集合$ss$为(干，稍干，潮湿，湿润)，请务必注意这个地方！ 那么我们可以得到五元组模型:$(s,ss,\pi,A,B)$，通常我们将模型简写为三元组:$\lambda =(\pi,A,B)$。 对于上述三元组，只有$\pi$是未知的，这个东西怎么统计出来呢？其实很简单，由于我们包含最近的数据，我们根据最近的天气可以统计出来每种天气出现的概率，那么这个分布就是$\pi$，公式表示为：$$\pi_i=P(q_1=s_i)$$当然这仅仅是对于这个任务是这样计算的，别的任务可能会不一样。上述公式表示的就是说第$i$种情况出现的概率 比如说统计后大概长成这个样子： 晴 雨 阴 P 0.5 0.05 0.45 求解过程我们现在已知观察序列$O=o_1….o_n$，模型$\lambda =(\pi,A,B)$ 。现在我们需要求给定模型与观察序列的情况下求状态序列$Q$。有我们需要以下概率最大：$$P(O|Q,\lambda )$$意思也很简单啊，就是我们求这个条件概率，当前模型$\lambda ​$来说，希望找到一个状态序列$Q​$使得观察序列O发生的可能性最大。 我们假设$Q=q_1…q_n$ 我们把上面的式子打开，有：$$P(O|Q,\lambda )=\pi_{q_1}A_{q_1q_2}A_{q_2q_3}…A_{q_{n-1}q_n}B_{q_1o_1}B_{q_2o_2}…B_{q_no_n}$$对上面的式子我们很简单的可以想到枚举每一种$Q$，对每一种都进行计算，之后输出使得$P(O|Q,\lambda )$最大的那个$Q$就可以了。但是这当然是不可以的，因为这个是指级别的，$n$大的话这个方法就凉了。 解释一下上面的公式： $\pi_{q_1}$表示最初天气为$q_1$成立的概率 $A_{ij}​$表示天气从$i​$到$j​$转移的概率 $B_{ij}$表示天气为$i$的情况下海藻状态为$j$的概率，也叫发射概率 viterbi算法维特比算法分为前向后向，但是我感觉会一种就行了，其实都是一样的。这里介绍一下前向算法。 前向算法思想：假设我们已知了对于$o_1…o_{n-1}o_n$最优的状态序列肯定是$o_1…o_{n-1}$的一个序列转移过来的，等于说我们对当前的一层计算来说，对于每个节点只需要考虑前面一层的结果就可以了。即有$$dp_{iq_i}=\max_{k\in s} dp_{(i-1)k}A_{k(q_j)}B_{q_jo_i}$$$dp$表示计算的结果，$q_i$表示当前层的状态，$k$表示枚举前一层的状态，从观察状态集合中枚举。$max$表示存储后面式子的最大值。 算法流程： C++的代码，写起来很简单，每种语言都可以按照这个方式写。但是为了保证精度，我们一般是取$log$的，这份代码的结果应该会很差！ 123456789101112131415161718192021void viterbi(int O[],double PI[],double A[][],double B[][])&#123;//传观察序列和模型 int i,j,back; int path[][]； double dp[][],MX； for(i=1;i&lt;=n;i++)&#123;//初始化 dp[1][i]=PI[i]; &#125; for(i=1;i&lt;=m;i++)&#123; for(j=1;j&lt;=n;j++)&#123; MX=-1; for(k=1;k&lt;=n;k++)&#123;//枚举前一层 if(MX&lt;dp[i-1][k]*A[k][j])&#123; MX=dp[i-1][k]*A[k][j];//找到最大的 back=k; &#125; &#125; path[i][j]=back;//用于回溯 dp[i][j]=MX*B[j][O[i]];//计算结果存起来 &#125; &#125;&#125; 一个简单例子为了结果看起来比较正常，我们把初始分布$\pi$ 设的极端一点，同时我们再次召唤之前的表。 晴 阴 雨 P 1 0 0 天气状态转移矩阵$A$： 雨 阴 晴 雨 0.625 0.125 0.25 阴 0.375 0.25 0.375 晴 0.25 0.25 0.5 天气对海藻干湿的影响$B$: 干 稍干 潮湿 湿润 晴 0.60 0.20 0.15 0.05 阴 0.25 0.25 0.25 0.25 雨 0.05 0.10 0.35 0.50 对于我们给定的观测序列：干，潮湿，湿润。我们计算的结果入上图所示。其中红色括号的结果就是表示结果是上一次的第几个节点过来的。 到此为止，隐马尔可夫模型的这个例子就介绍完了，当然问题有其他变种，一般是在viterbi算法上动刀，不会解决的话可以留言评论。 A和B矩阵怎么求？我们大部分时候，其实只有训练数据和测试数据，没有这些转移矩阵，这个时候我们可以通过最大似然估计来求这些矩阵(看起来吓人)。就好比这一题我没有给出矩阵$A,B$的话怎么办？ 其实我们也可以做出来这个题目，因为我在题目中特意强调了我们还有一些最近的标注数据，这时候我们可以通过最近的数据把矩阵$A,B$给估计出来，做法如下。 其实就是统计一下就行了，对于A矩阵：$$A_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的后面连接了天气状态为$j$的次数除以天气状态为$i$的出现总次数。 对于B矩阵：$$B_{ij}=\frac {total(i\&amp;j)}{total(i)}$$人话解释：天气状态为$i$的对应海藻的状态为$j$的次数除以天气状态为$i$的出现的总次数。 我们在求的时候一般加上松弛操作，即$$A_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ $$B_{ij}=\frac {total(i\&amp;j)+1}{total(i)+1}$$ 一个小项目为了使本文有一点实用价值，我把我的代码放在这里。一个序列标注的小东西。 参考资料国科大自然语言处理课件 胡玥]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[attention机制总结]]></title>
    <url>%2F2018%2F12%2F24%2FAttention%2F</url>
    <content type="text"><![CDATA[背景目前深度学习中用于做NLP的方法，大多都是首先将句子进行分句，之后将每个单词使用与训练好的词向量进行表示(其实这就是一种迁移学习？)，通过这一步我们把一个句子转化为向量的序列。这样的好处是我们可以把一个句子使用一个向量模型来表示，即每个句子我们都对应一个矩阵$x=(x_1,x_2,…x_n)$其中$x_i$表示第$i$个词的词向量，通常我们记为行向量，假如预训练好的向量维度为$d$,也就是说一个句子哦我们可以映射为$x\in R^{n*d}$。通过这个操作，我们可以将语言化作向量表示，这是一个很好的建模方式。 我们为了处理这些信息主要分为以下几个思路： RNN 可以很好的获得时序信息 CNN 可以很好的获得全局信息 Attention google提出的全新思想，在各大模型上都有提升 RNN第一个思路是RNN，思想是很简单的，进行递归式的信息提取：$$y_t=f(y_{t-1},x_t)$$由于我们在每一步进行计算的时候都需要依赖上一步计算的结果，这导致了RNN无法并行计算，这是这个模型原本就存在的缺陷，但是RNN可以很好的学习到时序信息。相应的，我们很难获取到全局信息，倘若我们想得到全局信息，我们一般使用双向RNN或者双向LSTM。 CNN第二个思路是CNN，思想也是十分简单的，进行窗口式的扫描，对于尺度为3的卷积：$$y_t=f(x_{t-1},x_t,x_{t+1})$$由于CNN的每一步进行计算的时候是独立的，故我们可以很方便的进行并行计算。其实通过这一步我们只能获取到局部的信息，但是我们通过堆叠的方式增大感受野，使得模型能够获得很好的全局信息！但是相应的，CNN的时序信息获取效果不如RNN。 AttentionGoogle在2017年的论文Attention is All You Need提供了第三个思路，这个想法也也是十分简单的，一般而言我们记作：$$y_t=f(q_t,K,V)$$其中$K$与$V$相同的时候，我们称作self-attention。计算方式在文章后续会详叙。 Transformer注意力机制 上图看起来十分复杂，我们简化一下。 这其实就是一个Seq2Seq模型，我们左边把一个encoder输入进去，右边就decoder得到输出。由于上图有两个部分，我们不妨将其中的的部分再打开看看： 我们随之而来的问题就是这个encoder是如何把信息传递给右边的呢？因为这个图和上面的压根就不一样好吗？我们再将图片打开拓展: 意思就是说对于每一层的encoder的输出，我们会和decoder的输入进行结合。 我们再取一种的一层做详细展示： 我们可以看到在解码层的做了两次attention，我们在第二次的attention时使用了左边的encoder传递过来的信息。上面这张图我觉得有点小问题，也许是我理解错误。。我认为左边的encoder端上面应该是没有箭头的。 传统Attention 机制定义Attention的翻译过来就是注意力，这表示了人类的偏好，我们在观察一个图片的时候，往往对图像的某一部分有更集中的偏好，就是所谓的”抢镜”。对某一部分有更高的关注度，这使得我们对图片有一个更加准确的感受。 我们拓展到文本中，例如我们在进行翻译任务的时候，翻译当前词的时候一般是对序列的局部信息有偏好，但是对于每个单词而言，对原序列的关注程度肯定是不一样的。attention就是使用最简单的方法实现这个功能。 计算方式我们再次召唤前面的公式:$$y_t=f(q_t,K,V)$$我们简述一下这个$f$是如何进行计算的。 我们输入一组$K$与$V$与查询$Q$，首先我们对每个$K_i$与$Q$计算相似度得到$S_i$,之后将$S$通过softmax函数进行归一化得到分布$a$，之后我们计算$a$与$V$的加权和得到对于查询$Q$的attention向量Att-V。 其中对于$f(Q,K_i)$(都为列向量的话)的计算方式主要分为以下几种： 点乘 dot product : $f(Q,K)=Q^TK$ 权重 general : $f(Q,K)=Q^TWK$ 拼接 concat ：$f(Q,K)=W[Q^T;K]$ 神经网络 perceptron ：$f(Q,K)=V^T\tanh (WQ+UK)$ 对于$Q$,$K$与$V$相同的情况下，我们也称作为自注意力机制，希望寻找文本中内在的联系。也就是说，在序列内部做Attention，寻找序列内部的联系。Google论文的主要贡献之一是它表明了内部注意力在机器翻译（甚至是一般的Seq2Seq任务）的序列编码上是相当重要的。 传统模型有一个非常明显的缺点，就是无法获得时序信息，就算我们吧顺序打乱，我们算出来的结果一样是不变的，这个就有点不太合理了！当然我们一般使用attention去辅助RNN和CNN，由于RNN和CNN已经包括了时序信息，可能会好一些。 transformer中的Attention机制在transformer当中的Attention机制与传统的attention机制还是有很大区别的。分别叫做 Scaled Dot-Product Attention 和 Multi-Head Attention。 Scaled Dot-Product Attention其结构图如下图所示： 首先第一个问题,$Q,K,V$从哪里来？按照我的理解，对于我们输入的句子$x=(x_1,x_2,…x_n)$，这一步可以是原本的词向量，也可以是对于输入的词向量做线性变化，例如$Q=xW^q,K=xW^k,V=xW^v$，例如： 我们对于input的词向量Thinking Machines，通过三个矩阵$W^q,W^k,W^v$，得到$Q,K,V$,于是我们依次遍历$Q_i$，计算每个的$Q_i$的注意力向量。 首先对于$q_1$,我们采用dot product进行计算相似度$S_i$ 之后我们将$S_i$ 除以$\sqrt{d_k}$其中$d_k$表示$K$的维度，加上这个是为了防止内积太大，如果内积过大，会导致softmax进入饱和区，就没有注意力这个作用了。接下来我们通过softmax计算概率分布。 最后我们计算加权和得到对于$q_1$的注意力向量$z_1$。直以递归下去，我们可以对每个$q$进行相同的操作即可。听上去这一步好像很麻烦，其实我们用矩阵表示的话，就很简单且简约了： 首先输入词向量$X$(行向量)，计算$Q,K,V$ 这一步就更加直观了，但是我们需要好好理解一下$QK^T$ 这一个计算过程，这一步实际上是计算了一个word2word的attention矩阵。例如我们对”I have a dream”计算的话 其中每个格子$grid_{ij}$ 表示的是第$i$个单词和第$j$个单词的相似度，这肯定是一个对称矩阵啦！现在我们回到transformer的结构图当中，可以很明显的看到有的self-attention前面加上了masked，这又是什么意思呢？ 简单来说就是为了防止程序看见未来的信息，而用灰色区域(0.0)覆盖上。 Multi-Head Attention多头的意思就很简单了，就是将工作重复做几次而已。论文中倒是画的很吓人的样子。所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接。 我们重复几次，将得到的$Z$矩阵进行拼接即可 首先我们对输入的$X$进行计算多次 将计算完的结果拼接$Z_0+Z_1+….+Z_n$，之后使用一个线性变换为指定维度的$Z$ 整个的框架如下： Position Embedding然而，我们经过思考之后，可以发现这个计算方式和传统的一样啊，我就算位置变了，对attention的计算结果不变。这个问题就很严重了，很有可能我们在机器翻译的任务当中，我们确实算出来了翻译结果应该包含了哪些单词，但是结果很有是乱序的，那么就没法用了。transformer为了解决这个问题，祭出了Position Embedding这个东西。Google是直接给出了一个公式来构造Position Embedding。$$PE_{2i}(p)=\sin(p/10000^{2i/d_{pos}})\PE_{2i+1}(p)=\cos(p/10000^{2i/d_{pos}})$$这里的意思是将position为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。Google在论文中说到他们比较过直接训练出来的位置向量和上述公式计算出来的位置向量，效果是接近的。那么为了减少计算复杂度，我们何必自己再训练呢? 原文中提到采用这个公式的另一个原因的是sin和cos，满足一些良好的性质:$$\sin(a+b)=\sin(a)\cos(b)-\cos(a)\sin(b)\\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$$这使得位置为p+k的向量可能可以使用位置为p的向量线性表出。$$PE_{2i}(p+k)=\sin((p+k)/10000^{2i/d_{pos}})=\\sin(p/10000^{2i/d_{pos}})\cos(k/10000^{2i/d_{pos}})-\cos(p/10000^{2i/d_{pos}})\sin(k/10000^{2i/d_{pos}})$$由于$k$是定值，而$\sin(p/10000^{2i/d_{pos}})$和$\cos(p/10000^{2i/d_{pos}})$确实是知道的，所以确实为线性表出提供了可能性吧。。 我们在input输入的时候加入位置信息，同时我们在decoder的输入过程中也加入了位置信息。 Position-wise Feed-forward Networks在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出：$$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$值得注意的是encoder的每一层的$W_1$和$W_2$都不相同。 The Residuals由于transformer的结构十分复杂，训练的时候很容易出现梯度消失，导致网络难以训练，文章采用的思想借鉴了何恺明的残差网络的思想，在每一次操作之后为了保证损失能够尽量回传，每层的输入会和输出进行叠加。 其中的Add&amp;Normalize就是叠加过程。公式记作：$$output=LayerNorm(x+Sublayer(x))$$但是这也带来一个小问题，就是每次sublayer输出的结果的为维度和原输入的维度应该一样，带来了一些限制。 Encoder在文章中encoder有6层，而其中的每一层包含两个sub-layer 第一个sub-layer是多头自注意力机制，用来计算文本内部的关联 第二个是全连接Position-wise Feed-forward Networks Decoder文章中decoder也是6层，其中的每一层包含三个sub-layer 第一个sub-layer是masked多头自注意力机制，原因很简单啊后面的结果还没生成呢！ 第二个sub-layer是多头注意力机制，将decoder和encoder结合，注意计算的时候$K,V$是由encoder提供的，decoder提供的是$Q$ 第三个是一个全链接网络Position-wise Feed-forward Networks The Final Linear and Softmax Layer 最后一层就比较简单了，简单来说就是首先把decoder的第6层输出经过一个神经网络对每个单词的可能性计算一个权值，再经过一个softmax是为了反向传播的时候有一个误差回传，使得整个网络能够训练。 这里还需要注意的是，每次输出的结果，都要像RNN一样作为输入传给decoder，依次输出结果。 transformer动画演示过程首先encoder的过程 接着就是decoder的过程 参考文献jalammar’s blog 苏剑林《《Attention is All You Need》浅读（简介+代码） 》]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simplex-单纯形算法介绍]]></title>
    <url>%2F2018%2F12%2F18%2FSimplex%2F</url>
    <content type="text"><![CDATA[单纯形算法（Simplex）有什么用？谈到单纯形算法，我们不得不提线性规划，所谓线性规划，就是在满足一定约束下优化目标函数。下面引用几个例子来进行简单介绍。 Food Energy Ca Price 燕麦 110 4 3 牛奶 160 8 9 猪肉 260 14 19 现在我们需要55个单位的Ca和2000个单位的Energy，问我们应该怎么购买最合适。这就是一类最经典的线性优化问题，我们可以很轻松的写出目标函数和约束条件： ​ $min$ $$3X_1+9X_2+19X_3$$ ​ $s.t.$ ​ $110X_1+160X_2+260X_3&gt;=2000$ ​ $4X_1+8X_2+14X_3&gt;=55$ ​ $X_i&gt;=0，i=1,2,3$ 上述不等式表示的含义很简单，$min$表示最小化后面的目标函数，$s.t.$ 表示subject to的缩写，意思是受限于。 针对这类问题，我们曾经学过的方法是图解法，画出可行域（阴影部分）（a），之后使用目标函数(b)在可行域上移动，之后靠直觉确定最优解。 但是计算机可没有人类的直觉！计算机求解这些问题的时候，需要更加通用的方法来求解。于是在二战时期，为了协助政府协调物资人员，前苏联的坎托诺维奇强行提出了Simplex算法。其实该算法的思想也是来自于图解法，我们观察上面的求解过程，可以很明显的发现，最优解一定是在顶点上的！！！！于是这个人就在想，我们是不是随便从一个顶点开始，之后每次迁移到这个顶点的相邻顶点上，对这个凸包每个点遍历一次就能得到答案了？事实确实也确实是这样的，而且可以更简单，实际上我们只需要随便从一个顶点开始，之后暴力这个点相邻的点，然后选择一个下降的方向迁移点就可以，直到当前的点的值最小，那么这个点就是最终的解。 但是为什么可以这样做呢？按照我的理解，因为可行域是一个凸多边形(为什么是凸的？其实画一画，就知道了。不可能是凹的，可行域一定是凸的！！)，那么这个凸多边形的顶点结果是单调的，因为全局最优解只有一个。那么求解过程就好像下山一样，只要每次向下走就可以走到谷底，类似于梯度下降法在凸二次函数上每次沿着下降的方向走，一直迭代就能得到最优解。当然这些是在扯淡，实际上这玩意是被证明了的，而我没看。。。 单纯形算法适用的情况标准的线性规划格式（也叫标准型）： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A*x&lt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$ 其中$c$表示的就是每一类$x$的花费，$A$表示的是约束矩阵，$x$表示的是擦书的列向量，$x_i$表示的是每一类的量(针对于整数线性规划，要求$x_i$必须为整数，比如$x_i$表示的是人的数量或者物品的份数) 但是有些时候我们可能碰到一些要最大化的最优化问题，比如我们固定钱数，怎么样买到的能量最多？这时候我们的优化目标变成了$max$，针对这类问题我们转换为标准形式的方式也很简单，$c*-1$ 就变为$min$ 了，同样的道理我们对$A$和$x$都可以这样干。 目标函数是$max$的时候，我们将$c$取反 $Ai*x&gt;=b$,我们将$A_i$和$b$取反 $x&lt;=0$,我们将$x$取反 松弛型我们上面介绍了标准型，现在介绍一下松弛型，其实也是一个很简单的东西，由于我们上面都是不等式，实在是太烦了，于是我们想要把不等式优化成等式，于是我们将标准型构造成下面的形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $A_i*x+x_{i+n}=b_i$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中$m$表示约束的个数，由于之前的不等式都是$&gt;=$的情况，我们可以减去一个非负的变量使得等号成立。这就是松弛形。 单纯形现在介绍单纯形，单纯形其实就是从松弛型过来的，它是为了单纯形算法求解简便一些而存在的，因为它可以导出单纯形表，一般写成如下形式： ​ $min$ $c^Tx$ ​ $s.t.$ ​ $x_{i+n}=b_i-A_ix$ ​ $x_i&gt;=0，i=1,2,3…,n+m$ 其中我们称左边的变量为基本变量，右边的变量称为非基本变量，我们很显然有一组基础解，就是令非基本变量为零，这时候基本变量的值都为其对应的$b_i$。我们一般情况下可以认为基础解就是前面说的可行解区域的一个顶点（因为原本就是边界）。 而单纯形表的初始形式很简单，举个例子 ​ $min$ $-x_1-14x_2-6x_3$ ​ $s.t.$ ​ $x_1+x_2+x_3+x_4$ $=4$ ​ $x_1$ $+x_5$ $=2$ ​ $x_3$ $+x_6$ $=3$ 当前的例子的基本变量为${x_4,x_5,x_6}$,非基本变量为${x_1,x_2,x_3}$。 其单纯形表的形式为: 0 -1 -14 -6 0 0 0 4 1 1 1 1 0 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 人话解释，第一行表示的是目标函数的系数，但是其中第0个位置表示的是当前目标函数取当前基础解作为解之后求得的值的相反数。后面的每一行表示的都是约束条件$b_i=a_{i1}x_1+a_{i2}x_2+…+a_{i(n+m)}x_{n+m}$ 求解步骤我们在使用单纯形法进行求解的时候，首先找到第一个目标函数中系数为负的非基本变量$y$，将其增大（这个过程相当于我们在凸包上沿着边缘走到另一个顶点），但是我们怎么确定这个非基本变量$y$最大能增大多少呢？这个其实很简单，只需要令除当前变量的其他变量为0，剩下$b_i$和$y$，我们可以对每个包含$y$的约束条件进行计算约束。找到使其最紧(即在当前的约束条件中让$y$的最大取值最小)的那一个约束即可，再使用当前约束条件下的基本变量与其进行替换即可。由于我们每次可以使约束z向更小的方向迁移，这使得我们的算法不会陷入死循环。 注意，假如对于非基本变量$y$不存在限制最紧的约束条件，那么该组线性规划无解，因为可以无限增大。 再次注意我们的规则(Bland规则)： 找到第一个目标函数中系数为负的非基本变量 找到限制最紧的约束条件 使用当前约束条件下的基本变量(也叫替入变量)与非基本变量（也叫替出变量）进行替换 我们对上述单纯形表进行一次示范操作： 首先我们看到第一个非基本变量$x_1$的系数为负数。那么我们开始对$x_1$进行增大。 对于第一个约束条件有$x_1=4$ 对于第二个约束条件有$x_1=2$ 对于第三个约束条件有$x_1=3$ 由于第二个约束是最紧的，那么我们选择第二个约束中的$x_5$作为替出变量，此时有单纯形表的变化为 2 0 -14 -6 0 1 0 2 0 1 1 1 -1 0 2 1 0 0 0 1 0 3 0 0 1 0 0 1 由于我们在替出时将$x_1=2-x_5$带入，很明显可以将单纯形表进行转换。如果没有想清楚的话我们可以手动带入一下即可。 一直重复这个操作，直到所有的非基本变量的系数都大于零（顶点无法再迁移），那么程序就求出了最优解。 注意，本文其实未完待续，因为其实在实现算法中间存在很多细节，但本文的目的是让读者对该算法有个系统的了解，明白算法的原由，至于一些具体的细节，读者可以参考链接进行进一步的了解！ 单纯形Simplex模板给出一份Simplex的模板与其应用，所求解的题目链接：BZOJ 1061： 注意！！该模板求解的是： ​ $max$ $c^Tx$ ​ $s.t.$ ​ $A*x&gt;=b$ ​ $x_i&gt;=0，i=1,2,3…,n$这类问题，稍稍转换一下就好了。之所以选择这个作为模板，是因为这份模板直接省去了替出变量，节省了大量空间，而且极其富有技巧性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;cstdio&gt;#include &lt;cmath&gt;typedef double DB;typedef long long LL;const int maxn = 1005, maxm = 10005;const DB inf = 0x3f3f3f3f3f3f3f3f, eps = 1e-7;int n, m;DB b[maxm], c[maxn], cof[maxm][maxn], ans;//约束、目标函数的系数、约束矩阵inline void pivot(int id, int pos) &#123; b[id] /= cof[id][pos];//首先归一化 cof[id][pos] = 1 / cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 for(int i = 1; i &lt;= n; i++) if(i != pos) cof[id][i] *= cof[id][pos];//归一化 for(int i = 1; i &lt;= m; i++) if(i != id &amp;&amp; fabs(cof[i][pos]) &gt; eps) &#123;//当前约束条件包含约束最紧的变量 b[i] -= cof[i][pos] * b[id];//搞死小圆(高斯消元)的方法更新变量的系数和b的值 for(int j = 1; j &lt;= n; j++) if(j != pos) cof[i][j] -= cof[i][pos] * cof[id][j]; cof[i][pos] = -cof[i][pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方 &#125; ans += c[pos] * b[id];//对c进行消元 for(int i = 1; i &lt;= n; i++) if(i != pos) c[i] -= c[pos] * cof[id][i]; c[pos] = -c[pos] * cof[id][pos];//将变量直接改变为松弛变量，技巧的地方&#125;inline DB simplex() &#123; while(1) &#123; int pos, id; for(pos = 1; pos &lt;= n; pos++) if(c[pos] &gt; eps) break;//找到第一个系数大于0的变量，进行增大 if(pos == n + 1) return ans;//找不到了结束，返回答案 DB tmp = inf; for(int i = 1; i &lt;= m; i++) if(cof[i][pos] &gt; eps &amp;&amp; b[i] / cof[i][pos] &lt; tmp) tmp = b[i] / cof[i][pos], id = i;//找到约束最紧的那个约束条件 if(tmp == inf) return inf; pivot(id, pos);//进行顶点的迁移（旋转） &#125;&#125;int main() &#123; scanf("%d%d", &amp;n, &amp;m); for(int i = 1; i &lt;= n; i++) scanf("%lf", &amp;c[i]);//其实将min改为max问题，可以直接采用结论，求对偶问题即可。 for(int i = 1; i &lt;= m; i++) &#123; int x, y; scanf("%d%d", &amp;x, &amp;y); for(int j = x; j &lt;= y; j++) cof[i][j] = 1; scanf("%lf", &amp;b[i]); &#125; printf("%lld\n", LL(simplex() + 0.5)); return 0;&#125; 参考文献hrwhisper]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>Simplex</tag>
        <tag>最优化</tag>
        <tag>线性规划</tag>
      </tags>
  </entry>
</search>
